For some reason post Reagan Republicans seem to be hell bent on destroying the United States of America. It seems to me they feel liberalism must be destroyed at any cost, even if it means burn the country to the ground and salt the earth.