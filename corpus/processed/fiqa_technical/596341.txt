I've spent a lot of time augmenting overgrown excel models with R. Basically any process you pull from excel into sql / r / python will run faster, sometimes much faster, so the more calculations you can push into R code, the better. Vlookup especially is very poorly optimized in excel and I've seen the equivalent in R run literally 100x faster over the same data. Additionally you can 'chunk' through data (or load -> process -> save as CSV) in R to minimize the memory load on your machine vs keeping a giant excel model open all day. A key insight is that the model does not need to be pulled wholesale from excel to R. You can in fact take specific tables or target specific computationally intense portions of the model to be pulled into R and processed before returning them to a CSV to be picked up by your core excel model (often you'll run this process once per day, in the morning). In this case, you effectively create a 'two step' model, where you use R to 'refresh' the model to reflect new data, and still maintain the excel as the core working GUI for when you have dashboarding / scenario analysis type tasks that need to be maintained.