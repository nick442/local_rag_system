#!/usr/bin/env python3
"""
E3 Hybrid Fusion: RRF vs. Z-Score on doc-aware rankings.

Inputs:
  - Requires per-query rankings for 'dense' and 'bm25' generated by e1_eval.py
    with --emit_diagnostics (so we have per-doc best_chunk scores):
      <outdir>/per_query.jsonl         # dense
      <outdir>/per_query_bm25.jsonl    # bm25
  - Ground-truth qrels: <dataset_dir>/qrels.tsv (BEIR format)

If the per-query files are missing and --db-path/--collection/--embedding-path
are provided, this script will invoke e1_eval.py to generate them.

Outputs:
  - <outdir>/metrics_rrf.json
  - <outdir>/metrics_zscore.json
  - <outdir>/per_query_rrf.jsonl
  - <outdir>/per_query_zscore.jsonl
  - <outdir>/FUSION_README.md (provenance and parameters)
"""

from __future__ import annotations

import argparse
import json
from pathlib import Path
from typing import Dict, Any, List, Tuple


def load_qrels(path: Path) -> Dict[str, Dict[str, float]]:
    truth: Dict[str, Dict[str, float]] = {}
    with path.open('r', encoding='utf-8') as f:
        for raw in f:
            s = raw.strip()
            if not s:
                continue
            parts = s.split('\t')
            try:
                if len(parts) >= 4:
                    qid, _zero, docid, rel = parts[0], parts[1], parts[2], parts[3]
                elif len(parts) == 3:
                    qid, docid, rel = parts[0], parts[1], parts[2]
                else:
                    continue
                rel_v = float(rel)
            except Exception:
                continue
            truth.setdefault(qid, {})[docid] = rel_v
    return truth


def load_per_query(path: Path) -> Dict[str, Dict[str, Any]]:
    """Return mapping qid -> {'docs': [doc_ids], 'scores': {doc_id: best_score}}."""
    res: Dict[str, Dict[str, Any]] = {}
    with path.open('r', encoding='utf-8') as f:
        for line in f:
            o = json.loads(line)
            qid = o['qid']
            docs: List[str] = o.get('ranked_docs', [])
            diags = o.get('diagnostics') or []
            scores: Dict[str, float] = {}
            for d in diags:
                doc_id = d.get('doc_id')
                sc = d.get('best_score')
                if doc_id is not None and isinstance(sc, (int, float)):
                    scores[doc_id] = float(sc)
            res[qid] = {'docs': docs, 'scores': scores}
    return res


def load_docid_aliases(db_path: Path, collection: str) -> Dict[str, str]:
    """Map internal doc_id -> external corpus id (filename stem)."""
    import sqlite3
    aliases: Dict[str, str] = {}
    with sqlite3.connect(str(db_path)) as con:
        con.row_factory = sqlite3.Row
        cur = con.cursor()
        cur.execute(
            "SELECT doc_id, source_path FROM documents WHERE collection_id = ?",
            (collection,),
        )
        for row in cur.fetchall():
            src = row["source_path"] or ""
            stem = Path(src).stem
            aliases[row["doc_id"]] = stem
    return aliases


def ensure_per_query(args) -> Tuple[Path, Path]:
    outdir = Path(args.outdir)
    pq_dense = outdir / 'per_query.jsonl'
    pq_bm25 = outdir / 'per_query_bm25.jsonl'
    eval_py = Path(__file__).with_name('e1_eval.py')
    if not pq_dense.exists():
        if not args.db_path or not args.collection or not args.embedding_path:
            raise RuntimeError('per_query.jsonl missing and no db/collection/embedding provided to regenerate')
        qopt = f" --queries '{args.queries}'" if args.queries else ""
        cmd = (
            f"python '{eval_py}' --db-path '{args.db_path}' --embedding-path '{args.embedding_path}' "
            f"--collection '{args.collection}' --dataset-dir '{args.dataset_dir}'{qopt} --mode dense "
            f"--kdocs {args.kdocs} --nchunks {args.nchunks} --cap {args.cap} --emit_diagnostics --outdir '{outdir}'"
        )
        print('[e3_fusion] Generating dense per_query via:', cmd)
        import os
        rc = os.system(cmd)
        if rc != 0:
            raise RuntimeError('Failed to generate dense per_query')
    if not pq_bm25.exists():
        if not args.db_path or not args.collection or not args.embedding_path:
            raise RuntimeError('per_query_bm25.jsonl missing and no db/collection/embedding provided to regenerate')
        qopt = f" --queries '{args.queries}'" if args.queries else ""
        cmd = (
            f"python '{eval_py}' --db-path '{args.db_path}' --embedding-path '{args.embedding_path}' "
            f"--collection '{args.collection}' --dataset-dir '{args.dataset_dir}'{qopt} --mode bm25 "
            f"--kdocs {args.kdocs} --nchunks {args.nchunks} --cap {args.cap} --emit_diagnostics --outdir '{outdir}'"
        )
        print('[e3_fusion] Generating bm25 per_query via:', cmd)
        import os
        rc = os.system(cmd)
        if rc != 0:
            raise RuntimeError('Failed to generate bm25 per_query')
    return pq_dense, pq_bm25


def rrf_fusion(
    dense: Dict[str, Dict[str, Any]],
    bm25: Dict[str, Dict[str, Any]],
    k: int = 60,
    kdocs: int = 10,
) -> Dict[str, List[str]]:
    out: Dict[str, List[str]] = {}
    for qid in dense.keys() | bm25.keys():
        d_docs = dense.get(qid, {}).get('docs', [])
        b_docs = bm25.get(qid, {}).get('docs', [])
        union = list(dict.fromkeys(d_docs + b_docs).keys())
        d_rank = {doc: i + 1 for i, doc in enumerate(d_docs)}
        b_rank = {doc: i + 1 for i, doc in enumerate(b_docs)}
        scores: Dict[str, float] = {}
        for doc in union:
            rd = d_rank.get(doc)
            rb = b_rank.get(doc)
            s = 0.0
            if rd is not None:
                s += 1.0 / (k + rd)
            if rb is not None:
                s += 1.0 / (k + rb)
            scores[doc] = s
        ranked = sorted(union, key=lambda d: scores[d], reverse=True)[:kdocs]
        out[qid] = ranked
    return out


def zscore_fusion(
    dense: Dict[str, Dict[str, Any]],
    bm25: Dict[str, Dict[str, Any]],
    kdocs: int = 10,
) -> Dict[str, List[str]]:
    out: Dict[str, List[str]] = {}
    import math
    for qid in dense.keys() | bm25.keys():
        d_docs = dense.get(qid, {}).get('docs', [])
        b_docs = bm25.get(qid, {}).get('docs', [])
        d_sco = dense.get(qid, {}).get('scores', {})
        b_sco = bm25.get(qid, {}).get('scores', {})
        union = list(dict.fromkeys(d_docs + b_docs).keys())
        # z-normalize within each method
        def zmap(vals: Dict[str, float]) -> Dict[str, float]:
            xs = list(vals.values())
            if not xs:
                return {}
            mu = sum(xs) / len(xs)
            var = sum((x - mu) ** 2 for x in xs) / max(1, len(xs) - 1)
            sd = math.sqrt(var) if var > 0 else 1.0
            return {k: (v - mu) / sd for k, v in vals.items()}

        dz = zmap(d_sco)
        bz = zmap(b_sco)
        scores: Dict[str, float] = {}
        for doc in union:
            scores[doc] = dz.get(doc, 0.0) + bz.get(doc, 0.0)
        ranked = sorted(union, key=lambda d: scores[d], reverse=True)[:kdocs]
        out[qid] = ranked
    return out


def write_per_query(path: Path, results_docs: Dict[str, List[str]]) -> None:
    with path.open('w', encoding='utf-8') as f:
        for qid, docs in results_docs.items():
            f.write(json.dumps({'qid': qid, 'ranked_docs': docs}) + '\n')


def main() -> None:
    ap = argparse.ArgumentParser()
    ap.add_argument('--dataset-dir', required=True)
    ap.add_argument('--outdir', required=True)
    ap.add_argument('--db-path', default=None)
    ap.add_argument('--embedding-path', default=None)
    ap.add_argument('--collection', default=None)
    ap.add_argument('--kdocs', type=int, default=10)
    ap.add_argument('--nchunks', type=int, default=100)
    ap.add_argument('--cap', type=int, default=3)
    ap.add_argument('--rrf_k', type=int, default=60)
    ap.add_argument('--queries', default=None, help='Optional queries.jsonl to pass to e1_eval when regenerating per_query')
    args = ap.parse_args()

    outdir = Path(args.outdir)
    outdir.mkdir(parents=True, exist_ok=True)

    # Ensure project root is importable
    import sys
    sys.path.insert(0, str(Path('.').resolve()))

    # Ensure per-query inputs exist (generate if possible)
    pq_dense, pq_bm25 = ensure_per_query(args)

    dense = load_per_query(pq_dense)
    bm25 = load_per_query(pq_bm25)

    # If doc IDs are internal (GUIDs), alias them to corpus doc IDs using source_path stem
    if args.db_path and args.collection:
        aliases = load_docid_aliases(Path(args.db_path), args.collection)
        def remap(pq: Dict[str, Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:
            out: Dict[str, Dict[str, Any]] = {}
            for qid, data in pq.items():
                docs = [aliases.get(d, d) for d in data.get('docs', [])]
                scores = {aliases.get(d, d): sc for d, sc in data.get('scores', {}).items()}
                out[qid] = {'docs': docs, 'scores': scores}
            return out
        dense = remap(dense)
        bm25 = remap(bm25)

    # Fusions
    fused_rrf = rrf_fusion(dense, bm25, k=args.rrf_k, kdocs=args.kdocs)
    fused_z = zscore_fusion(dense, bm25, kdocs=args.kdocs)

    # Load qrels and evaluate
    from src.evaluation_metrics import RetrievalQualityEvaluator
    qrels = load_qrels(Path(args.dataset_dir) / 'qrels.tsv')

    rqe = RetrievalQualityEvaluator(ground_truth_relevance=qrels)
    m_rrf = rqe.evaluate_query_set(fused_rrf, k_values=[1, 5, 10, 20])
    m_z = rqe.evaluate_query_set(fused_z, k_values=[1, 5, 10, 20])

    # Write outputs
    (outdir / 'metrics_rrf.json').write_text(json.dumps(m_rrf, indent=2))
    (outdir / 'metrics_zscore.json').write_text(json.dumps(m_z, indent=2))
    write_per_query(outdir / 'per_query_rrf.jsonl', fused_rrf)
    write_per_query(outdir / 'per_query_zscore.jsonl', fused_z)

    # README/provenance
    readme = f"""
E3 Hybrid Fusion (RRF vs Z-Score)
Dataset dir: {args.dataset_dir}
Variant outdir: {outdir}
Parameters: kdocs={args.kdocs}, nchunks={args.nchunks}, cap={args.cap}, rrf_k={args.rrf_k}

Inputs:
  - Dense per-query: {pq_dense}
  - BM25 per-query: {pq_bm25}

Outputs:
  - metrics_rrf.json, metrics_zscore.json
  - per_query_rrf.jsonl, per_query_zscore.jsonl

Notes:
  - Fusion operates on doc-aware rankings. Z-score uses per-doc best_chunk scores captured by e1_eval diagnostics; RRF uses 1/(k+rank).
  - If inputs were missing, per-query files were (re)generated via e1_eval.py.
"""
    (outdir / 'FUSION_README.md').write_text(readme)
    print('[e3_fusion] Wrote RRF and Z-Score fusion metrics to', outdir)


if __name__ == '__main__':
    main()
