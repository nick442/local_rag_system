üî¨ Queuing SciFact experiment to run after FIQA completes...
Monitoring FIQA experiment results file...
‚è≥ Waiting for FIQA experiment to generate results file... (21:40:31)
‚è≥ Waiting for FIQA experiment to generate results file... (21:45:31)
‚è≥ Waiting for FIQA experiment to generate results file... (21:50:31)
‚è≥ Waiting for FIQA experiment to generate results file... (21:55:31)
‚è≥ Waiting for FIQA experiment to generate results file... (22:00:31)
‚è≥ Waiting for FIQA experiment to generate results file... (22:05:32)
‚è≥ Waiting for FIQA experiment to generate results file... (22:10:32)
‚è≥ Waiting for FIQA experiment to generate results file... (22:15:32)
‚è≥ Waiting for FIQA experiment to generate results file... (22:20:32)
‚è≥ Waiting for FIQA experiment to generate results file... (22:57:58)
‚è≥ Waiting for FIQA experiment to generate results file... (23:33:48)
‚è≥ Waiting for FIQA experiment to generate results file... (23:53:04)
‚è≥ Waiting for FIQA experiment to generate results file... (23:58:04)
‚è≥ Waiting for FIQA experiment to generate results file... (00:03:04)
‚è≥ Waiting for FIQA experiment to generate results file... (00:08:04)
‚è≥ Waiting for FIQA experiment to generate results file... (00:13:05)
‚è≥ Waiting for FIQA experiment to generate results file... (00:18:05)
‚è≥ Waiting for FIQA experiment to generate results file... (00:23:05)
‚è≥ Waiting for FIQA experiment to generate results file... (00:28:05)
‚è≥ Waiting for FIQA experiment to generate results file... (00:33:05)
‚è≥ Waiting for FIQA experiment to generate results file... (00:38:05)
‚è≥ Waiting for FIQA experiment to generate results file... (00:43:06)
‚úÖ FIQA experiment completed! Starting SciFact experiment...
üöÄ Starting SciFact chunk optimization experiment...
üî¨ Running experiment template: Chunk Size and Overlap Optimization
Systematic study of chunking strategies on retrieval quality and response 
accuracy
Parameters: 3 | Corpus: scifact_scientific
Estimated: 60 configs √ó 10 queries ‚âà 30.0min
This will run 600 experiments. Continue? [y/N]: 2025-09-05 00:45:07,243 - src.config_manager - INFO - Loaded configuration from config/rag_config.yaml
2025-09-05 00:45:07,245 - src.experiment_runner - INFO - Running template experiment 'Chunk Size and Overlap Optimization' (template_Chunk Size and Overlap Optimization_1757025907)
2025-09-05 00:45:07,245 - src.experiment_runner - INFO - Starting parameter sweep experiment template_Chunk Size and Overlap Optimization_1757025907
2025-09-05 00:45:07,248 - src.experiment_runner - INFO - Created experiment template_Chunk Size and Overlap Optimization_1757025907 of type parameter_sweep
2025-09-05 00:45:07,249 - src.experiment_runner - WARNING - Skipping invalid configuration: ['chunk_overlap (128) must be less than chunk_size (128)']
2025-09-05 00:45:07,249 - src.experiment_runner - WARNING - Skipping invalid configuration: ['chunk_overlap (128) must be less than chunk_size (128)']
2025-09-05 00:45:07,249 - src.experiment_runner - WARNING - Skipping invalid configuration: ['chunk_overlap (128) must be less than chunk_size (128)']
2025-09-05 00:45:07,249 - src.experiment_runner - WARNING - Skipping invalid configuration: ['chunk_overlap (256) must be less than chunk_size (128)']
2025-09-05 00:45:07,249 - src.experiment_runner - WARNING - Skipping invalid configuration: ['chunk_overlap (256) must be less than chunk_size (128)']
2025-09-05 00:45:07,249 - src.experiment_runner - WARNING - Skipping invalid configuration: ['chunk_overlap (256) must be less than chunk_size (128)']
2025-09-05 00:45:07,249 - src.experiment_runner - WARNING - Skipping invalid configuration: ['chunk_overlap (256) must be less than chunk_size (256)']
2025-09-05 00:45:07,249 - src.experiment_runner - WARNING - Skipping invalid configuration: ['chunk_overlap (256) must be less than chunk_size (256)']
2025-09-05 00:45:07,249 - src.experiment_runner - WARNING - Skipping invalid configuration: ['chunk_overlap (256) must be less than chunk_size (256)']
2025-09-05 00:45:07,251 - src.experiment_runner - INFO - Generated 51 configurations √ó 10 queries = 510 runs
2025-09-05 00:45:07,251 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:45:07,251 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:45:08,023 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
/Users/nickwiebe/miniforge3/envs/rag_env/lib/python3.11/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'dlopen(/Users/nickwiebe/miniforge3/envs/rag_env/lib/python3.11/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libjpeg.9.dylib
  Referenced from: <EB3FF92A-5EB1-3EE8-AF8B-5923C1265422> /Users/nickwiebe/miniforge3/envs/rag_env/lib/python3.11/site-packages/torchvision/image.so
  Reason: tried: '/Users/nickwiebe/miniforge3/envs/rag_env/lib/python3.11/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/Users/nickwiebe/miniforge3/envs/rag_env/lib/python3.11/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/Users/nickwiebe/miniforge3/envs/rag_env/lib/python3.11/lib-dynload/../../libjpeg.9.dylib' (no such file), '/Users/nickwiebe/miniforge3/envs/rag_env/bin/../lib/libjpeg.9.dylib' (no such file)'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
2025-09-05 00:45:09,797 - datasets - INFO - PyTorch version 2.5.1 available.
2025-09-05 00:45:10,007 - src.embedding_service - INFO - Loading embedding model from: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:45:10,007 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
/Users/nickwiebe/miniforge3/envs/rag_env/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
2025-09-05 00:45:11,687 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:45:11,687 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:45:11,687 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:45:11,687 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:45:11,693 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:45:11,693 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:45:11,864 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:45:11,864 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:45:11,864 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:45:11,904 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:45:12,933 - src.llm_wrapper - INFO - Model ready in 1.07s
2025-09-05 00:45:12,934 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:45:12,934 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:45:12,934 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:45:12,934 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:45:12,934 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:45:12,934 - src.rag_pipeline - INFO - Processing validated query: 'What is machine learning?'
2025-09-05 00:45:12,936 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is machine learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.67it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.67it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.62it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.62it/s, batch=1/1, memory=N/A]
2025-09-05 00:45:13,306 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:45:13,307 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:45:13,589 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.653s
2025-09-05 00:45:13,593 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:45:13,593 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1162 tokens) in 0.003s
2025-09-05 00:45:19,139 - src.rag_pipeline - INFO - Generated response (59 tokens) in 5.545s
2025-09-05 00:45:19,239 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:45:19,289 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:45:19,294 - src.experiment_runner - INFO - Completed run 1/510: 11.89s
2025-09-05 00:45:19,294 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:45:19,294 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:45:19,294 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:45:19,294 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:45:19,294 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:45:19,295 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:45:19,295 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:45:19,299 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:45:19,299 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:45:19,299 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:45:19,299 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:45:19,300 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:45:19,300 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:45:19,782 - src.llm_wrapper - INFO - Model ready in 0.48s
2025-09-05 00:45:19,782 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:45:19,782 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:45:19,782 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:45:19,782 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:45:19,782 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:45:19,782 - src.rag_pipeline - INFO - Processing validated query: 'How does artificial intelligence work?'
2025-09-05 00:45:19,789 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does artificial intelligence work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 10.89it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 10.75it/s, batch=1/1, memory=N/A]
2025-09-05 00:45:19,995 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:45:19,996 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:45:20,254 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.465s
2025-09-05 00:45:20,255 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:45:20,256 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1211 tokens) in 0.001s
2025-09-05 00:45:36,733 - src.rag_pipeline - INFO - Generated response (287 tokens) in 16.476s
2025-09-05 00:45:36,842 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:45:36,882 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:45:36,892 - src.experiment_runner - INFO - Completed run 2/510: 17.44s
2025-09-05 00:45:36,892 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:45:36,892 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:45:36,892 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:45:36,893 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:45:36,893 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:45:36,893 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:45:36,893 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:45:36,898 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:45:36,898 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:45:36,898 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:45:36,898 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:45:36,898 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:45:36,899 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:45:37,370 - src.llm_wrapper - INFO - Model ready in 0.47s
2025-09-05 00:45:37,370 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:45:37,370 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:45:37,370 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:45:37,370 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:45:37,370 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:45:37,370 - src.rag_pipeline - INFO - Processing validated query: 'Explain deep learning algorithms.'
2025-09-05 00:45:37,377 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain deep learning algorithms....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.35it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.34it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.30it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.30it/s, batch=1/1, memory=N/A]
2025-09-05 00:45:37,788 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:45:37,789 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:45:38,057 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.680s
2025-09-05 00:45:38,059 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:45:38,060 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1198 tokens) in 0.002s
2025-09-05 00:45:43,214 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.154s
2025-09-05 00:45:43,308 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:45:43,357 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:45:43,362 - src.experiment_runner - INFO - Completed run 3/510: 6.32s
2025-09-05 00:45:43,363 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:45:43,363 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:45:43,363 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:45:43,363 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:45:43,363 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:45:43,363 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:45:43,363 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:45:43,368 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:45:43,369 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:45:43,369 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:45:43,369 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:45:43,369 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:45:43,369 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:45:43,870 - src.llm_wrapper - INFO - Model ready in 0.50s
2025-09-05 00:45:43,870 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:45:43,870 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:45:43,870 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:45:43,870 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:45:43,870 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:45:43,870 - src.rag_pipeline - INFO - Processing validated query: 'What are neural networks?'
2025-09-05 00:45:43,877 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are neural networks?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 26.06it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 25.30it/s, batch=1/1, memory=N/A]
2025-09-05 00:45:44,001 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:45:44,002 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:45:44,250 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.372s
2025-09-05 00:45:44,250 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:45:44,251 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1353 tokens) in 0.001s
2025-09-05 00:45:58,796 - src.rag_pipeline - INFO - Generated response (238 tokens) in 14.543s
2025-09-05 00:45:58,936 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:45:58,982 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:45:58,987 - src.experiment_runner - INFO - Completed run 4/510: 15.43s
2025-09-05 00:45:58,987 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:45:58,987 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:45:58,987 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:45:58,988 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:45:58,988 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:45:58,988 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:45:58,988 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:45:58,993 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:45:58,994 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:45:58,994 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:45:58,994 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:45:58,994 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:45:58,994 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:45:59,489 - src.llm_wrapper - INFO - Model ready in 0.50s
2025-09-05 00:45:59,489 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:45:59,489 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:45:59,489 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:45:59,489 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:45:59,489 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:45:59,489 - src.rag_pipeline - INFO - Processing validated query: 'How do large language models work?'
2025-09-05 00:45:59,498 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How do large language models work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.42it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.41it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.39it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.39it/s, batch=1/1, memory=N/A]
2025-09-05 00:45:59,832 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:45:59,833 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:46:00,142 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.644s
2025-09-05 00:46:00,144 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:46:00,144 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1337 tokens) in 0.002s
2025-09-05 00:46:05,623 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.478s
2025-09-05 00:46:05,775 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:46:05,826 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:46:05,834 - src.experiment_runner - INFO - Completed run 5/510: 6.64s
2025-09-05 00:46:05,834 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:46:05,834 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:46:05,835 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:46:05,835 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:46:05,835 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:46:05,835 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:46:05,835 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:46:05,840 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:46:05,840 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:46:05,840 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:46:05,840 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:46:05,840 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:46:05,840 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:46:06,319 - src.llm_wrapper - INFO - Model ready in 0.48s
2025-09-05 00:46:06,319 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:46:06,319 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:46:06,319 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:46:06,319 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:46:06,319 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:46:06,319 - src.rag_pipeline - INFO - Processing validated query: 'What is natural language processing?'
2025-09-05 00:46:06,326 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is natural language processing?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  8.37it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  8.34it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  8.11it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  8.11it/s, batch=1/1, memory=N/A]
2025-09-05 00:46:06,574 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:46:06,575 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:46:06,852 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.526s
2025-09-05 00:46:06,854 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:46:06,854 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1749 tokens) in 0.002s
2025-09-05 00:46:13,974 - src.rag_pipeline - INFO - Generated response (55 tokens) in 7.119s
2025-09-05 00:46:14,097 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:46:14,141 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:46:14,148 - src.experiment_runner - INFO - Completed run 6/510: 8.14s
2025-09-05 00:46:14,148 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:46:14,148 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:46:14,148 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:46:14,148 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:46:14,148 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:46:14,148 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:46:14,148 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:46:14,154 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:46:14,154 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:46:14,154 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:46:14,154 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:46:14,154 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:46:14,154 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:46:14,638 - src.llm_wrapper - INFO - Model ready in 0.48s
2025-09-05 00:46:14,638 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:46:14,638 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:46:14,638 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:46:14,638 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:46:14,638 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:46:14,638 - src.rag_pipeline - INFO - Processing validated query: 'Explain computer vision techniques.'
2025-09-05 00:46:14,646 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain computer vision techniques....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  8.77it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  8.76it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  8.65it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  8.65it/s, batch=1/1, memory=N/A]
2025-09-05 00:46:14,900 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:46:14,902 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:46:15,182 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.536s
2025-09-05 00:46:15,184 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:46:15,184 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1297 tokens) in 0.002s
2025-09-05 00:46:33,567 - src.rag_pipeline - INFO - Generated response (318 tokens) in 18.381s
2025-09-05 00:46:33,709 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:46:33,751 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:46:33,757 - src.experiment_runner - INFO - Completed run 7/510: 19.42s
2025-09-05 00:46:33,757 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:46:33,757 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:46:33,757 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:46:33,758 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:46:33,758 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:46:33,758 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:46:33,758 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:46:33,763 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:46:33,763 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:46:33,763 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:46:33,763 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:46:33,763 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:46:33,763 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:46:34,260 - src.llm_wrapper - INFO - Model ready in 0.50s
2025-09-05 00:46:34,260 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:46:34,260 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:46:34,260 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:46:34,260 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:46:34,260 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:46:34,260 - src.rag_pipeline - INFO - Processing validated query: 'What is reinforcement learning?'
2025-09-05 00:46:34,268 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is reinforcement learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.96it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.91it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.82it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.82it/s, batch=1/1, memory=N/A]
2025-09-05 00:46:34,548 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:46:34,549 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:46:34,825 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.558s
2025-09-05 00:46:34,827 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:46:34,827 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1236 tokens) in 0.002s
2025-09-05 00:46:46,456 - src.rag_pipeline - INFO - Generated response (180 tokens) in 11.627s
2025-09-05 00:46:46,597 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:46:46,642 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:46:46,649 - src.experiment_runner - INFO - Completed run 8/510: 12.70s
2025-09-05 00:46:46,649 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:46:46,649 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:46:46,649 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:46:46,649 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:46:46,650 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:46:46,650 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:46:46,650 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:46:46,655 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:46:46,655 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:46:46,655 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:46:46,655 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:46:46,655 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:46:46,655 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:46:47,141 - src.llm_wrapper - INFO - Model ready in 0.49s
2025-09-05 00:46:47,141 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:46:47,141 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:46:47,141 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:46:47,141 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:46:47,141 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:46:47,141 - src.rag_pipeline - INFO - Processing validated query: 'How does data preprocessing work?'
2025-09-05 00:46:47,147 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does data preprocessing work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.85it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.84it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.76it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.76it/s, batch=1/1, memory=N/A]
2025-09-05 00:46:47,447 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:46:47,448 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:46:47,707 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.559s
2025-09-05 00:46:47,711 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:46:47,712 - src.rag_pipeline - INFO - Built prompt with 5 contexts (909 tokens) in 0.005s
2025-09-05 00:46:56,135 - src.rag_pipeline - INFO - Generated response (139 tokens) in 8.415s
2025-09-05 00:46:56,368 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:46:56,417 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:46:56,426 - src.experiment_runner - INFO - Completed run 9/510: 9.49s
2025-09-05 00:46:56,426 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:46:56,426 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:46:56,426 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:46:56,426 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:46:56,426 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:46:56,426 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:46:56,426 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:46:56,431 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:46:56,432 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:46:56,432 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:46:56,432 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:46:56,432 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:46:56,432 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:46:56,905 - src.llm_wrapper - INFO - Model ready in 0.47s
2025-09-05 00:46:56,905 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:46:56,905 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:46:56,905 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:46:56,905 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:46:56,905 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:46:56,905 - src.rag_pipeline - INFO - Processing validated query: 'What are the challenges in AI development?'
2025-09-05 00:46:56,912 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are the challenges in AI development?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 10.12it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 10.00it/s, batch=1/1, memory=N/A]
2025-09-05 00:46:57,090 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:46:57,091 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:46:57,474 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.562s
2025-09-05 00:46:57,475 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:46:57,488 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1445 tokens) in 0.013s
2025-09-05 00:47:11,787 - src.rag_pipeline - INFO - Generated response (237 tokens) in 14.298s
2025-09-05 00:47:11,880 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:47:11,918 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:47:11,933 - src.experiment_runner - INFO - Completed run 10/510: 15.36s
2025-09-05 00:47:11,934 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:47:11,934 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:47:11,934 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:47:11,934 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:47:11,934 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:47:11,934 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:47:11,934 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:47:11,940 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:47:11,941 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:47:11,941 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:47:11,941 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:47:11,941 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:47:11,941 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:47:12,392 - src.llm_wrapper - INFO - Model ready in 0.45s
2025-09-05 00:47:12,392 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:47:12,392 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:47:12,392 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:47:12,392 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:47:12,392 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:47:12,392 - src.rag_pipeline - INFO - Processing validated query: 'What is machine learning?'
2025-09-05 00:47:12,398 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is machine learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 24.55it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 23.90it/s, batch=1/1, memory=N/A]
2025-09-05 00:47:12,524 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:47:12,525 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:47:12,788 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.390s
2025-09-05 00:47:12,789 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:47:12,789 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1162 tokens) in 0.001s
2025-09-05 00:47:18,109 - src.rag_pipeline - INFO - Generated response (59 tokens) in 5.320s
2025-09-05 00:47:18,184 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:47:18,219 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:47:18,223 - src.experiment_runner - INFO - Completed run 11/510: 6.18s
2025-09-05 00:47:18,223 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:47:18,223 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:47:18,223 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:47:18,223 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:47:18,223 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:47:18,223 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:47:18,223 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:47:18,228 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:47:18,228 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:47:18,228 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:47:18,228 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:47:18,228 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:47:18,228 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:47:18,648 - src.llm_wrapper - INFO - Model ready in 0.42s
2025-09-05 00:47:18,648 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:47:18,648 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:47:18,648 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:47:18,648 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:47:18,648 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:47:18,648 - src.rag_pipeline - INFO - Processing validated query: 'How does artificial intelligence work?'
2025-09-05 00:47:18,654 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does artificial intelligence work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 60.37it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 57.09it/s, batch=1/1, memory=N/A]
2025-09-05 00:47:18,750 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:47:18,751 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:47:18,798 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.143s
2025-09-05 00:47:18,799 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:47:18,799 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1211 tokens) in 0.001s
2025-09-05 00:47:33,819 - src.rag_pipeline - INFO - Generated response (287 tokens) in 15.019s
2025-09-05 00:47:33,895 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:47:33,927 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:47:33,931 - src.experiment_runner - INFO - Completed run 12/510: 15.60s
2025-09-05 00:47:33,931 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:47:33,931 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:47:33,931 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:47:33,931 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:47:33,931 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:47:33,931 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:47:33,931 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:47:33,936 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:47:33,936 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:47:33,936 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:47:33,936 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:47:33,936 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:47:33,936 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:47:34,364 - src.llm_wrapper - INFO - Model ready in 0.43s
2025-09-05 00:47:34,364 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:47:34,364 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:47:34,364 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:47:34,364 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:47:34,364 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:47:34,364 - src.rag_pipeline - INFO - Processing validated query: 'Explain deep learning algorithms.'
2025-09-05 00:47:34,371 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain deep learning algorithms....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 60.81it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 57.13it/s, batch=1/1, memory=N/A]
2025-09-05 00:47:34,468 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:47:34,469 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:47:34,516 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.145s
2025-09-05 00:47:34,517 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:47:34,517 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1198 tokens) in 0.001s
2025-09-05 00:47:39,309 - src.rag_pipeline - INFO - Generated response (47 tokens) in 4.792s
2025-09-05 00:47:39,384 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:47:39,417 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:47:39,420 - src.experiment_runner - INFO - Completed run 13/510: 5.38s
2025-09-05 00:47:39,420 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:47:39,420 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:47:39,420 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:47:39,420 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:47:39,420 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:47:39,420 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:47:39,420 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:47:39,424 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:47:39,424 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:47:39,424 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:47:39,424 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:47:39,425 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:47:39,425 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:47:39,844 - src.llm_wrapper - INFO - Model ready in 0.42s
2025-09-05 00:47:39,844 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:47:39,844 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:47:39,844 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:47:39,844 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:47:39,844 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:47:39,844 - src.rag_pipeline - INFO - Processing validated query: 'What are neural networks?'
2025-09-05 00:47:39,850 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are neural networks?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 61.64it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 57.82it/s, batch=1/1, memory=N/A]
2025-09-05 00:47:39,945 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:47:39,946 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:47:39,991 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.141s
2025-09-05 00:47:39,992 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:47:39,992 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1353 tokens) in 0.001s
2025-09-05 00:47:53,235 - src.rag_pipeline - INFO - Generated response (238 tokens) in 13.241s
2025-09-05 00:47:53,310 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:47:53,344 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:47:53,348 - src.experiment_runner - INFO - Completed run 14/510: 13.82s
2025-09-05 00:47:53,348 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:47:53,348 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:47:53,348 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:47:53,348 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:47:53,348 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:47:53,348 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:47:53,348 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:47:53,352 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:47:53,353 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:47:53,353 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:47:53,353 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:47:53,353 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:47:53,353 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:47:53,773 - src.llm_wrapper - INFO - Model ready in 0.42s
2025-09-05 00:47:53,773 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:47:53,773 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:47:53,773 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:47:53,773 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:47:53,773 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:47:53,773 - src.rag_pipeline - INFO - Processing validated query: 'How do large language models work?'
2025-09-05 00:47:53,779 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How do large language models work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 63.08it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 59.50it/s, batch=1/1, memory=N/A]
2025-09-05 00:47:53,881 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:47:53,882 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:47:53,930 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.151s
2025-09-05 00:47:53,931 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:47:53,931 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1337 tokens) in 0.001s
2025-09-05 00:47:59,038 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.106s
2025-09-05 00:47:59,120 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:47:59,158 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:47:59,161 - src.experiment_runner - INFO - Completed run 15/510: 5.69s
2025-09-05 00:47:59,162 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:47:59,162 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:47:59,162 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:47:59,162 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:47:59,162 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:47:59,162 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:47:59,162 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:47:59,166 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:47:59,166 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:47:59,166 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:47:59,166 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:47:59,166 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:47:59,166 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:47:59,602 - src.llm_wrapper - INFO - Model ready in 0.44s
2025-09-05 00:47:59,602 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:47:59,602 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:47:59,602 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:47:59,602 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:47:59,602 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:47:59,602 - src.rag_pipeline - INFO - Processing validated query: 'What is natural language processing?'
2025-09-05 00:47:59,608 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is natural language processing?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 62.05it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 58.55it/s, batch=1/1, memory=N/A]
2025-09-05 00:47:59,695 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:47:59,696 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:47:59,743 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.135s
2025-09-05 00:47:59,744 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:47:59,744 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1749 tokens) in 0.001s
2025-09-05 00:48:06,271 - src.rag_pipeline - INFO - Generated response (55 tokens) in 6.526s
2025-09-05 00:48:06,351 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:48:06,387 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:48:06,390 - src.experiment_runner - INFO - Completed run 16/510: 7.11s
2025-09-05 00:48:06,390 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:48:06,390 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:48:06,390 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:48:06,390 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:48:06,390 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:48:06,390 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:48:06,390 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:48:06,394 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:48:06,395 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:48:06,395 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:48:06,395 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:48:06,395 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:48:06,395 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:48:06,826 - src.llm_wrapper - INFO - Model ready in 0.43s
2025-09-05 00:48:06,826 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:48:06,826 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:48:06,826 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:48:06,826 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:48:06,826 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:48:06,826 - src.rag_pipeline - INFO - Processing validated query: 'Explain computer vision techniques.'
2025-09-05 00:48:06,833 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain computer vision techniques....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 41.45it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 39.86it/s, batch=1/1, memory=N/A]
2025-09-05 00:48:06,942 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:48:06,942 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:48:06,990 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.157s
2025-09-05 00:48:06,991 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:48:06,991 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1297 tokens) in 0.001s
2025-09-05 00:48:23,572 - src.rag_pipeline - INFO - Generated response (318 tokens) in 16.580s
2025-09-05 00:48:23,652 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:48:23,692 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:48:23,696 - src.experiment_runner - INFO - Completed run 17/510: 17.18s
2025-09-05 00:48:23,696 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:48:23,696 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:48:23,696 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:48:23,696 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:48:23,696 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:48:23,696 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:48:23,696 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:48:23,701 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:48:23,701 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:48:23,701 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:48:23,701 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:48:23,701 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:48:23,701 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:48:24,126 - src.llm_wrapper - INFO - Model ready in 0.42s
2025-09-05 00:48:24,126 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:48:24,126 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:48:24,126 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:48:24,126 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:48:24,126 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:48:24,126 - src.rag_pipeline - INFO - Processing validated query: 'What is reinforcement learning?'
2025-09-05 00:48:24,133 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is reinforcement learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 60.92it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 57.17it/s, batch=1/1, memory=N/A]
2025-09-05 00:48:24,223 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:48:24,224 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:48:24,267 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.134s
2025-09-05 00:48:24,268 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:48:24,268 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1236 tokens) in 0.001s
2025-09-05 00:48:34,795 - src.rag_pipeline - INFO - Generated response (180 tokens) in 10.526s
2025-09-05 00:48:34,877 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:48:34,912 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:48:34,915 - src.experiment_runner - INFO - Completed run 18/510: 11.10s
2025-09-05 00:48:34,915 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:48:34,915 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:48:34,915 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:48:34,915 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:48:34,915 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:48:34,915 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:48:34,915 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:48:34,920 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:48:34,920 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:48:34,920 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:48:34,920 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:48:34,920 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:48:34,920 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:48:35,348 - src.llm_wrapper - INFO - Model ready in 0.43s
2025-09-05 00:48:35,348 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:48:35,348 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:48:35,348 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:48:35,348 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:48:35,348 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:48:35,348 - src.rag_pipeline - INFO - Processing validated query: 'How does data preprocessing work?'
2025-09-05 00:48:35,354 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does data preprocessing work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 63.29it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 59.56it/s, batch=1/1, memory=N/A]
2025-09-05 00:48:35,444 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:48:35,445 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:48:35,491 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.137s
2025-09-05 00:48:35,492 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:48:35,492 - src.rag_pipeline - INFO - Built prompt with 5 contexts (909 tokens) in 0.001s
2025-09-05 00:48:43,395 - src.rag_pipeline - INFO - Generated response (139 tokens) in 7.901s
2025-09-05 00:48:43,479 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:48:43,519 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:48:43,523 - src.experiment_runner - INFO - Completed run 19/510: 8.48s
2025-09-05 00:48:43,523 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:48:43,523 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:48:43,523 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:48:43,523 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:48:43,523 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:48:43,523 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:48:43,523 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:48:43,528 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:48:43,528 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:48:43,528 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:48:43,528 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:48:43,528 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:48:43,528 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:48:43,949 - src.llm_wrapper - INFO - Model ready in 0.42s
2025-09-05 00:48:43,949 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:48:43,949 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:48:43,949 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:48:43,949 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:48:43,949 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:48:43,949 - src.rag_pipeline - INFO - Processing validated query: 'What are the challenges in AI development?'
2025-09-05 00:48:43,955 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are the challenges in AI development?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 64.55it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 60.69it/s, batch=1/1, memory=N/A]
2025-09-05 00:48:44,044 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:48:44,045 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:48:44,088 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.133s
2025-09-05 00:48:44,089 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:48:44,089 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1445 tokens) in 0.001s
2025-09-05 00:48:57,620 - src.rag_pipeline - INFO - Generated response (237 tokens) in 13.530s
2025-09-05 00:48:57,695 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:48:57,731 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:48:57,734 - src.experiment_runner - INFO - Completed run 20/510: 14.10s
2025-09-05 00:48:57,735 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:48:57,735 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:48:57,735 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:48:57,735 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:48:57,735 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:48:57,735 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:48:57,735 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:48:57,740 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:48:57,740 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:48:57,740 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:48:57,740 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:48:57,740 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:48:57,740 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:48:58,173 - src.llm_wrapper - INFO - Model ready in 0.43s
2025-09-05 00:48:58,173 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:48:58,173 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:48:58,173 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:48:58,173 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:48:58,173 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:48:58,173 - src.rag_pipeline - INFO - Processing validated query: 'What is machine learning?'
2025-09-05 00:48:58,180 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is machine learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 55.96it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 52.73it/s, batch=1/1, memory=N/A]
2025-09-05 00:48:58,276 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:48:58,277 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:48:58,321 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.140s
2025-09-05 00:48:58,321 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:48:58,322 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1162 tokens) in 0.001s
2025-09-05 00:49:03,548 - src.rag_pipeline - INFO - Generated response (59 tokens) in 5.225s
2025-09-05 00:49:03,621 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:49:03,657 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:49:03,660 - src.experiment_runner - INFO - Completed run 21/510: 5.81s
2025-09-05 00:49:03,660 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:49:03,660 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:49:03,660 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:49:03,661 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:49:03,661 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:49:03,661 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:49:03,661 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:49:03,665 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:49:03,665 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:49:03,665 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:49:03,665 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:49:03,665 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:49:03,665 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:49:04,096 - src.llm_wrapper - INFO - Model ready in 0.43s
2025-09-05 00:49:04,096 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:49:04,096 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:49:04,096 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:49:04,096 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:49:04,096 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:49:04,096 - src.rag_pipeline - INFO - Processing validated query: 'How does artificial intelligence work?'
2025-09-05 00:49:04,102 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does artificial intelligence work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 60.16it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 56.64it/s, batch=1/1, memory=N/A]
2025-09-05 00:49:04,206 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:49:04,207 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:49:04,252 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.150s
2025-09-05 00:49:04,253 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:49:04,253 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1211 tokens) in 0.001s
2025-09-05 00:49:19,220 - src.rag_pipeline - INFO - Generated response (287 tokens) in 14.965s
2025-09-05 00:49:19,293 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:49:19,327 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:49:19,330 - src.experiment_runner - INFO - Completed run 22/510: 15.56s
2025-09-05 00:49:19,330 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:49:19,330 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:49:19,330 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:49:19,331 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:49:19,331 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:49:19,331 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:49:19,331 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:49:19,335 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:49:19,335 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:49:19,335 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:49:19,335 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:49:19,336 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:49:19,336 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:49:19,753 - src.llm_wrapper - INFO - Model ready in 0.42s
2025-09-05 00:49:19,753 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:49:19,753 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:49:19,753 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:49:19,753 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:49:19,753 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:49:19,753 - src.rag_pipeline - INFO - Processing validated query: 'Explain deep learning algorithms.'
2025-09-05 00:49:19,760 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain deep learning algorithms....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 57.29it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 54.15it/s, batch=1/1, memory=N/A]
2025-09-05 00:49:19,857 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:49:19,858 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:49:19,905 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.145s
2025-09-05 00:49:19,906 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:49:19,906 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1198 tokens) in 0.001s
2025-09-05 00:49:24,698 - src.rag_pipeline - INFO - Generated response (47 tokens) in 4.791s
2025-09-05 00:49:24,772 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:49:24,803 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:49:24,806 - src.experiment_runner - INFO - Completed run 23/510: 5.37s
2025-09-05 00:49:24,806 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:49:24,806 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:49:24,806 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:49:24,806 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:49:24,806 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:49:24,806 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:49:24,806 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:49:24,811 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:49:24,811 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:49:24,811 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:49:24,811 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:49:24,811 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:49:24,811 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:49:25,246 - src.llm_wrapper - INFO - Model ready in 0.43s
2025-09-05 00:49:25,246 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:49:25,246 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:49:25,246 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:49:25,246 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:49:25,246 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:49:25,246 - src.rag_pipeline - INFO - Processing validated query: 'What are neural networks?'
2025-09-05 00:49:25,253 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are neural networks?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 61.70it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 57.92it/s, batch=1/1, memory=N/A]
2025-09-05 00:49:25,348 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:49:25,349 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:49:25,395 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.142s
2025-09-05 00:49:25,396 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:49:25,396 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1353 tokens) in 0.001s
2025-09-05 00:49:38,623 - src.rag_pipeline - INFO - Generated response (238 tokens) in 13.226s
2025-09-05 00:49:38,699 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:49:38,731 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:49:38,734 - src.experiment_runner - INFO - Completed run 24/510: 13.82s
2025-09-05 00:49:38,734 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:49:38,734 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:49:38,734 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:49:38,735 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:49:38,735 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:49:38,735 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:49:38,735 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:49:38,740 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:49:38,740 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:49:38,740 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:49:38,740 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:49:38,740 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:49:38,740 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:49:39,172 - src.llm_wrapper - INFO - Model ready in 0.43s
2025-09-05 00:49:39,172 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:49:39,172 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:49:39,172 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:49:39,172 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:49:39,172 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:49:39,172 - src.rag_pipeline - INFO - Processing validated query: 'How do large language models work?'
2025-09-05 00:49:39,178 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How do large language models work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 63.35it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 59.61it/s, batch=1/1, memory=N/A]
2025-09-05 00:49:39,272 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:49:39,273 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:49:39,319 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.140s
2025-09-05 00:49:39,319 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:49:39,320 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1337 tokens) in 0.001s
2025-09-05 00:49:44,416 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.095s
2025-09-05 00:49:44,499 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:49:44,535 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:49:44,539 - src.experiment_runner - INFO - Completed run 25/510: 5.68s
2025-09-05 00:49:44,539 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:49:44,539 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:49:44,539 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:49:44,539 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:49:44,539 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:49:44,539 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:49:44,539 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:49:44,543 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:49:44,544 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:49:44,545 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:49:44,545 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:49:44,545 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:49:44,545 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:49:44,976 - src.llm_wrapper - INFO - Model ready in 0.43s
2025-09-05 00:49:44,976 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:49:44,976 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:49:44,976 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:49:44,976 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:49:44,976 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:49:44,976 - src.rag_pipeline - INFO - Processing validated query: 'What is natural language processing?'
2025-09-05 00:49:44,982 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is natural language processing?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 61.94it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 58.50it/s, batch=1/1, memory=N/A]
2025-09-05 00:49:45,073 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:49:45,074 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:49:45,121 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.139s
2025-09-05 00:49:45,122 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:49:45,123 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1749 tokens) in 0.001s
2025-09-05 00:49:51,623 - src.rag_pipeline - INFO - Generated response (55 tokens) in 6.500s
2025-09-05 00:49:51,703 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:49:51,741 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:49:51,744 - src.experiment_runner - INFO - Completed run 26/510: 7.08s
2025-09-05 00:49:51,744 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:49:51,744 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:49:51,744 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:49:51,744 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:49:51,744 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:49:51,744 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:49:51,744 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:49:51,749 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:49:51,749 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:49:51,749 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:49:51,749 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:49:51,749 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:49:51,749 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:49:52,174 - src.llm_wrapper - INFO - Model ready in 0.42s
2025-09-05 00:49:52,174 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:49:52,174 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:49:52,174 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:49:52,174 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:49:52,174 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:49:52,174 - src.rag_pipeline - INFO - Processing validated query: 'Explain computer vision techniques.'
2025-09-05 00:49:52,180 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain computer vision techniques....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 61.35it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 58.00it/s, batch=1/1, memory=N/A]
2025-09-05 00:49:52,281 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:49:52,282 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:49:52,327 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.147s
2025-09-05 00:49:52,328 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:49:52,329 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1297 tokens) in 0.001s
2025-09-05 00:50:08,908 - src.rag_pipeline - INFO - Generated response (318 tokens) in 16.578s
2025-09-05 00:50:08,994 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:50:09,034 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:50:09,038 - src.experiment_runner - INFO - Completed run 27/510: 17.16s
2025-09-05 00:50:09,038 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:50:09,038 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:50:09,038 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:50:09,038 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:50:09,038 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:50:09,038 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:50:09,038 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:50:09,043 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:50:09,043 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:50:09,043 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:50:09,043 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:50:09,044 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:50:09,044 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:50:09,481 - src.llm_wrapper - INFO - Model ready in 0.44s
2025-09-05 00:50:09,482 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:50:09,482 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:50:09,482 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:50:09,482 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:50:09,482 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:50:09,482 - src.rag_pipeline - INFO - Processing validated query: 'What is reinforcement learning?'
2025-09-05 00:50:09,489 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is reinforcement learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 60.26it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 56.68it/s, batch=1/1, memory=N/A]
2025-09-05 00:50:09,583 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:50:09,583 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:50:09,627 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.138s
2025-09-05 00:50:09,628 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:50:09,628 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1236 tokens) in 0.001s
2025-09-05 00:50:20,124 - src.rag_pipeline - INFO - Generated response (180 tokens) in 10.496s
2025-09-05 00:50:20,208 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:50:20,249 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:50:20,253 - src.experiment_runner - INFO - Completed run 28/510: 11.09s
2025-09-05 00:50:20,253 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:50:20,253 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:50:20,253 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:50:20,253 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:50:20,253 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:50:20,253 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:50:20,253 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:50:20,258 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:50:20,258 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:50:20,258 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:50:20,258 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:50:20,258 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:50:20,258 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:50:20,687 - src.llm_wrapper - INFO - Model ready in 0.43s
2025-09-05 00:50:20,687 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:50:20,687 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:50:20,687 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:50:20,687 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:50:20,687 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:50:20,687 - src.rag_pipeline - INFO - Processing validated query: 'How does data preprocessing work?'
2025-09-05 00:50:20,694 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does data preprocessing work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 62.18it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 58.41it/s, batch=1/1, memory=N/A]
2025-09-05 00:50:20,780 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:50:20,781 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:50:20,825 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.131s
2025-09-05 00:50:20,825 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:50:20,825 - src.rag_pipeline - INFO - Built prompt with 5 contexts (909 tokens) in 0.001s
2025-09-05 00:50:28,762 - src.rag_pipeline - INFO - Generated response (139 tokens) in 7.936s
2025-09-05 00:50:28,839 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:50:28,878 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:50:28,881 - src.experiment_runner - INFO - Completed run 29/510: 8.51s
2025-09-05 00:50:28,881 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:50:28,881 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:50:28,881 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:50:28,881 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:50:28,881 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:50:28,881 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:50:28,881 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:50:28,886 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:50:28,886 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:50:28,886 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:50:28,886 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:50:28,886 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:50:28,887 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:50:29,310 - src.llm_wrapper - INFO - Model ready in 0.42s
2025-09-05 00:50:29,310 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:50:29,310 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:50:29,310 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:50:29,310 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:50:29,310 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:50:29,310 - src.rag_pipeline - INFO - Processing validated query: 'What are the challenges in AI development?'
2025-09-05 00:50:29,317 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are the challenges in AI development?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 61.66it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 57.96it/s, batch=1/1, memory=N/A]
2025-09-05 00:50:29,409 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:50:29,410 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:50:29,454 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.137s
2025-09-05 00:50:29,454 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:50:29,455 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1445 tokens) in 0.001s
2025-09-05 00:50:42,992 - src.rag_pipeline - INFO - Generated response (237 tokens) in 13.535s
2025-09-05 00:50:43,070 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:50:43,105 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:50:43,109 - src.experiment_runner - INFO - Completed run 30/510: 14.11s
2025-09-05 00:50:43,109 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:50:43,109 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:50:43,109 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:50:43,109 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:50:43,109 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:50:43,109 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:50:43,109 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:50:43,114 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:50:43,114 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:50:43,114 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:50:43,114 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:50:43,114 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:50:43,114 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:50:43,544 - src.llm_wrapper - INFO - Model ready in 0.43s
2025-09-05 00:50:43,544 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:50:43,544 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:50:43,544 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:50:43,544 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:50:43,544 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:50:43,544 - src.rag_pipeline - INFO - Processing validated query: 'What is machine learning?'
2025-09-05 00:50:43,550 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is machine learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 65.50it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 61.29it/s, batch=1/1, memory=N/A]
2025-09-05 00:50:43,655 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:50:43,656 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:50:43,703 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.153s
2025-09-05 00:50:43,704 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:50:43,704 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1162 tokens) in 0.001s
2025-09-05 00:50:48,915 - src.rag_pipeline - INFO - Generated response (59 tokens) in 5.210s
2025-09-05 00:50:48,997 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:50:49,032 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:50:49,036 - src.experiment_runner - INFO - Completed run 31/510: 5.81s
2025-09-05 00:50:49,036 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:50:49,036 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:50:49,036 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:50:49,036 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:50:49,036 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:50:49,036 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:50:49,036 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:50:49,041 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:50:49,041 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:50:49,041 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:50:49,041 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:50:49,041 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:50:49,041 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:50:49,467 - src.llm_wrapper - INFO - Model ready in 0.43s
2025-09-05 00:50:49,467 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:50:49,467 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:50:49,467 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:50:49,467 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:50:49,467 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:50:49,467 - src.rag_pipeline - INFO - Processing validated query: 'How does artificial intelligence work?'
2025-09-05 00:50:49,474 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does artificial intelligence work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 67.76it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 63.45it/s, batch=1/1, memory=N/A]
2025-09-05 00:50:49,559 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:50:49,560 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:50:49,605 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.132s
2025-09-05 00:50:49,606 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:50:49,606 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1211 tokens) in 0.001s
2025-09-05 00:51:04,726 - src.rag_pipeline - INFO - Generated response (287 tokens) in 15.118s
2025-09-05 00:51:04,805 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:51:04,841 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:51:04,847 - src.experiment_runner - INFO - Completed run 32/510: 15.69s
2025-09-05 00:51:04,847 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:51:04,847 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:51:04,847 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:51:04,847 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:51:04,847 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:51:04,847 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:51:04,847 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:51:04,852 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:51:04,853 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:51:04,853 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:51:04,853 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:51:04,853 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:51:04,853 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:51:05,306 - src.llm_wrapper - INFO - Model ready in 0.45s
2025-09-05 00:51:05,306 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:51:05,306 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:51:05,306 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:51:05,306 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:51:05,306 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:51:05,306 - src.rag_pipeline - INFO - Processing validated query: 'Explain deep learning algorithms.'
2025-09-05 00:51:05,313 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain deep learning algorithms....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 52.90it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 50.20it/s, batch=1/1, memory=N/A]
2025-09-05 00:51:05,411 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:51:05,412 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:51:05,590 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.277s
2025-09-05 00:51:05,591 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:51:05,591 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1198 tokens) in 0.001s
2025-09-05 00:51:10,456 - src.rag_pipeline - INFO - Generated response (47 tokens) in 4.864s
2025-09-05 00:51:10,532 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:51:10,573 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:51:10,577 - src.experiment_runner - INFO - Completed run 33/510: 5.61s
2025-09-05 00:51:10,577 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:51:10,577 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:51:10,577 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:51:10,577 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:51:10,577 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:51:10,577 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:51:10,577 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:51:10,582 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:51:10,582 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:51:10,582 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:51:10,582 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:51:10,582 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:51:10,582 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:51:11,023 - src.llm_wrapper - INFO - Model ready in 0.44s
2025-09-05 00:51:11,023 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:51:11,023 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:51:11,023 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:51:11,023 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:51:11,023 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:51:11,023 - src.rag_pipeline - INFO - Processing validated query: 'What are neural networks?'
2025-09-05 00:51:11,030 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are neural networks?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 58.52it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 55.08it/s, batch=1/1, memory=N/A]
2025-09-05 00:51:11,129 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:51:11,130 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:51:11,174 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.144s
2025-09-05 00:51:11,174 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:51:11,175 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1353 tokens) in 0.001s
2025-09-05 00:51:24,870 - src.rag_pipeline - INFO - Generated response (238 tokens) in 13.694s
2025-09-05 00:51:24,955 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:51:24,991 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:51:24,995 - src.experiment_runner - INFO - Completed run 34/510: 14.29s
2025-09-05 00:51:24,995 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:51:24,995 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:51:24,995 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:51:24,995 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:51:24,995 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:51:24,995 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:51:24,995 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:51:25,000 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:51:25,000 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:51:25,000 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:51:25,000 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:51:25,000 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:51:25,000 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:51:25,460 - src.llm_wrapper - INFO - Model ready in 0.46s
2025-09-05 00:51:25,460 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:51:25,460 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:51:25,460 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:51:25,460 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:51:25,460 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:51:25,460 - src.rag_pipeline - INFO - Processing validated query: 'How do large language models work?'
2025-09-05 00:51:25,466 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How do large language models work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 56.61it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 53.58it/s, batch=1/1, memory=N/A]
2025-09-05 00:51:25,568 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:51:25,569 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:51:25,618 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.151s
2025-09-05 00:51:25,618 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:51:25,619 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1337 tokens) in 0.001s
2025-09-05 00:51:30,943 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.323s
2025-09-05 00:51:31,024 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:51:31,058 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:51:31,063 - src.experiment_runner - INFO - Completed run 35/510: 5.95s
2025-09-05 00:51:31,063 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:51:31,063 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:51:31,063 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:51:31,063 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:51:31,063 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:51:31,063 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:51:31,063 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:51:31,068 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:51:31,068 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:51:31,068 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:51:31,068 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:51:31,068 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:51:31,068 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:51:31,514 - src.llm_wrapper - INFO - Model ready in 0.45s
2025-09-05 00:51:31,514 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:51:31,514 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:51:31,514 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:51:31,514 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:51:31,515 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:51:31,515 - src.rag_pipeline - INFO - Processing validated query: 'What is natural language processing?'
2025-09-05 00:51:31,521 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is natural language processing?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 53.16it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 50.09it/s, batch=1/1, memory=N/A]
2025-09-05 00:51:31,626 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:51:31,627 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:51:31,816 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.294s
2025-09-05 00:51:31,816 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:51:31,817 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1749 tokens) in 0.001s
2025-09-05 00:51:38,561 - src.rag_pipeline - INFO - Generated response (55 tokens) in 6.743s
2025-09-05 00:51:38,647 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:51:38,684 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:51:38,688 - src.experiment_runner - INFO - Completed run 36/510: 7.50s
2025-09-05 00:51:38,688 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:51:38,688 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:51:38,688 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:51:38,688 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:51:38,688 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:51:38,688 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:51:38,688 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:51:38,693 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:51:38,693 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:51:38,693 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:51:38,693 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:51:38,693 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:51:38,693 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:51:39,131 - src.llm_wrapper - INFO - Model ready in 0.44s
2025-09-05 00:51:39,132 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:51:39,132 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:51:39,132 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:51:39,132 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:51:39,132 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:51:39,132 - src.rag_pipeline - INFO - Processing validated query: 'Explain computer vision techniques.'
2025-09-05 00:51:39,138 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain computer vision techniques....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 60.71it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 57.25it/s, batch=1/1, memory=N/A]
2025-09-05 00:51:39,234 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:51:39,235 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:51:39,282 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.143s
2025-09-05 00:51:39,282 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:51:39,283 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1297 tokens) in 0.001s
2025-09-05 00:51:56,541 - src.rag_pipeline - INFO - Generated response (318 tokens) in 17.257s
2025-09-05 00:51:56,620 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:51:56,661 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:51:56,666 - src.experiment_runner - INFO - Completed run 37/510: 17.85s
2025-09-05 00:51:56,666 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:51:56,666 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:51:56,666 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:51:56,666 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:51:56,666 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:51:56,666 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:51:56,666 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:51:56,672 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:51:56,672 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:51:56,672 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:51:56,672 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:51:56,672 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:51:56,672 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:51:57,109 - src.llm_wrapper - INFO - Model ready in 0.44s
2025-09-05 00:51:57,110 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:51:57,110 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:51:57,110 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:51:57,110 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:51:57,110 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:51:57,110 - src.rag_pipeline - INFO - Processing validated query: 'What is reinforcement learning?'
2025-09-05 00:51:57,117 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is reinforcement learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 56.84it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 53.66it/s, batch=1/1, memory=N/A]
2025-09-05 00:51:57,218 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:51:57,219 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:51:57,392 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.275s
2025-09-05 00:51:57,393 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:51:57,393 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1236 tokens) in 0.001s
2025-09-05 00:52:08,266 - src.rag_pipeline - INFO - Generated response (180 tokens) in 10.872s
2025-09-05 00:52:08,354 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:52:08,394 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:52:08,400 - src.experiment_runner - INFO - Completed run 38/510: 11.60s
2025-09-05 00:52:08,400 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:52:08,400 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:52:08,400 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:52:08,400 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:52:08,400 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:52:08,400 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:52:08,400 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:52:08,405 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:52:08,405 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:52:08,405 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:52:08,406 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:52:08,406 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:52:08,406 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:52:08,858 - src.llm_wrapper - INFO - Model ready in 0.45s
2025-09-05 00:52:08,858 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:52:08,858 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:52:08,858 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:52:08,858 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:52:08,858 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:52:08,858 - src.rag_pipeline - INFO - Processing validated query: 'How does data preprocessing work?'
2025-09-05 00:52:08,864 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does data preprocessing work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 60.24it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 56.41it/s, batch=1/1, memory=N/A]
2025-09-05 00:52:08,954 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:52:08,955 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:52:09,126 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.262s
2025-09-05 00:52:09,127 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:52:09,127 - src.rag_pipeline - INFO - Built prompt with 5 contexts (909 tokens) in 0.001s
2025-09-05 00:52:17,345 - src.rag_pipeline - INFO - Generated response (139 tokens) in 8.217s
2025-09-05 00:52:17,424 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:52:17,464 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:52:17,468 - src.experiment_runner - INFO - Completed run 39/510: 8.94s
2025-09-05 00:52:17,468 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:52:17,468 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:52:17,468 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:52:17,468 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:52:17,468 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:52:17,468 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:52:17,468 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:52:17,473 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:52:17,473 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:52:17,473 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:52:17,473 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:52:17,474 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:52:17,474 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:52:17,920 - src.llm_wrapper - INFO - Model ready in 0.45s
2025-09-05 00:52:17,920 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:52:17,920 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:52:17,920 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:52:17,920 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:52:17,920 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:52:17,920 - src.rag_pipeline - INFO - Processing validated query: 'What are the challenges in AI development?'
2025-09-05 00:52:17,927 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are the challenges in AI development?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 55.80it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 52.60it/s, batch=1/1, memory=N/A]
2025-09-05 00:52:18,026 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:52:18,027 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:52:18,076 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.149s
2025-09-05 00:52:18,077 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:52:18,077 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1445 tokens) in 0.001s
2025-09-05 00:52:32,113 - src.rag_pipeline - INFO - Generated response (237 tokens) in 14.034s
2025-09-05 00:52:32,202 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:52:32,244 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:52:32,250 - src.experiment_runner - INFO - Completed run 40/510: 14.65s
2025-09-05 00:52:32,250 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:52:32,250 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:52:32,250 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:52:32,250 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:52:32,250 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:52:32,250 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:52:32,250 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:52:32,255 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:52:32,256 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:52:32,256 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:52:32,256 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:52:32,256 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:52:32,256 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:52:32,715 - src.llm_wrapper - INFO - Model ready in 0.46s
2025-09-05 00:52:32,715 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:52:32,715 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:52:32,715 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:52:32,715 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:52:32,715 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:52:32,715 - src.rag_pipeline - INFO - Processing validated query: 'What is machine learning?'
2025-09-05 00:52:32,721 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is machine learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 60.35it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 56.99it/s, batch=1/1, memory=N/A]
2025-09-05 00:52:32,810 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:52:32,811 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:52:32,923 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.202s
2025-09-05 00:52:32,924 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:52:32,924 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1162 tokens) in 0.001s
2025-09-05 00:52:38,370 - src.rag_pipeline - INFO - Generated response (59 tokens) in 5.445s
2025-09-05 00:52:38,451 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:52:38,486 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:52:38,494 - src.experiment_runner - INFO - Completed run 41/510: 6.12s
2025-09-05 00:52:38,494 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:52:38,494 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:52:38,494 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:52:38,494 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:52:38,494 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:52:38,494 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:52:38,494 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:52:38,498 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:52:38,498 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:52:38,498 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:52:38,498 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:52:38,498 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:52:38,498 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:52:38,959 - src.llm_wrapper - INFO - Model ready in 0.46s
2025-09-05 00:52:38,959 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:52:38,959 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:52:38,959 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:52:38,959 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:52:38,959 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:52:38,959 - src.rag_pipeline - INFO - Processing validated query: 'How does artificial intelligence work?'
2025-09-05 00:52:38,966 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does artificial intelligence work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 54.06it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 50.96it/s, batch=1/1, memory=N/A]
2025-09-05 00:52:39,069 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:52:39,070 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:52:39,116 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.150s
2025-09-05 00:52:39,117 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:52:39,117 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1211 tokens) in 0.001s
2025-09-05 00:52:54,890 - src.rag_pipeline - INFO - Generated response (287 tokens) in 15.767s
2025-09-05 00:52:55,049 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:52:55,093 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:52:55,099 - src.experiment_runner - INFO - Completed run 42/510: 16.40s
2025-09-05 00:52:55,099 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:52:55,099 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:52:55,099 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:52:55,099 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:52:55,099 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:52:55,099 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:52:55,099 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:52:55,104 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:52:55,105 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:52:55,105 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:52:55,105 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:52:55,105 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:52:55,105 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:52:55,615 - src.llm_wrapper - INFO - Model ready in 0.51s
2025-09-05 00:52:55,615 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:52:55,615 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:52:55,615 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:52:55,615 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:52:55,615 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:52:55,615 - src.rag_pipeline - INFO - Processing validated query: 'Explain deep learning algorithms.'
2025-09-05 00:52:55,623 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain deep learning algorithms....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.19it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.19it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.15it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.15it/s, batch=1/1, memory=N/A]
2025-09-05 00:52:55,965 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:52:55,967 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:52:56,219 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.597s
2025-09-05 00:52:56,221 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:52:56,221 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1198 tokens) in 0.001s
2025-09-05 00:53:01,288 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.065s
2025-09-05 00:53:01,458 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:53:01,501 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:53:01,507 - src.experiment_runner - INFO - Completed run 43/510: 6.19s
2025-09-05 00:53:01,508 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:53:01,508 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:53:01,508 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:53:01,508 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:53:01,508 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:53:01,508 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:53:01,508 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:53:01,515 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:53:01,515 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:53:01,515 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:53:01,515 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:53:01,515 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:53:01,515 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:53:02,025 - src.llm_wrapper - INFO - Model ready in 0.51s
2025-09-05 00:53:02,025 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:53:02,025 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:53:02,025 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:53:02,025 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:53:02,025 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:53:02,025 - src.rag_pipeline - INFO - Processing validated query: 'What are neural networks?'
2025-09-05 00:53:02,032 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are neural networks?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 10.35it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 10.12it/s, batch=1/1, memory=N/A]
2025-09-05 00:53:02,224 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:53:02,225 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:53:02,467 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.435s
2025-09-05 00:53:02,469 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:53:02,470 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1353 tokens) in 0.002s
2025-09-05 00:53:17,077 - src.rag_pipeline - INFO - Generated response (238 tokens) in 14.604s
2025-09-05 00:53:17,265 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:53:17,309 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:53:17,316 - src.experiment_runner - INFO - Completed run 44/510: 15.57s
2025-09-05 00:53:17,317 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:53:17,317 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:53:17,317 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:53:17,317 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:53:17,317 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:53:17,317 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:53:17,317 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:53:17,324 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:53:17,324 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:53:17,324 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:53:17,324 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:53:17,325 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:53:17,325 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:53:17,826 - src.llm_wrapper - INFO - Model ready in 0.50s
2025-09-05 00:53:17,826 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:53:17,826 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:53:17,826 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:53:17,826 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:53:17,826 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:53:17,826 - src.rag_pipeline - INFO - Processing validated query: 'How do large language models work?'
2025-09-05 00:53:17,835 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How do large language models work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.75it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.74it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.71it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.71it/s, batch=1/1, memory=N/A]
2025-09-05 00:53:18,226 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:53:18,227 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:53:18,506 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.671s
2025-09-05 00:53:18,508 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:53:18,508 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1337 tokens) in 0.002s
2025-09-05 00:53:24,110 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.601s
2025-09-05 00:53:24,264 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:53:24,315 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:53:24,333 - src.experiment_runner - INFO - Completed run 45/510: 6.80s
2025-09-05 00:53:24,333 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:53:24,333 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:53:24,333 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:53:24,333 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:53:24,333 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:53:24,333 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:53:24,333 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:53:24,338 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:53:24,338 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:53:24,338 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:53:24,338 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:53:24,338 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:53:24,339 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:53:24,850 - src.llm_wrapper - INFO - Model ready in 0.51s
2025-09-05 00:53:24,850 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:53:24,850 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:53:24,850 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:53:24,850 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:53:24,850 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:53:24,850 - src.rag_pipeline - INFO - Processing validated query: 'What is natural language processing?'
2025-09-05 00:53:24,857 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is natural language processing?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.06it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.05it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.02it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.01it/s, batch=1/1, memory=N/A]
2025-09-05 00:53:25,161 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:53:25,163 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:53:25,513 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.656s
2025-09-05 00:53:25,517 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:53:25,517 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1749 tokens) in 0.004s
2025-09-05 00:53:32,831 - src.rag_pipeline - INFO - Generated response (55 tokens) in 7.313s
2025-09-05 00:53:32,977 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:53:33,026 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:53:33,032 - src.experiment_runner - INFO - Completed run 46/510: 8.50s
2025-09-05 00:53:33,032 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:53:33,032 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:53:33,032 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:53:33,032 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:53:33,032 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:53:33,032 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:53:33,032 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:53:33,037 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:53:33,038 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:53:33,038 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:53:33,038 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:53:33,038 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:53:33,038 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:53:33,519 - src.llm_wrapper - INFO - Model ready in 0.48s
2025-09-05 00:53:33,519 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:53:33,519 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:53:33,519 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:53:33,519 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:53:33,519 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:53:33,519 - src.rag_pipeline - INFO - Processing validated query: 'Explain computer vision techniques.'
2025-09-05 00:53:33,527 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain computer vision techniques....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.72it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.72it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.66it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.66it/s, batch=1/1, memory=N/A]
2025-09-05 00:53:33,867 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:53:33,868 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:53:34,145 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.618s
2025-09-05 00:53:34,148 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:53:34,149 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1297 tokens) in 0.004s
2025-09-05 00:53:52,559 - src.rag_pipeline - INFO - Generated response (318 tokens) in 18.405s
2025-09-05 00:53:52,768 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:53:52,815 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:53:52,827 - src.experiment_runner - INFO - Completed run 47/510: 19.53s
2025-09-05 00:53:52,827 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:53:52,827 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:53:52,827 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:53:52,827 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:53:52,827 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:53:52,827 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:53:52,827 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:53:52,832 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:53:52,832 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:53:52,832 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:53:52,832 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:53:52,832 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:53:52,833 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:53:53,340 - src.llm_wrapper - INFO - Model ready in 0.51s
2025-09-05 00:53:53,340 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:53:53,340 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:53:53,340 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:53:53,340 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:53:53,340 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:53:53,340 - src.rag_pipeline - INFO - Processing validated query: 'What is reinforcement learning?'
2025-09-05 00:53:53,362 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is reinforcement learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.41it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.40it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.35it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.35it/s, batch=1/1, memory=N/A]
2025-09-05 00:53:53,801 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:53:53,802 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:53:54,197 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.835s
2025-09-05 00:53:54,203 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:53:54,204 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1236 tokens) in 0.006s
2025-09-05 00:54:05,690 - src.rag_pipeline - INFO - Generated response (180 tokens) in 11.482s
2025-09-05 00:54:05,851 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:54:05,900 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:54:05,912 - src.experiment_runner - INFO - Completed run 48/510: 12.87s
2025-09-05 00:54:05,912 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:54:05,912 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:54:05,912 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:54:05,912 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:54:05,912 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:54:05,912 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:54:05,912 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:54:05,917 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:54:05,917 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:54:05,917 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:54:05,917 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:54:05,918 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:54:05,918 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:54:06,387 - src.llm_wrapper - INFO - Model ready in 0.47s
2025-09-05 00:54:06,388 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:54:06,388 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:54:06,388 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:54:06,388 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:54:06,388 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:54:06,388 - src.rag_pipeline - INFO - Processing validated query: 'How does data preprocessing work?'
2025-09-05 00:54:06,394 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does data preprocessing work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.19it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.19it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.16it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.16it/s, batch=1/1, memory=N/A]
2025-09-05 00:54:06,736 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:54:06,737 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:54:07,040 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.646s
2025-09-05 00:54:07,042 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:54:07,043 - src.rag_pipeline - INFO - Built prompt with 5 contexts (909 tokens) in 0.002s
2025-09-05 00:54:15,763 - src.rag_pipeline - INFO - Generated response (139 tokens) in 8.717s
2025-09-05 00:54:15,987 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:54:16,037 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:54:16,043 - src.experiment_runner - INFO - Completed run 49/510: 9.85s
2025-09-05 00:54:16,043 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:54:16,043 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:54:16,044 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:54:16,044 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:54:16,044 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:54:16,044 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:54:16,044 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:54:16,050 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:54:16,050 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:54:16,050 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:54:16,050 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:54:16,050 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:54:16,050 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:54:16,550 - src.llm_wrapper - INFO - Model ready in 0.50s
2025-09-05 00:54:16,550 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:54:16,550 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:54:16,550 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:54:16,550 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:54:16,550 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:54:16,550 - src.rag_pipeline - INFO - Processing validated query: 'What are the challenges in AI development?'
2025-09-05 00:54:16,557 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are the challenges in AI development?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.02it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.00it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.96it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.95it/s, batch=1/1, memory=N/A]
2025-09-05 00:54:17,118 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:54:17,120 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:54:17,470 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.913s
2025-09-05 00:54:17,479 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:54:17,479 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1445 tokens) in 0.007s
2025-09-05 00:54:32,645 - src.rag_pipeline - INFO - Generated response (237 tokens) in 15.161s
2025-09-05 00:54:32,832 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:54:32,879 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:54:32,887 - src.experiment_runner - INFO - Completed run 50/510: 16.60s
2025-09-05 00:54:32,887 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:54:32,887 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:54:32,887 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:54:32,887 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:54:32,887 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:54:32,887 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:54:32,887 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:54:32,892 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:54:32,892 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:54:32,892 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:54:32,892 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:54:32,893 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:54:32,893 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:54:33,407 - src.llm_wrapper - INFO - Model ready in 0.51s
2025-09-05 00:54:33,407 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:54:33,407 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:54:33,407 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:54:33,407 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:54:33,407 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:54:33,407 - src.rag_pipeline - INFO - Processing validated query: 'What is machine learning?'
2025-09-05 00:54:33,423 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is machine learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.60it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.60it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.56it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.56it/s, batch=1/1, memory=N/A]
2025-09-05 00:54:34,080 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:54:34,083 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:54:34,392 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.969s
2025-09-05 00:54:34,402 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:54:34,402 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1162 tokens) in 0.009s
2025-09-05 00:54:40,089 - src.rag_pipeline - INFO - Generated response (59 tokens) in 5.683s
2025-09-05 00:54:40,257 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:54:40,310 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:54:40,315 - src.experiment_runner - INFO - Completed run 51/510: 7.20s
2025-09-05 00:54:40,315 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:54:40,315 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:54:40,316 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:54:40,316 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:54:40,316 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:54:40,316 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:54:40,316 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:54:40,321 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:54:40,321 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:54:40,321 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:54:40,321 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:54:40,322 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:54:40,322 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:54:40,832 - src.llm_wrapper - INFO - Model ready in 0.51s
2025-09-05 00:54:40,832 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:54:40,832 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:54:40,832 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:54:40,832 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:54:40,832 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:54:40,832 - src.rag_pipeline - INFO - Processing validated query: 'How does artificial intelligence work?'
2025-09-05 00:54:40,840 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does artificial intelligence work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.77it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.76it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.75it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.75it/s, batch=1/1, memory=N/A]
2025-09-05 00:54:41,457 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:54:41,458 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:54:41,773 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.932s
2025-09-05 00:54:41,781 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:54:41,781 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1211 tokens) in 0.007s
2025-09-05 00:54:58,506 - src.rag_pipeline - INFO - Generated response (287 tokens) in 16.708s
2025-09-05 00:54:58,752 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:54:58,810 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:54:58,826 - src.experiment_runner - INFO - Completed run 52/510: 18.19s
2025-09-05 00:54:58,826 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:54:58,827 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:54:58,827 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:54:58,827 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:54:58,827 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:54:58,827 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:54:58,827 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:54:58,835 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:54:58,835 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:54:58,835 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:54:58,835 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:54:58,836 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:54:58,836 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:54:59,378 - src.llm_wrapper - INFO - Model ready in 0.54s
2025-09-05 00:54:59,379 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:54:59,379 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:54:59,379 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:54:59,379 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:54:59,379 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:54:59,379 - src.rag_pipeline - INFO - Processing validated query: 'Explain deep learning algorithms.'
2025-09-05 00:54:59,388 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain deep learning algorithms....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.64it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.63it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.61it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.61it/s, batch=1/1, memory=N/A]
2025-09-05 00:55:00,029 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:55:00,031 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:55:00,385 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.998s
2025-09-05 00:55:00,392 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:55:00,392 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1198 tokens) in 0.006s
2025-09-05 00:55:05,562 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.166s
2025-09-05 00:55:05,729 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:55:05,784 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:55:05,788 - src.experiment_runner - INFO - Completed run 53/510: 6.74s
2025-09-05 00:55:05,789 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:55:05,789 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:55:05,789 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:55:05,789 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:55:05,789 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:55:05,789 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:55:05,789 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:55:05,795 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:55:05,795 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:55:05,795 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:55:05,795 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:55:05,795 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:55:05,795 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:55:06,284 - src.llm_wrapper - INFO - Model ready in 0.49s
2025-09-05 00:55:06,284 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:55:06,284 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:55:06,284 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:55:06,284 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:55:06,284 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:55:06,284 - src.rag_pipeline - INFO - Processing validated query: 'What are neural networks?'
2025-09-05 00:55:06,291 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are neural networks?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.52it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.52it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.45it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.45it/s, batch=1/1, memory=N/A]
2025-09-05 00:55:06,651 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:55:06,653 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:55:06,950 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.659s
2025-09-05 00:55:06,954 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:55:06,956 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1353 tokens) in 0.005s
2025-09-05 00:55:21,484 - src.rag_pipeline - INFO - Generated response (238 tokens) in 14.523s
2025-09-05 00:55:21,698 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:55:21,745 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:55:21,759 - src.experiment_runner - INFO - Completed run 54/510: 15.70s
2025-09-05 00:55:21,759 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:55:21,759 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:55:21,759 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:55:21,759 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:55:21,759 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:55:21,759 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:55:21,759 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:55:21,764 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:55:21,764 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:55:21,764 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:55:21,764 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:55:21,765 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:55:21,765 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:55:22,325 - src.llm_wrapper - INFO - Model ready in 0.56s
2025-09-05 00:55:22,325 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:55:22,325 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:55:22,325 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:55:22,325 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:55:22,325 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:55:22,325 - src.rag_pipeline - INFO - Processing validated query: 'How do large language models work?'
2025-09-05 00:55:22,337 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How do large language models work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.47it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.44it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.41it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.41it/s, batch=1/1, memory=N/A]
2025-09-05 00:55:22,825 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:55:22,826 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:55:23,191 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.854s
2025-09-05 00:55:23,203 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:55:23,203 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1337 tokens) in 0.010s
2025-09-05 00:55:28,675 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.469s
2025-09-05 00:55:28,879 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:55:28,933 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:55:28,938 - src.experiment_runner - INFO - Completed run 55/510: 6.92s
2025-09-05 00:55:28,939 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:55:28,939 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:55:28,939 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:55:28,939 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:55:28,939 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:55:28,939 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:55:28,939 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:55:28,944 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:55:28,945 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:55:28,945 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:55:28,945 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:55:28,945 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:55:28,945 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:55:29,467 - src.llm_wrapper - INFO - Model ready in 0.52s
2025-09-05 00:55:29,467 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:55:29,467 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:55:29,467 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:55:29,467 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:55:29,467 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:55:29,467 - src.rag_pipeline - INFO - Processing validated query: 'What is natural language processing?'
2025-09-05 00:55:29,474 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is natural language processing?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.32it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.30it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.27it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.27it/s, batch=1/1, memory=N/A]
2025-09-05 00:55:30,015 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:55:30,018 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:55:30,343 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.868s
2025-09-05 00:55:30,352 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:55:30,353 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1749 tokens) in 0.008s
2025-09-05 00:55:37,367 - src.rag_pipeline - INFO - Generated response (55 tokens) in 7.011s
2025-09-05 00:55:37,551 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:55:37,606 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:55:37,610 - src.experiment_runner - INFO - Completed run 56/510: 8.43s
2025-09-05 00:55:37,610 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:55:37,610 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:55:37,610 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:55:37,610 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:55:37,610 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:55:37,610 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:55:37,610 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:55:37,617 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:55:37,617 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:55:37,617 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:55:37,617 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:55:37,617 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:55:37,617 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:55:38,124 - src.llm_wrapper - INFO - Model ready in 0.51s
2025-09-05 00:55:38,124 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:55:38,124 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:55:38,124 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:55:38,124 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:55:38,124 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:55:38,124 - src.rag_pipeline - INFO - Processing validated query: 'Explain computer vision techniques.'
2025-09-05 00:55:38,131 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain computer vision techniques....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.46it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.45it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.42it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.42it/s, batch=1/1, memory=N/A]
2025-09-05 00:55:38,585 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:55:38,589 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:55:38,896 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.764s
2025-09-05 00:55:38,900 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:55:38,901 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1297 tokens) in 0.003s
2025-09-05 00:55:57,281 - src.rag_pipeline - INFO - Generated response (318 tokens) in 18.373s
2025-09-05 00:55:57,470 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:55:57,528 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:55:57,535 - src.experiment_runner - INFO - Completed run 57/510: 19.67s
2025-09-05 00:55:57,535 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:55:57,535 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:55:57,535 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:55:57,535 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:55:57,535 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:55:57,535 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:55:57,535 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:55:57,540 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:55:57,540 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:55:57,541 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:55:57,541 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:55:57,541 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:55:57,541 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:55:58,047 - src.llm_wrapper - INFO - Model ready in 0.51s
2025-09-05 00:55:58,047 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:55:58,047 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:55:58,047 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:55:58,047 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:55:58,047 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:55:58,047 - src.rag_pipeline - INFO - Processing validated query: 'What is reinforcement learning?'
2025-09-05 00:55:58,055 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is reinforcement learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.37it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.37it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.34it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.34it/s, batch=1/1, memory=N/A]
2025-09-05 00:55:58,485 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:55:58,486 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:55:58,797 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.742s
2025-09-05 00:55:58,804 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:55:58,805 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1236 tokens) in 0.006s
2025-09-05 00:56:10,344 - src.rag_pipeline - INFO - Generated response (180 tokens) in 11.535s
2025-09-05 00:56:10,530 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:56:10,582 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:56:10,594 - src.experiment_runner - INFO - Completed run 58/510: 12.81s
2025-09-05 00:56:10,594 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:56:10,594 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:56:10,594 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:56:10,594 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:56:10,594 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:56:10,594 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:56:10,594 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:56:10,600 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:56:10,600 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:56:10,600 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:56:10,600 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:56:10,600 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:56:10,600 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:56:11,104 - src.llm_wrapper - INFO - Model ready in 0.50s
2025-09-05 00:56:11,104 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:56:11,104 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:56:11,104 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:56:11,104 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:56:11,104 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:56:11,104 - src.rag_pipeline - INFO - Processing validated query: 'How does data preprocessing work?'
2025-09-05 00:56:11,114 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does data preprocessing work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.51it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.50it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.49it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.49it/s, batch=1/1, memory=N/A]
2025-09-05 00:56:11,515 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:56:11,516 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:56:11,838 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.722s
2025-09-05 00:56:11,843 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:56:11,843 - src.rag_pipeline - INFO - Built prompt with 5 contexts (909 tokens) in 0.004s
2025-09-05 00:56:20,622 - src.rag_pipeline - INFO - Generated response (139 tokens) in 8.773s
2025-09-05 00:56:20,833 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:56:20,881 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:56:20,891 - src.experiment_runner - INFO - Completed run 59/510: 10.03s
2025-09-05 00:56:20,891 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:56:20,891 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:56:20,891 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:56:20,891 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:56:20,891 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:56:20,891 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:56:20,891 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:56:20,897 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:56:20,897 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:56:20,897 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:56:20,897 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:56:20,897 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:56:20,898 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:56:21,411 - src.llm_wrapper - INFO - Model ready in 0.51s
2025-09-05 00:56:21,411 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:56:21,411 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:56:21,411 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:56:21,411 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:56:21,411 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:56:21,411 - src.rag_pipeline - INFO - Processing validated query: 'What are the challenges in AI development?'
2025-09-05 00:56:21,418 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are the challenges in AI development?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.18it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.18it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.16it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.16it/s, batch=1/1, memory=N/A]
2025-09-05 00:56:21,928 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:56:21,929 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:56:22,244 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.826s
2025-09-05 00:56:22,254 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:56:22,254 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1445 tokens) in 0.008s
2025-09-05 00:56:37,164 - src.rag_pipeline - INFO - Generated response (237 tokens) in 14.904s
2025-09-05 00:56:37,393 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:56:37,447 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:56:37,453 - src.experiment_runner - INFO - Completed run 60/510: 16.27s
2025-09-05 00:56:37,454 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:56:37,454 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:56:37,454 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:56:37,454 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:56:37,454 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:56:37,454 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:56:37,454 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:56:37,459 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:56:37,459 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:56:37,459 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:56:37,459 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:56:37,460 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:56:37,460 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:56:38,002 - src.llm_wrapper - INFO - Model ready in 0.54s
2025-09-05 00:56:38,002 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:56:38,002 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:56:38,002 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:56:38,002 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:56:38,002 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:56:38,002 - src.rag_pipeline - INFO - Processing validated query: 'What is machine learning?'
2025-09-05 00:56:38,010 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is machine learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.56it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.55it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.51it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.51it/s, batch=1/1, memory=N/A]
2025-09-05 00:56:38,553 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:56:38,555 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:56:38,869 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.859s
2025-09-05 00:56:38,876 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:56:38,877 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1162 tokens) in 0.007s
2025-09-05 00:56:44,413 - src.rag_pipeline - INFO - Generated response (59 tokens) in 5.533s
2025-09-05 00:56:44,601 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:56:44,651 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:56:44,659 - src.experiment_runner - INFO - Completed run 61/510: 6.96s
2025-09-05 00:56:44,659 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:56:44,659 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:56:44,659 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:56:44,659 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:56:44,659 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:56:44,659 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:56:44,659 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:56:44,665 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:56:44,665 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:56:44,665 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:56:44,666 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:56:44,666 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:56:44,666 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:56:45,207 - src.llm_wrapper - INFO - Model ready in 0.54s
2025-09-05 00:56:45,207 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:56:45,207 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:56:45,207 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:56:45,207 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:56:45,207 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:56:45,207 - src.rag_pipeline - INFO - Processing validated query: 'How does artificial intelligence work?'
2025-09-05 00:56:45,215 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does artificial intelligence work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.29it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.26it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.22it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.21it/s, batch=1/1, memory=N/A]
2025-09-05 00:56:45,880 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:56:45,881 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:56:46,278 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.062s
2025-09-05 00:56:46,293 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:56:46,293 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1211 tokens) in 0.009s
2025-09-05 00:57:03,409 - src.rag_pipeline - INFO - Generated response (287 tokens) in 17.108s
2025-09-05 00:57:03,627 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:57:03,678 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:57:03,684 - src.experiment_runner - INFO - Completed run 62/510: 18.75s
2025-09-05 00:57:03,684 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:57:03,684 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:57:03,684 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:57:03,684 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:57:03,684 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:57:03,684 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:57:03,684 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:57:03,690 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:57:03,690 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:57:03,690 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:57:03,690 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:57:03,690 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:57:03,690 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:57:04,210 - src.llm_wrapper - INFO - Model ready in 0.52s
2025-09-05 00:57:04,210 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:57:04,210 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:57:04,210 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:57:04,210 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:57:04,210 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:57:04,210 - src.rag_pipeline - INFO - Processing validated query: 'Explain deep learning algorithms.'
2025-09-05 00:57:04,218 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain deep learning algorithms....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.77it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.77it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.73it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.73it/s, batch=1/1, memory=N/A]
2025-09-05 00:57:04,878 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:57:04,879 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:57:05,456 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.237s
2025-09-05 00:57:05,467 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:57:05,468 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1198 tokens) in 0.011s
2025-09-05 00:57:10,579 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.109s
2025-09-05 00:57:10,755 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:57:10,805 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:57:10,810 - src.experiment_runner - INFO - Completed run 63/510: 6.90s
2025-09-05 00:57:10,810 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:57:10,810 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:57:10,810 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:57:10,810 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:57:10,810 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:57:10,810 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:57:10,810 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:57:10,815 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:57:10,816 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:57:10,816 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:57:10,816 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:57:10,816 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:57:10,816 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:57:11,311 - src.llm_wrapper - INFO - Model ready in 0.49s
2025-09-05 00:57:11,311 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:57:11,311 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:57:11,311 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:57:11,311 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:57:11,311 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:57:11,311 - src.rag_pipeline - INFO - Processing validated query: 'What are neural networks?'
2025-09-05 00:57:11,322 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are neural networks?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.09it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.08it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.04it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.04it/s, batch=1/1, memory=N/A]
2025-09-05 00:57:11,883 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:57:11,885 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:57:12,439 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.115s
2025-09-05 00:57:12,446 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:57:12,446 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1353 tokens) in 0.005s
2025-09-05 00:57:26,823 - src.rag_pipeline - INFO - Generated response (238 tokens) in 14.371s
2025-09-05 00:57:27,006 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:57:27,059 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:57:27,070 - src.experiment_runner - INFO - Completed run 64/510: 16.01s
2025-09-05 00:57:27,070 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:57:27,070 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:57:27,070 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:57:27,070 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:57:27,070 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:57:27,070 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:57:27,070 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:57:27,079 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:57:27,080 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:57:27,080 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:57:27,080 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:57:27,080 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:57:27,080 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:57:27,589 - src.llm_wrapper - INFO - Model ready in 0.51s
2025-09-05 00:57:27,589 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:57:27,589 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:57:27,589 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:57:27,589 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:57:27,589 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:57:27,589 - src.rag_pipeline - INFO - Processing validated query: 'How do large language models work?'
2025-09-05 00:57:27,597 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How do large language models work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.21it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.21it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.19it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.19it/s, batch=1/1, memory=N/A]
2025-09-05 00:57:28,128 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:57:28,129 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:57:28,679 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.082s
2025-09-05 00:57:28,684 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:57:28,685 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1337 tokens) in 0.005s
2025-09-05 00:57:34,077 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.389s
2025-09-05 00:57:34,245 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:57:34,295 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:57:34,301 - src.experiment_runner - INFO - Completed run 65/510: 7.01s
2025-09-05 00:57:34,301 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:57:34,301 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:57:34,301 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:57:34,302 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:57:34,302 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:57:34,302 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:57:34,302 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:57:34,307 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:57:34,307 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:57:34,308 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:57:34,308 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:57:34,308 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:57:34,308 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:57:34,819 - src.llm_wrapper - INFO - Model ready in 0.51s
2025-09-05 00:57:34,819 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:57:34,819 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:57:34,819 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:57:34,819 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:57:34,819 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:57:34,819 - src.rag_pipeline - INFO - Processing validated query: 'What is natural language processing?'
2025-09-05 00:57:34,825 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is natural language processing?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.64it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.63it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.60it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.60it/s, batch=1/1, memory=N/A]
2025-09-05 00:57:35,152 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:57:35,153 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:57:35,680 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.855s
2025-09-05 00:57:35,687 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:57:35,688 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1749 tokens) in 0.007s
2025-09-05 00:57:42,709 - src.rag_pipeline - INFO - Generated response (55 tokens) in 7.016s
2025-09-05 00:57:42,917 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:57:42,975 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:57:42,981 - src.experiment_runner - INFO - Completed run 66/510: 8.41s
2025-09-05 00:57:42,981 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:57:42,981 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:57:42,981 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:57:42,981 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:57:42,981 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:57:42,981 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:57:42,981 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:57:42,987 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:57:42,988 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:57:42,988 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:57:42,988 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:57:42,988 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:57:42,988 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:57:43,503 - src.llm_wrapper - INFO - Model ready in 0.51s
2025-09-05 00:57:43,503 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:57:43,503 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:57:43,503 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:57:43,503 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:57:43,503 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:57:43,503 - src.rag_pipeline - INFO - Processing validated query: 'Explain computer vision techniques.'
2025-09-05 00:57:43,512 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain computer vision techniques....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.43it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.40it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.36it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.36it/s, batch=1/1, memory=N/A]
2025-09-05 00:57:44,078 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:57:44,080 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:57:44,734 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.219s
2025-09-05 00:57:44,749 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:57:44,750 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1297 tokens) in 0.012s
2025-09-05 00:58:03,983 - src.rag_pipeline - INFO - Generated response (318 tokens) in 19.223s
2025-09-05 00:58:04,215 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:58:04,271 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:58:04,282 - src.experiment_runner - INFO - Completed run 67/510: 21.00s
2025-09-05 00:58:04,282 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:58:04,282 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:58:04,282 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:58:04,282 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:58:04,282 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:58:04,282 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:58:04,282 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:58:04,288 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:58:04,288 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:58:04,288 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:58:04,288 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:58:04,288 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:58:04,288 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:58:04,850 - src.llm_wrapper - INFO - Model ready in 0.56s
2025-09-05 00:58:04,850 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:58:04,850 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:58:04,850 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:58:04,850 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:58:04,850 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:58:04,850 - src.rag_pipeline - INFO - Processing validated query: 'What is reinforcement learning?'
2025-09-05 00:58:04,862 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is reinforcement learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.84it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.84it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.81it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.81it/s, batch=1/1, memory=N/A]
2025-09-05 00:58:05,719 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:58:05,721 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:58:06,173 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.310s
2025-09-05 00:58:06,183 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:58:06,184 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1236 tokens) in 0.008s
2025-09-05 00:58:17,682 - src.rag_pipeline - INFO - Generated response (180 tokens) in 11.494s
2025-09-05 00:58:17,881 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:58:17,932 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:58:17,938 - src.experiment_runner - INFO - Completed run 68/510: 13.40s
2025-09-05 00:58:17,939 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:58:17,939 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:58:17,939 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:58:17,939 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:58:17,939 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:58:17,939 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:58:17,939 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:58:17,944 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:58:17,944 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:58:17,944 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:58:17,944 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:58:17,944 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:58:17,944 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:58:18,442 - src.llm_wrapper - INFO - Model ready in 0.50s
2025-09-05 00:58:18,442 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:58:18,442 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:58:18,442 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:58:18,442 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:58:18,442 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:58:18,442 - src.rag_pipeline - INFO - Processing validated query: 'How does data preprocessing work?'
2025-09-05 00:58:18,449 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does data preprocessing work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.99it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.98it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.96it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.96it/s, batch=1/1, memory=N/A]
2025-09-05 00:58:19,002 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:58:19,005 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:58:19,390 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.940s
2025-09-05 00:58:19,402 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:58:19,403 - src.rag_pipeline - INFO - Built prompt with 5 contexts (909 tokens) in 0.011s
2025-09-05 00:58:28,118 - src.rag_pipeline - INFO - Generated response (139 tokens) in 8.709s
2025-09-05 00:58:28,304 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:58:28,354 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:58:28,359 - src.experiment_runner - INFO - Completed run 69/510: 10.18s
2025-09-05 00:58:28,360 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:58:28,360 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:58:28,360 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:58:28,360 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:58:28,360 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:58:28,360 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:58:28,360 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:58:28,365 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:58:28,366 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:58:28,366 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:58:28,366 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:58:28,366 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:58:28,366 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:58:28,908 - src.llm_wrapper - INFO - Model ready in 0.54s
2025-09-05 00:58:28,908 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:58:28,908 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:58:28,908 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:58:28,908 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:58:28,908 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:58:28,908 - src.rag_pipeline - INFO - Processing validated query: 'What are the challenges in AI development?'
2025-09-05 00:58:28,916 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are the challenges in AI development?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.12it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.11it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.04it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.04it/s, batch=1/1, memory=N/A]
2025-09-05 00:58:29,509 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:58:29,510 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:58:29,961 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.045s
2025-09-05 00:58:29,974 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:58:29,974 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1445 tokens) in 0.012s
2025-09-05 00:58:45,021 - src.rag_pipeline - INFO - Generated response (237 tokens) in 15.043s
2025-09-05 00:58:45,203 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:58:45,259 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:58:45,267 - src.experiment_runner - INFO - Completed run 70/510: 16.66s
2025-09-05 00:58:45,267 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:58:45,267 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:58:45,267 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:58:45,267 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:58:45,267 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:58:45,267 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:58:45,267 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:58:45,272 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:58:45,273 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:58:45,275 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:58:45,275 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:58:45,275 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:58:45,276 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:58:45,772 - src.llm_wrapper - INFO - Model ready in 0.50s
2025-09-05 00:58:45,772 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:58:45,772 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:58:45,772 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:58:45,772 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:58:45,772 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:58:45,772 - src.rag_pipeline - INFO - Processing validated query: 'What is machine learning?'
2025-09-05 00:58:45,780 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is machine learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.03it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.03it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.99it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.99it/s, batch=1/1, memory=N/A]
2025-09-05 00:58:46,320 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:58:46,321 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:58:46,787 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.006s
2025-09-05 00:58:46,796 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:58:46,797 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1162 tokens) in 0.008s
2025-09-05 00:58:52,452 - src.rag_pipeline - INFO - Generated response (59 tokens) in 5.652s
2025-09-05 00:58:52,622 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:58:52,674 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:58:52,681 - src.experiment_runner - INFO - Completed run 71/510: 7.19s
2025-09-05 00:58:52,681 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:58:52,681 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:58:52,681 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:58:52,681 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:58:52,681 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:58:52,681 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:58:52,681 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:58:52,687 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:58:52,688 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:58:52,688 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:58:52,688 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:58:52,688 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:58:52,688 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:58:53,195 - src.llm_wrapper - INFO - Model ready in 0.51s
2025-09-05 00:58:53,195 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:58:53,195 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:58:53,195 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:58:53,195 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:58:53,195 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:58:53,195 - src.rag_pipeline - INFO - Processing validated query: 'How does artificial intelligence work?'
2025-09-05 00:58:53,203 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does artificial intelligence work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.25it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.24it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.21it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.21it/s, batch=1/1, memory=N/A]
2025-09-05 00:58:53,750 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:58:53,753 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:58:54,193 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.990s
2025-09-05 00:58:54,203 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:58:54,204 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1211 tokens) in 0.009s
2025-09-05 00:59:10,708 - src.rag_pipeline - INFO - Generated response (287 tokens) in 16.500s
2025-09-05 00:59:10,894 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:59:10,946 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:59:10,952 - src.experiment_runner - INFO - Completed run 72/510: 18.03s
2025-09-05 00:59:10,952 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:59:10,952 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:59:10,952 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:59:10,952 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:59:10,952 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:59:10,952 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:59:10,952 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:59:10,957 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:59:10,957 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:59:10,958 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:59:10,958 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:59:10,958 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:59:10,959 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:59:11,457 - src.llm_wrapper - INFO - Model ready in 0.50s
2025-09-05 00:59:11,457 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:59:11,457 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:59:11,457 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:59:11,457 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:59:11,457 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:59:11,457 - src.rag_pipeline - INFO - Processing validated query: 'Explain deep learning algorithms.'
2025-09-05 00:59:11,466 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain deep learning algorithms....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.37it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.36it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.33it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.33it/s, batch=1/1, memory=N/A]
2025-09-05 00:59:11,971 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:59:11,972 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:59:12,545 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.079s
2025-09-05 00:59:12,550 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:59:12,551 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1198 tokens) in 0.006s
2025-09-05 00:59:17,781 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.228s
2025-09-05 00:59:17,948 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:59:17,995 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:59:18,005 - src.experiment_runner - INFO - Completed run 73/510: 6.83s
2025-09-05 00:59:18,005 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:59:18,005 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:59:18,006 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:59:18,006 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:59:18,006 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:59:18,006 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:59:18,006 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:59:18,012 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:59:18,012 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:59:18,012 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:59:18,012 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:59:18,013 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:59:18,013 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:59:18,517 - src.llm_wrapper - INFO - Model ready in 0.50s
2025-09-05 00:59:18,517 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:59:18,517 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:59:18,517 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:59:18,518 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:59:18,518 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:59:18,518 - src.rag_pipeline - INFO - Processing validated query: 'What are neural networks?'
2025-09-05 00:59:18,525 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are neural networks?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.66it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.66it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.64it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.64it/s, batch=1/1, memory=N/A]
2025-09-05 00:59:18,935 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:59:18,938 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:59:19,487 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.962s
2025-09-05 00:59:19,490 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:59:19,490 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1353 tokens) in 0.002s
2025-09-05 00:59:34,078 - src.rag_pipeline - INFO - Generated response (238 tokens) in 14.583s
2025-09-05 00:59:34,300 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:59:34,355 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:59:34,361 - src.experiment_runner - INFO - Completed run 74/510: 16.07s
2025-09-05 00:59:34,361 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:59:34,361 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:59:34,361 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:59:34,361 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:59:34,361 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:59:34,361 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:59:34,361 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:59:34,366 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:59:34,366 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:59:34,366 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:59:34,366 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:59:34,367 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:59:34,367 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:59:34,911 - src.llm_wrapper - INFO - Model ready in 0.54s
2025-09-05 00:59:34,911 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:59:34,911 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:59:34,911 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:59:34,911 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:59:34,911 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:59:34,911 - src.rag_pipeline - INFO - Processing validated query: 'How do large language models work?'
2025-09-05 00:59:34,921 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How do large language models work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.18it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.17it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.15it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.15it/s, batch=1/1, memory=N/A]
2025-09-05 00:59:35,505 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:59:35,506 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:59:36,405 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.484s
2025-09-05 00:59:36,414 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:59:36,415 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1337 tokens) in 0.009s
2025-09-05 00:59:41,845 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.429s
2025-09-05 00:59:41,990 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:59:42,042 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:59:42,049 - src.experiment_runner - INFO - Completed run 75/510: 7.48s
2025-09-05 00:59:42,050 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:59:42,050 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:59:42,050 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:59:42,050 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:59:42,050 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:59:42,050 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:59:42,050 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:59:42,056 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:59:42,056 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:59:42,056 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:59:42,056 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:59:42,056 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:59:42,056 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:59:42,565 - src.llm_wrapper - INFO - Model ready in 0.51s
2025-09-05 00:59:42,565 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:59:42,565 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:59:42,565 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:59:42,565 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:59:42,565 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:59:42,565 - src.rag_pipeline - INFO - Processing validated query: 'What is natural language processing?'
2025-09-05 00:59:42,575 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is natural language processing?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.82it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.82it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.79it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.79it/s, batch=1/1, memory=N/A]
2025-09-05 00:59:42,964 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:59:42,967 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:59:43,610 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.035s
2025-09-05 00:59:43,617 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:59:43,618 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1749 tokens) in 0.006s
2025-09-05 00:59:50,501 - src.rag_pipeline - INFO - Generated response (55 tokens) in 6.881s
2025-09-05 00:59:50,684 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 00:59:50,736 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 00:59:50,741 - src.experiment_runner - INFO - Completed run 76/510: 8.45s
2025-09-05 00:59:50,741 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 00:59:50,742 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 00:59:50,742 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 00:59:50,742 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 00:59:50,742 - src.embedding_service - INFO -   Device: mps
2025-09-05 00:59:50,742 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 00:59:50,742 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 00:59:50,747 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:59:50,747 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 00:59:50,747 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 00:59:50,747 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 00:59:50,747 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 00:59:50,747 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 00:59:51,254 - src.llm_wrapper - INFO - Model ready in 0.51s
2025-09-05 00:59:51,254 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 00:59:51,254 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 00:59:51,254 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 00:59:51,254 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 00:59:51,254 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 00:59:51,254 - src.rag_pipeline - INFO - Processing validated query: 'Explain computer vision techniques.'
2025-09-05 00:59:51,261 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain computer vision techniques....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.94it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.94it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.91it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.91it/s, batch=1/1, memory=N/A]
2025-09-05 00:59:51,637 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:59:51,638 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 00:59:52,126 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.865s
2025-09-05 00:59:52,132 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 00:59:52,134 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1297 tokens) in 0.005s
2025-09-05 01:00:10,488 - src.rag_pipeline - INFO - Generated response (318 tokens) in 18.348s
2025-09-05 01:00:10,684 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:00:10,734 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:00:10,742 - src.experiment_runner - INFO - Completed run 77/510: 19.75s
2025-09-05 01:00:10,742 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:00:10,742 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:00:10,742 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:00:10,743 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:00:10,743 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:00:10,743 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:00:10,743 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:00:10,748 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:00:10,748 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:00:10,748 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:00:10,748 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:00:10,748 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:00:10,748 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:00:11,441 - src.llm_wrapper - INFO - Model ready in 0.69s
2025-09-05 01:00:11,441 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:00:11,441 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:00:11,441 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:00:11,441 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:00:11,441 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:00:11,441 - src.rag_pipeline - INFO - Processing validated query: 'What is reinforcement learning?'
2025-09-05 01:00:11,450 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is reinforcement learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.55it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.55it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.53it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.53it/s, batch=1/1, memory=N/A]
2025-09-05 01:00:12,191 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:00:12,195 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:00:12,724 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.273s
2025-09-05 01:00:12,729 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:00:12,730 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1236 tokens) in 0.005s
2025-09-05 01:00:24,289 - src.rag_pipeline - INFO - Generated response (180 tokens) in 11.554s
2025-09-05 01:00:24,471 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:00:24,529 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:00:24,537 - src.experiment_runner - INFO - Completed run 78/510: 13.55s
2025-09-05 01:00:24,537 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:00:24,537 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:00:24,537 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:00:24,537 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:00:24,537 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:00:24,537 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:00:24,537 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:00:24,542 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:00:24,542 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:00:24,542 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:00:24,542 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:00:24,542 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:00:24,543 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:00:25,093 - src.llm_wrapper - INFO - Model ready in 0.55s
2025-09-05 01:00:25,093 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:00:25,093 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:00:25,093 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:00:25,093 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:00:25,093 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:00:25,093 - src.rag_pipeline - INFO - Processing validated query: 'How does data preprocessing work?'
2025-09-05 01:00:25,101 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does data preprocessing work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.47it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.47it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.44it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.44it/s, batch=1/1, memory=N/A]
2025-09-05 01:00:25,626 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:00:25,628 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:00:26,098 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.997s
2025-09-05 01:00:26,104 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:00:26,104 - src.rag_pipeline - INFO - Built prompt with 5 contexts (909 tokens) in 0.005s
2025-09-05 01:00:34,767 - src.rag_pipeline - INFO - Generated response (139 tokens) in 8.657s
2025-09-05 01:00:34,939 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:00:34,989 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:00:34,996 - src.experiment_runner - INFO - Completed run 79/510: 10.23s
2025-09-05 01:00:34,996 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:00:34,996 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:00:34,996 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:00:34,996 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:00:34,996 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:00:34,996 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:00:34,996 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:00:35,001 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:00:35,002 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:00:35,002 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:00:35,002 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:00:35,002 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:00:35,002 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:00:35,491 - src.llm_wrapper - INFO - Model ready in 0.49s
2025-09-05 01:00:35,491 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:00:35,491 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:00:35,491 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:00:35,491 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:00:35,492 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:00:35,492 - src.rag_pipeline - INFO - Processing validated query: 'What are the challenges in AI development?'
2025-09-05 01:00:35,498 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are the challenges in AI development?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.39it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.39it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.37it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.37it/s, batch=1/1, memory=N/A]
2025-09-05 01:00:35,910 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:00:35,911 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:00:36,382 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.884s
2025-09-05 01:00:36,384 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:00:36,384 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1445 tokens) in 0.002s
2025-09-05 01:00:51,339 - src.rag_pipeline - INFO - Generated response (237 tokens) in 14.950s
2025-09-05 01:00:51,528 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:00:51,582 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:00:51,589 - src.experiment_runner - INFO - Completed run 80/510: 16.34s
2025-09-05 01:00:51,589 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:00:51,589 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:00:51,589 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:00:51,589 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:00:51,589 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:00:51,590 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:00:51,590 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:00:51,595 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:00:51,595 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:00:51,595 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:00:51,595 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:00:51,595 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:00:51,595 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:00:52,110 - src.llm_wrapper - INFO - Model ready in 0.51s
2025-09-05 01:00:52,110 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:00:52,110 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:00:52,110 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:00:52,110 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:00:52,110 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:00:52,110 - src.rag_pipeline - INFO - Processing validated query: 'What is machine learning?'
2025-09-05 01:00:52,119 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is machine learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.22it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.22it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.19it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.19it/s, batch=1/1, memory=N/A]
2025-09-05 01:00:52,448 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:00:52,449 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:00:52,786 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.667s
2025-09-05 01:00:52,787 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:00:52,788 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1162 tokens) in 0.002s
2025-09-05 01:00:58,514 - src.rag_pipeline - INFO - Generated response (59 tokens) in 5.723s
2025-09-05 01:00:58,709 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:00:58,770 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:00:58,777 - src.experiment_runner - INFO - Completed run 81/510: 6.93s
2025-09-05 01:00:58,777 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:00:58,777 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:00:58,777 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:00:58,777 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:00:58,777 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:00:58,777 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:00:58,777 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:00:58,783 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:00:58,783 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:00:58,783 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:00:58,783 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:00:58,783 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:00:58,784 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:00:59,276 - src.llm_wrapper - INFO - Model ready in 0.49s
2025-09-05 01:00:59,276 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:00:59,276 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:00:59,276 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:00:59,276 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:00:59,276 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:00:59,276 - src.rag_pipeline - INFO - Processing validated query: 'How does artificial intelligence work?'
2025-09-05 01:00:59,283 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does artificial intelligence work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.31it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.31it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.28it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.28it/s, batch=1/1, memory=N/A]
2025-09-05 01:00:59,637 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:00:59,638 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:01:00,073 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.789s
2025-09-05 01:01:00,075 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:01:00,075 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1211 tokens) in 0.002s
2025-09-05 01:01:16,694 - src.rag_pipeline - INFO - Generated response (287 tokens) in 16.615s
2025-09-05 01:01:16,883 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:01:16,933 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:01:16,944 - src.experiment_runner - INFO - Completed run 82/510: 17.92s
2025-09-05 01:01:16,944 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:01:16,944 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:01:16,944 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:01:16,944 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:01:16,944 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:01:16,944 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:01:16,944 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:01:16,949 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:01:16,949 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:01:16,949 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:01:16,949 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:01:16,949 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:01:16,949 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:01:17,475 - src.llm_wrapper - INFO - Model ready in 0.53s
2025-09-05 01:01:17,475 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:01:17,475 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:01:17,475 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:01:17,475 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:01:17,475 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:01:17,475 - src.rag_pipeline - INFO - Processing validated query: 'Explain deep learning algorithms.'
2025-09-05 01:01:17,484 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain deep learning algorithms....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.38it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.37it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.35it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.35it/s, batch=1/1, memory=N/A]
2025-09-05 01:01:17,904 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:01:17,905 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:01:18,514 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.030s
2025-09-05 01:01:18,516 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:01:18,517 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1198 tokens) in 0.002s
2025-09-05 01:01:23,625 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.107s
2025-09-05 01:01:23,798 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:01:23,850 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:01:23,860 - src.experiment_runner - INFO - Completed run 83/510: 6.68s
2025-09-05 01:01:23,860 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:01:23,860 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:01:23,860 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:01:23,860 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:01:23,860 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:01:23,860 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:01:23,860 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:01:23,866 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:01:23,866 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:01:23,866 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:01:23,866 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:01:23,866 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:01:23,867 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:01:24,366 - src.llm_wrapper - INFO - Model ready in 0.50s
2025-09-05 01:01:24,366 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:01:24,366 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:01:24,366 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:01:24,366 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:01:24,366 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:01:24,366 - src.rag_pipeline - INFO - Processing validated query: 'What are neural networks?'
2025-09-05 01:01:24,373 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are neural networks?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.66it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.62it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.58it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.57it/s, batch=1/1, memory=N/A]
2025-09-05 01:01:24,656 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:01:24,658 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:01:25,104 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.730s
2025-09-05 01:01:25,107 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:01:25,107 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1353 tokens) in 0.002s
2025-09-05 01:01:39,726 - src.rag_pipeline - INFO - Generated response (238 tokens) in 14.618s
2025-09-05 01:01:39,883 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:01:39,934 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:01:39,940 - src.experiment_runner - INFO - Completed run 84/510: 15.87s
2025-09-05 01:01:39,940 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:01:39,940 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:01:39,940 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:01:39,940 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:01:39,940 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:01:39,940 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:01:39,940 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:01:39,945 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:01:39,945 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:01:39,945 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:01:39,945 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:01:39,945 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:01:39,945 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:01:40,448 - src.llm_wrapper - INFO - Model ready in 0.50s
2025-09-05 01:01:40,448 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:01:40,448 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:01:40,448 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:01:40,448 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:01:40,448 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:01:40,448 - src.rag_pipeline - INFO - Processing validated query: 'How do large language models work?'
2025-09-05 01:01:40,462 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How do large language models work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.38it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.37it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.35it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.35it/s, batch=1/1, memory=N/A]
2025-09-05 01:01:40,882 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:01:40,883 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:01:41,445 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.983s
2025-09-05 01:01:41,448 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:01:41,448 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1337 tokens) in 0.002s
2025-09-05 01:01:46,939 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.490s
2025-09-05 01:01:47,112 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:01:47,159 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:01:47,164 - src.experiment_runner - INFO - Completed run 85/510: 7.00s
2025-09-05 01:01:47,164 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:01:47,164 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:01:47,164 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:01:47,165 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:01:47,165 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:01:47,165 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:01:47,165 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:01:47,170 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:01:47,170 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:01:47,170 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:01:47,170 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:01:47,171 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:01:47,171 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:01:47,685 - src.llm_wrapper - INFO - Model ready in 0.51s
2025-09-05 01:01:47,685 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:01:47,685 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:01:47,685 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:01:47,685 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:01:47,685 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:01:47,685 - src.rag_pipeline - INFO - Processing validated query: 'What is natural language processing?'
2025-09-05 01:01:47,692 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is natural language processing?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.76it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.69it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.64it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.64it/s, batch=1/1, memory=N/A]
2025-09-05 01:01:48,102 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:01:48,103 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:01:48,758 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.066s
2025-09-05 01:01:48,764 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:01:48,764 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1749 tokens) in 0.005s
2025-09-05 01:01:55,664 - src.rag_pipeline - INFO - Generated response (55 tokens) in 6.899s
2025-09-05 01:01:55,836 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:01:55,890 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:01:55,898 - src.experiment_runner - INFO - Completed run 86/510: 8.50s
2025-09-05 01:01:55,898 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:01:55,899 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:01:55,899 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:01:55,899 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:01:55,899 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:01:55,899 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:01:55,899 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:01:55,904 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:01:55,904 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:01:55,904 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:01:55,904 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:01:55,904 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:01:55,904 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:01:56,431 - src.llm_wrapper - INFO - Model ready in 0.53s
2025-09-05 01:01:56,431 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:01:56,431 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:01:56,431 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:01:56,431 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:01:56,431 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:01:56,431 - src.rag_pipeline - INFO - Processing validated query: 'Explain computer vision techniques.'
2025-09-05 01:01:56,440 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain computer vision techniques....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.82it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.80it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.77it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.76it/s, batch=1/1, memory=N/A]
2025-09-05 01:01:56,761 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:01:56,763 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:01:57,382 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.942s
2025-09-05 01:01:57,384 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:01:57,385 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1297 tokens) in 0.002s
2025-09-05 01:02:15,886 - src.rag_pipeline - INFO - Generated response (318 tokens) in 18.496s
2025-09-05 01:02:16,079 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:02:16,127 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:02:16,133 - src.experiment_runner - INFO - Completed run 87/510: 19.99s
2025-09-05 01:02:16,133 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:02:16,133 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:02:16,133 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:02:16,133 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:02:16,133 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:02:16,133 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:02:16,133 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:02:16,138 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:02:16,138 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:02:16,138 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:02:16,139 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:02:16,139 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:02:16,139 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:02:16,632 - src.llm_wrapper - INFO - Model ready in 0.49s
2025-09-05 01:02:16,632 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:02:16,632 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:02:16,632 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:02:16,632 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:02:16,632 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:02:16,632 - src.rag_pipeline - INFO - Processing validated query: 'What is reinforcement learning?'
2025-09-05 01:02:16,639 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is reinforcement learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.41it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.39it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.35it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.35it/s, batch=1/1, memory=N/A]
2025-09-05 01:02:17,048 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:02:17,049 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:02:17,660 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.021s
2025-09-05 01:02:17,665 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:02:17,665 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1236 tokens) in 0.005s
2025-09-05 01:02:29,182 - src.rag_pipeline - INFO - Generated response (180 tokens) in 11.514s
2025-09-05 01:02:29,366 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:02:29,416 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:02:29,423 - src.experiment_runner - INFO - Completed run 88/510: 13.05s
2025-09-05 01:02:29,423 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:02:29,423 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:02:29,423 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:02:29,423 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:02:29,423 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:02:29,423 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:02:29,423 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:02:29,431 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:02:29,431 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:02:29,431 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:02:29,431 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:02:29,431 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:02:29,431 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:02:29,943 - src.llm_wrapper - INFO - Model ready in 0.51s
2025-09-05 01:02:29,943 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:02:29,943 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:02:29,943 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:02:29,943 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:02:29,943 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:02:29,943 - src.rag_pipeline - INFO - Processing validated query: 'How does data preprocessing work?'
2025-09-05 01:02:29,950 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does data preprocessing work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.08it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.07it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.02it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.02it/s, batch=1/1, memory=N/A]
2025-09-05 01:02:30,352 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:02:30,353 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:02:30,910 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.959s
2025-09-05 01:02:30,915 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:02:30,915 - src.rag_pipeline - INFO - Built prompt with 5 contexts (909 tokens) in 0.004s
2025-09-05 01:02:39,787 - src.rag_pipeline - INFO - Generated response (139 tokens) in 8.863s
2025-09-05 01:02:40,001 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:02:40,057 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:02:40,063 - src.experiment_runner - INFO - Completed run 89/510: 10.37s
2025-09-05 01:02:40,063 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:02:40,063 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:02:40,063 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:02:40,064 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:02:40,064 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:02:40,064 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:02:40,064 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:02:40,070 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:02:40,071 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:02:40,071 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:02:40,071 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:02:40,071 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:02:40,071 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:02:40,595 - src.llm_wrapper - INFO - Model ready in 0.52s
2025-09-05 01:02:40,595 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:02:40,595 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:02:40,595 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:02:40,595 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:02:40,595 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:02:40,595 - src.rag_pipeline - INFO - Processing validated query: 'What are the challenges in AI development?'
2025-09-05 01:02:40,604 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are the challenges in AI development?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.55it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.55it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.49it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.49it/s, batch=1/1, memory=N/A]
2025-09-05 01:02:41,149 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:02:41,151 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:02:41,611 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.007s
2025-09-05 01:02:41,614 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:02:41,615 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1445 tokens) in 0.003s
2025-09-05 01:02:56,529 - src.rag_pipeline - INFO - Generated response (237 tokens) in 14.912s
2025-09-05 01:02:56,725 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:02:56,777 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:02:56,783 - src.experiment_runner - INFO - Completed run 90/510: 16.47s
2025-09-05 01:02:56,783 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:02:56,783 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:02:56,783 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:02:56,783 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:02:56,783 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:02:56,783 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:02:56,783 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:02:56,788 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:02:56,788 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:02:56,788 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:02:56,788 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:02:56,788 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:02:56,788 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:02:57,278 - src.llm_wrapper - INFO - Model ready in 0.49s
2025-09-05 01:02:57,278 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:02:57,278 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:02:57,278 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:02:57,278 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:02:57,278 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:02:57,278 - src.rag_pipeline - INFO - Processing validated query: 'What is machine learning?'
2025-09-05 01:02:57,286 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is machine learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.57it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.56it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.53it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.53it/s, batch=1/1, memory=N/A]
2025-09-05 01:02:57,786 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:02:57,789 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:02:58,222 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.936s
2025-09-05 01:02:58,227 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:02:58,228 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1162 tokens) in 0.005s
2025-09-05 01:03:03,855 - src.rag_pipeline - INFO - Generated response (59 tokens) in 5.626s
2025-09-05 01:03:04,020 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:03:04,075 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:03:04,081 - src.experiment_runner - INFO - Completed run 91/510: 7.07s
2025-09-05 01:03:04,081 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:03:04,081 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:03:04,081 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:03:04,081 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:03:04,081 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:03:04,081 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:03:04,081 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:03:04,087 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:03:04,088 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:03:04,088 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:03:04,088 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:03:04,088 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:03:04,088 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:03:04,594 - src.llm_wrapper - INFO - Model ready in 0.51s
2025-09-05 01:03:04,594 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:03:04,595 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:03:04,595 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:03:04,595 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:03:04,595 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:03:04,595 - src.rag_pipeline - INFO - Processing validated query: 'How does artificial intelligence work?'
2025-09-05 01:03:04,602 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does artificial intelligence work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.16it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.16it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.09it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.08it/s, batch=1/1, memory=N/A]
2025-09-05 01:03:04,919 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:03:04,920 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:03:05,323 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.721s
2025-09-05 01:03:05,326 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:03:05,327 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1211 tokens) in 0.003s
2025-09-05 01:03:21,741 - src.rag_pipeline - INFO - Generated response (287 tokens) in 16.412s
2025-09-05 01:03:21,916 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:03:21,970 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:03:21,976 - src.experiment_runner - INFO - Completed run 92/510: 17.66s
2025-09-05 01:03:21,977 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:03:21,977 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:03:21,977 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:03:21,977 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:03:21,977 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:03:21,977 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:03:21,977 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:03:21,982 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:03:21,983 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:03:21,983 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:03:21,983 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:03:21,983 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:03:21,983 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:03:22,467 - src.llm_wrapper - INFO - Model ready in 0.48s
2025-09-05 01:03:22,467 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:03:22,468 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:03:22,468 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:03:22,468 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:03:22,468 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:03:22,468 - src.rag_pipeline - INFO - Processing validated query: 'Explain deep learning algorithms.'
2025-09-05 01:03:22,474 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain deep learning algorithms....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.84it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.84it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.81it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.81it/s, batch=1/1, memory=N/A]
2025-09-05 01:03:22,861 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:03:22,862 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:03:23,233 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.759s
2025-09-05 01:03:23,235 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:03:23,235 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1198 tokens) in 0.002s
2025-09-05 01:03:28,365 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.125s
2025-09-05 01:03:28,701 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:03:28,758 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:03:28,766 - src.experiment_runner - INFO - Completed run 93/510: 6.51s
2025-09-05 01:03:28,766 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:03:28,766 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:03:28,766 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:03:28,766 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:03:28,766 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:03:28,766 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:03:28,766 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:03:28,773 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:03:28,778 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:03:28,789 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:03:28,789 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:03:28,789 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:03:28,789 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:03:29,309 - src.llm_wrapper - INFO - Model ready in 0.52s
2025-09-05 01:03:29,309 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:03:29,309 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:03:29,309 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:03:29,309 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:03:29,309 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:03:29,309 - src.rag_pipeline - INFO - Processing validated query: 'What are neural networks?'
2025-09-05 01:03:29,316 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are neural networks?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.32it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.31it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.29it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.29it/s, batch=1/1, memory=N/A]
2025-09-05 01:03:29,658 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:03:29,660 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:03:30,062 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.746s
2025-09-05 01:03:30,065 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:03:30,065 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1353 tokens) in 0.002s
2025-09-05 01:03:44,855 - src.rag_pipeline - INFO - Generated response (238 tokens) in 14.785s
2025-09-05 01:03:45,041 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:03:45,091 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:03:45,096 - src.experiment_runner - INFO - Completed run 94/510: 16.09s
2025-09-05 01:03:45,097 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:03:45,097 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:03:45,097 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:03:45,097 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:03:45,097 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:03:45,097 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:03:45,097 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:03:45,102 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:03:45,102 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:03:45,102 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:03:45,102 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:03:45,102 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:03:45,102 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:03:45,599 - src.llm_wrapper - INFO - Model ready in 0.50s
2025-09-05 01:03:45,599 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:03:45,599 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:03:45,599 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:03:45,599 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:03:45,599 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:03:45,599 - src.rag_pipeline - INFO - Processing validated query: 'How do large language models work?'
2025-09-05 01:03:45,606 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How do large language models work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.34it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.34it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.30it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.30it/s, batch=1/1, memory=N/A]
2025-09-05 01:03:46,240 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:03:46,241 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:03:46,629 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.023s
2025-09-05 01:03:46,632 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:03:46,633 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1337 tokens) in 0.002s
2025-09-05 01:03:52,087 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.446s
2025-09-05 01:03:52,259 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:03:52,314 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:03:52,320 - src.experiment_runner - INFO - Completed run 95/510: 6.99s
2025-09-05 01:03:52,320 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:03:52,320 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:03:52,320 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:03:52,320 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:03:52,320 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:03:52,320 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:03:52,320 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:03:52,325 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:03:52,326 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:03:52,326 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:03:52,326 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:03:52,326 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:03:52,326 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:03:52,836 - src.llm_wrapper - INFO - Model ready in 0.51s
2025-09-05 01:03:52,836 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:03:52,836 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:03:52,836 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:03:52,836 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:03:52,836 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:03:52,836 - src.rag_pipeline - INFO - Processing validated query: 'What is natural language processing?'
2025-09-05 01:03:52,844 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is natural language processing?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.59it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.58it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.52it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.52it/s, batch=1/1, memory=N/A]
2025-09-05 01:03:53,107 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:03:53,110 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:03:53,477 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.633s
2025-09-05 01:03:53,479 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:03:53,480 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1749 tokens) in 0.002s
2025-09-05 01:04:00,369 - src.rag_pipeline - INFO - Generated response (55 tokens) in 6.888s
2025-09-05 01:04:00,517 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:04:00,567 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:04:00,574 - src.experiment_runner - INFO - Completed run 96/510: 8.05s
2025-09-05 01:04:00,574 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:04:00,574 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:04:00,574 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:04:00,574 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:04:00,574 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:04:00,574 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:04:00,574 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:04:00,580 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:04:00,580 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:04:00,580 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:04:00,580 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:04:00,580 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:04:00,580 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:04:01,091 - src.llm_wrapper - INFO - Model ready in 0.51s
2025-09-05 01:04:01,091 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:04:01,091 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:04:01,091 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:04:01,091 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:04:01,091 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:04:01,091 - src.rag_pipeline - INFO - Processing validated query: 'Explain computer vision techniques.'
2025-09-05 01:04:01,098 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain computer vision techniques....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.63it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.62it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.58it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.58it/s, batch=1/1, memory=N/A]
2025-09-05 01:04:01,437 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:04:01,438 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:04:01,864 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.765s
2025-09-05 01:04:01,872 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:04:01,874 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1297 tokens) in 0.008s
2025-09-05 01:04:20,063 - src.rag_pipeline - INFO - Generated response (318 tokens) in 18.186s
2025-09-05 01:04:20,230 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:04:20,275 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:04:20,283 - src.experiment_runner - INFO - Completed run 97/510: 19.49s
2025-09-05 01:04:20,283 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:04:20,283 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:04:20,283 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:04:20,283 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:04:20,283 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:04:20,283 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:04:20,283 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:04:20,290 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:04:20,291 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:04:20,291 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:04:20,291 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:04:20,291 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:04:20,291 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:04:20,826 - src.llm_wrapper - INFO - Model ready in 0.53s
2025-09-05 01:04:20,826 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:04:20,826 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:04:20,826 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:04:20,826 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:04:20,826 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:04:20,826 - src.rag_pipeline - INFO - Processing validated query: 'What is reinforcement learning?'
2025-09-05 01:04:20,833 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is reinforcement learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.50it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.49it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.47it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.47it/s, batch=1/1, memory=N/A]
2025-09-05 01:04:21,229 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:04:21,231 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:04:21,606 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.774s
2025-09-05 01:04:21,608 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:04:21,609 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1236 tokens) in 0.002s
2025-09-05 01:04:33,327 - src.rag_pipeline - INFO - Generated response (180 tokens) in 11.713s
2025-09-05 01:04:33,560 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:04:33,619 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:04:33,626 - src.experiment_runner - INFO - Completed run 98/510: 13.05s
2025-09-05 01:04:33,626 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:04:33,626 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:04:33,626 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:04:33,626 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:04:33,626 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:04:33,626 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:04:33,626 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:04:33,631 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:04:33,631 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:04:33,631 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:04:33,632 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:04:33,632 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:04:33,632 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:04:34,174 - src.llm_wrapper - INFO - Model ready in 0.54s
2025-09-05 01:04:34,174 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:04:34,174 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:04:34,174 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:04:34,174 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:04:34,174 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:04:34,174 - src.rag_pipeline - INFO - Processing validated query: 'How does data preprocessing work?'
2025-09-05 01:04:34,183 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does data preprocessing work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.41it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.40it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.33it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.33it/s, batch=1/1, memory=N/A]
2025-09-05 01:04:34,736 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:04:34,738 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:04:35,221 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.038s
2025-09-05 01:04:35,226 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:04:35,226 - src.rag_pipeline - INFO - Built prompt with 5 contexts (909 tokens) in 0.004s
2025-09-05 01:04:44,130 - src.rag_pipeline - INFO - Generated response (139 tokens) in 8.900s
2025-09-05 01:04:44,314 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:04:44,361 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:04:44,367 - src.experiment_runner - INFO - Completed run 99/510: 10.50s
2025-09-05 01:04:44,367 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:04:44,367 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:04:44,367 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:04:44,367 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:04:44,367 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:04:44,367 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:04:44,367 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:04:44,372 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:04:44,373 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:04:44,373 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:04:44,373 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:04:44,373 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:04:44,373 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:04:44,868 - src.llm_wrapper - INFO - Model ready in 0.50s
2025-09-05 01:04:44,868 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:04:44,868 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:04:44,868 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:04:44,868 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:04:44,868 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:04:44,868 - src.rag_pipeline - INFO - Processing validated query: 'What are the challenges in AI development?'
2025-09-05 01:04:44,876 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are the challenges in AI development?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.62it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.60it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.57it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.57it/s, batch=1/1, memory=N/A]
2025-09-05 01:04:45,368 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:04:45,370 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:04:45,792 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.915s
2025-09-05 01:04:45,801 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:04:45,802 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1445 tokens) in 0.009s
2025-09-05 01:05:00,587 - src.rag_pipeline - INFO - Generated response (237 tokens) in 14.781s
2025-09-05 01:05:00,765 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:05:00,824 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:05:00,835 - src.experiment_runner - INFO - Completed run 100/510: 16.22s
2025-09-05 01:05:00,835 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:05:00,835 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:05:00,835 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:05:00,836 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:05:00,836 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:05:00,836 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:05:00,836 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:05:00,842 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:05:00,842 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:05:00,842 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:05:00,842 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:05:00,843 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:05:00,843 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:05:01,334 - src.llm_wrapper - INFO - Model ready in 0.49s
2025-09-05 01:05:01,334 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:05:01,334 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:05:01,334 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:05:01,334 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:05:01,334 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:05:01,334 - src.rag_pipeline - INFO - Processing validated query: 'What is machine learning?'
2025-09-05 01:05:01,342 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is machine learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.77it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.77it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.73it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.73it/s, batch=1/1, memory=N/A]
2025-09-05 01:05:01,787 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:05:01,788 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:05:02,473 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.132s
2025-09-05 01:05:02,476 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:05:02,476 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1162 tokens) in 0.002s
2025-09-05 01:05:08,114 - src.rag_pipeline - INFO - Generated response (59 tokens) in 5.636s
2025-09-05 01:05:08,276 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:05:08,329 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:05:08,336 - src.experiment_runner - INFO - Completed run 101/510: 7.28s
2025-09-05 01:05:08,337 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:05:08,337 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:05:08,337 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:05:08,337 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:05:08,337 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:05:08,337 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:05:08,337 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:05:08,342 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:05:08,342 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:05:08,342 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:05:08,342 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:05:08,343 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:05:08,343 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:05:08,862 - src.llm_wrapper - INFO - Model ready in 0.52s
2025-09-05 01:05:08,862 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:05:08,862 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:05:08,862 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:05:08,862 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:05:08,862 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:05:08,862 - src.rag_pipeline - INFO - Processing validated query: 'How does artificial intelligence work?'
2025-09-05 01:05:08,869 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does artificial intelligence work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.07it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.06it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.02it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.01it/s, batch=1/1, memory=N/A]
2025-09-05 01:05:09,258 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:05:09,259 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:05:09,796 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.928s
2025-09-05 01:05:09,798 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:05:09,798 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1211 tokens) in 0.002s
2025-09-05 01:05:26,247 - src.rag_pipeline - INFO - Generated response (287 tokens) in 16.446s
2025-09-05 01:05:26,347 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:05:26,395 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:05:26,407 - src.experiment_runner - INFO - Completed run 102/510: 17.91s
2025-09-05 01:05:26,407 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:05:26,407 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:05:26,408 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:05:26,408 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:05:26,408 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:05:26,408 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:05:26,408 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:05:26,419 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:05:26,419 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:05:26,419 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:05:26,419 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:05:26,419 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:05:26,419 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:05:26,912 - src.llm_wrapper - INFO - Model ready in 0.49s
2025-09-05 01:05:26,912 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:05:26,912 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:05:26,912 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:05:26,912 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:05:26,912 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:05:26,912 - src.rag_pipeline - INFO - Processing validated query: 'Explain deep learning algorithms.'
2025-09-05 01:05:26,919 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain deep learning algorithms....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.50it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.48it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.42it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.42it/s, batch=1/1, memory=N/A]
2025-09-05 01:05:27,211 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:05:27,212 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:05:27,815 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.896s
2025-09-05 01:05:27,817 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:05:27,817 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1198 tokens) in 0.002s
2025-09-05 01:05:32,937 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.119s
2025-09-05 01:05:33,029 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:05:33,075 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:05:33,088 - src.experiment_runner - INFO - Completed run 103/510: 6.53s
2025-09-05 01:05:33,088 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:05:33,088 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:05:33,089 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:05:33,089 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:05:33,089 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:05:33,089 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:05:33,089 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:05:33,094 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:05:33,094 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:05:33,094 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:05:33,094 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:05:33,094 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:05:33,094 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:05:33,593 - src.llm_wrapper - INFO - Model ready in 0.50s
2025-09-05 01:05:33,593 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:05:33,593 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:05:33,593 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:05:33,593 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:05:33,593 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:05:33,593 - src.rag_pipeline - INFO - Processing validated query: 'What are neural networks?'
2025-09-05 01:05:33,600 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are neural networks?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 15.27it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 14.42it/s, batch=1/1, memory=N/A]
2025-09-05 01:05:33,788 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:05:33,789 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:05:34,210 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.610s
2025-09-05 01:05:34,212 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:05:34,212 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1353 tokens) in 0.001s
2025-09-05 01:05:48,650 - src.rag_pipeline - INFO - Generated response (238 tokens) in 14.436s
2025-09-05 01:05:48,769 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:05:48,819 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:05:48,828 - src.experiment_runner - INFO - Completed run 104/510: 15.56s
2025-09-05 01:05:48,828 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:05:48,828 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:05:48,828 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:05:48,828 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:05:48,828 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:05:48,828 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:05:48,828 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:05:48,833 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:05:48,834 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:05:48,834 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:05:48,834 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:05:48,834 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:05:48,834 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:05:49,335 - src.llm_wrapper - INFO - Model ready in 0.50s
2025-09-05 01:05:49,336 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:05:49,336 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:05:49,336 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:05:49,336 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:05:49,336 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:05:49,336 - src.rag_pipeline - INFO - Processing validated query: 'How do large language models work?'
2025-09-05 01:05:49,343 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How do large language models work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 10.21it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 10.06it/s, batch=1/1, memory=N/A]
2025-09-05 01:05:49,552 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:05:49,553 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:05:49,937 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.594s
2025-09-05 01:05:49,939 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:05:49,940 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1337 tokens) in 0.002s
2025-09-05 01:05:55,341 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.400s
2025-09-05 01:05:55,456 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:05:55,509 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:05:55,516 - src.experiment_runner - INFO - Completed run 105/510: 6.51s
2025-09-05 01:05:55,516 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:05:55,516 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:05:55,516 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:05:55,516 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:05:55,516 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:05:55,516 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:05:55,516 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:05:55,522 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:05:55,523 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:05:55,523 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:05:55,523 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:05:55,523 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:05:55,523 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:05:56,041 - src.llm_wrapper - INFO - Model ready in 0.52s
2025-09-05 01:05:56,041 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:05:56,041 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:05:56,041 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:05:56,041 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:05:56,041 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:05:56,042 - src.rag_pipeline - INFO - Processing validated query: 'What is natural language processing?'
2025-09-05 01:05:56,048 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is natural language processing?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  9.25it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  9.20it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  8.88it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  8.88it/s, batch=1/1, memory=N/A]
2025-09-05 01:05:56,397 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:05:56,401 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:05:56,803 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.755s
2025-09-05 01:05:56,806 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:05:56,807 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1749 tokens) in 0.002s
2025-09-05 01:06:04,062 - src.rag_pipeline - INFO - Generated response (55 tokens) in 7.252s
2025-09-05 01:06:04,227 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:06:04,278 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:06:04,285 - src.experiment_runner - INFO - Completed run 106/510: 8.55s
2025-09-05 01:06:04,286 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:06:04,286 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:06:04,286 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:06:04,286 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:06:04,286 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:06:04,286 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:06:04,286 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:06:04,292 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:06:04,292 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:06:04,292 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:06:04,292 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:06:04,292 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:06:04,292 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:06:04,807 - src.llm_wrapper - INFO - Model ready in 0.52s
2025-09-05 01:06:04,808 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:06:04,808 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:06:04,808 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:06:04,808 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:06:04,808 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:06:04,808 - src.rag_pipeline - INFO - Processing validated query: 'Explain computer vision techniques.'
2025-09-05 01:06:04,817 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain computer vision techniques....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.89it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.88it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.84it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.84it/s, batch=1/1, memory=N/A]
2025-09-05 01:06:05,572 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:06:05,577 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:06:06,060 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.243s
2025-09-05 01:06:06,068 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:06:06,069 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1297 tokens) in 0.008s
2025-09-05 01:06:24,420 - src.rag_pipeline - INFO - Generated response (318 tokens) in 18.336s
2025-09-05 01:06:24,601 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:06:24,654 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:06:24,663 - src.experiment_runner - INFO - Completed run 107/510: 20.14s
2025-09-05 01:06:24,663 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:06:24,663 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:06:24,663 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:06:24,663 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:06:24,663 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:06:24,663 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:06:24,663 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:06:24,669 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:06:24,669 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:06:24,669 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:06:24,669 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:06:24,670 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:06:24,670 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:06:25,165 - src.llm_wrapper - INFO - Model ready in 0.50s
2025-09-05 01:06:25,165 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:06:25,165 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:06:25,165 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:06:25,165 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:06:25,165 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:06:25,165 - src.rag_pipeline - INFO - Processing validated query: 'What is reinforcement learning?'
2025-09-05 01:06:25,172 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is reinforcement learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.18it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.18it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.16it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.16it/s, batch=1/1, memory=N/A]
2025-09-05 01:06:25,623 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:06:25,624 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:06:26,235 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.063s
2025-09-05 01:06:26,241 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:06:26,241 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1236 tokens) in 0.005s
2025-09-05 01:06:37,877 - src.rag_pipeline - INFO - Generated response (180 tokens) in 11.631s
2025-09-05 01:06:38,035 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:06:38,090 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:06:38,105 - src.experiment_runner - INFO - Completed run 108/510: 13.21s
2025-09-05 01:06:38,105 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:06:38,105 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:06:38,105 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:06:38,105 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:06:38,105 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:06:38,105 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:06:38,105 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:06:38,114 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:06:38,114 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:06:38,114 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:06:38,114 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:06:38,115 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:06:38,115 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:06:38,601 - src.llm_wrapper - INFO - Model ready in 0.49s
2025-09-05 01:06:38,601 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:06:38,601 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:06:38,601 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:06:38,601 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:06:38,601 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:06:38,601 - src.rag_pipeline - INFO - Processing validated query: 'How does data preprocessing work?'
2025-09-05 01:06:38,609 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does data preprocessing work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.93it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.93it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.90it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.90it/s, batch=1/1, memory=N/A]
2025-09-05 01:06:38,987 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:06:38,988 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:06:39,574 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.964s
2025-09-05 01:06:39,575 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:06:39,576 - src.rag_pipeline - INFO - Built prompt with 5 contexts (909 tokens) in 0.002s
2025-09-05 01:06:48,269 - src.rag_pipeline - INFO - Generated response (139 tokens) in 8.692s
2025-09-05 01:06:48,385 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:06:48,430 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:06:48,447 - src.experiment_runner - INFO - Completed run 109/510: 10.16s
2025-09-05 01:06:48,447 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:06:48,447 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:06:48,447 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:06:48,447 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:06:48,447 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:06:48,447 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:06:48,447 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:06:48,453 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:06:48,453 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:06:48,454 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:06:48,454 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:06:48,454 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:06:48,454 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:06:48,947 - src.llm_wrapper - INFO - Model ready in 0.49s
2025-09-05 01:06:48,947 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:06:48,947 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:06:48,947 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:06:48,947 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:06:48,947 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:06:48,947 - src.rag_pipeline - INFO - Processing validated query: 'What are the challenges in AI development?'
2025-09-05 01:06:48,954 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are the challenges in AI development?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.75it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.73it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.67it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.66it/s, batch=1/1, memory=N/A]
2025-09-05 01:06:49,355 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:06:49,356 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:06:49,917 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.963s
2025-09-05 01:06:49,919 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:06:49,919 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1445 tokens) in 0.002s
2025-09-05 01:07:04,836 - src.rag_pipeline - INFO - Generated response (237 tokens) in 14.914s
2025-09-05 01:07:04,975 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:07:05,021 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:07:05,066 - src.experiment_runner - INFO - Completed run 110/510: 16.39s
2025-09-05 01:07:05,066 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:07:05,066 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:07:05,066 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:07:05,066 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:07:05,066 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:07:05,066 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:07:05,066 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:07:05,075 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:07:05,075 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:07:05,076 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:07:05,076 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:07:05,076 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:07:05,076 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:07:05,574 - src.llm_wrapper - INFO - Model ready in 0.50s
2025-09-05 01:07:05,574 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:07:05,574 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:07:05,574 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:07:05,574 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:07:05,574 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:07:05,574 - src.rag_pipeline - INFO - Processing validated query: 'What is machine learning?'
2025-09-05 01:07:05,585 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is machine learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.84it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.83it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.78it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.78it/s, batch=1/1, memory=N/A]
2025-09-05 01:07:05,980 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:07:05,981 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:07:06,600 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.015s
2025-09-05 01:07:06,602 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:07:06,602 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1162 tokens) in 0.002s
2025-09-05 01:07:12,224 - src.rag_pipeline - INFO - Generated response (59 tokens) in 5.621s
2025-09-05 01:07:12,361 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:07:12,412 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:07:12,419 - src.experiment_runner - INFO - Completed run 111/510: 7.16s
2025-09-05 01:07:12,419 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:07:12,419 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:07:12,419 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:07:12,419 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:07:12,419 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:07:12,419 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:07:12,419 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:07:12,424 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:07:12,425 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:07:12,425 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:07:12,425 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:07:12,425 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:07:12,425 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:07:12,917 - src.llm_wrapper - INFO - Model ready in 0.49s
2025-09-05 01:07:12,918 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:07:12,918 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:07:12,918 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:07:12,918 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:07:12,918 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:07:12,918 - src.rag_pipeline - INFO - Processing validated query: 'How does artificial intelligence work?'
2025-09-05 01:07:12,926 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does artificial intelligence work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.17it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.07it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.97it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.96it/s, batch=1/1, memory=N/A]
2025-09-05 01:07:13,214 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:07:13,215 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:07:13,626 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.700s
2025-09-05 01:07:13,631 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:07:13,632 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1211 tokens) in 0.005s
2025-09-05 01:07:30,314 - src.rag_pipeline - INFO - Generated response (287 tokens) in 16.680s
2025-09-05 01:07:30,470 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:07:30,524 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:07:30,538 - src.experiment_runner - INFO - Completed run 112/510: 17.90s
2025-09-05 01:07:30,538 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:07:30,538 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:07:30,538 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:07:30,538 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:07:30,538 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:07:30,538 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:07:30,538 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:07:30,543 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:07:30,543 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:07:30,543 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:07:30,543 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:07:30,544 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:07:30,544 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:07:31,050 - src.llm_wrapper - INFO - Model ready in 0.51s
2025-09-05 01:07:31,051 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:07:31,051 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:07:31,051 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:07:31,051 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:07:31,051 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:07:31,051 - src.rag_pipeline - INFO - Processing validated query: 'Explain deep learning algorithms.'
2025-09-05 01:07:31,058 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain deep learning algorithms....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.53it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.52it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.48it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.48it/s, batch=1/1, memory=N/A]
2025-09-05 01:07:31,471 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:07:31,474 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:07:31,855 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.797s
2025-09-05 01:07:31,857 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:07:31,858 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1198 tokens) in 0.002s
2025-09-05 01:07:36,983 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.124s
2025-09-05 01:07:37,122 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:07:37,171 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:07:37,176 - src.experiment_runner - INFO - Completed run 113/510: 6.45s
2025-09-05 01:07:37,176 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:07:37,176 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:07:37,176 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:07:37,176 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:07:37,176 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:07:37,176 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:07:37,176 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:07:37,181 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:07:37,181 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:07:37,181 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:07:37,182 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:07:37,182 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:07:37,182 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:07:37,698 - src.llm_wrapper - INFO - Model ready in 0.52s
2025-09-05 01:07:37,698 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:07:37,698 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:07:37,698 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:07:37,698 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:07:37,698 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:07:37,698 - src.rag_pipeline - INFO - Processing validated query: 'What are neural networks?'
2025-09-05 01:07:37,705 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are neural networks?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.43it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.40it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.29it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.29it/s, batch=1/1, memory=N/A]
2025-09-05 01:07:37,997 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:07:37,999 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:07:38,408 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.703s
2025-09-05 01:07:38,412 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:07:38,413 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1353 tokens) in 0.004s
2025-09-05 01:07:52,992 - src.rag_pipeline - INFO - Generated response (238 tokens) in 14.578s
2025-09-05 01:07:53,145 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:07:53,198 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:07:53,203 - src.experiment_runner - INFO - Completed run 114/510: 15.82s
2025-09-05 01:07:53,203 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:07:53,204 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:07:53,204 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:07:53,204 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:07:53,204 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:07:53,204 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:07:53,204 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:07:53,210 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:07:53,210 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:07:53,210 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:07:53,210 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:07:53,210 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:07:53,210 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:07:53,718 - src.llm_wrapper - INFO - Model ready in 0.51s
2025-09-05 01:07:53,718 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:07:53,718 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:07:53,718 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:07:53,718 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:07:53,718 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:07:53,718 - src.rag_pipeline - INFO - Processing validated query: 'How do large language models work?'
2025-09-05 01:07:53,725 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How do large language models work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.37it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.35it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.32it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.32it/s, batch=1/1, memory=N/A]
2025-09-05 01:07:54,082 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:07:54,083 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:07:54,454 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.730s
2025-09-05 01:07:54,457 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:07:54,457 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1337 tokens) in 0.002s
2025-09-05 01:07:59,871 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.413s
2025-09-05 01:08:00,019 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:08:00,071 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:08:00,077 - src.experiment_runner - INFO - Completed run 115/510: 6.67s
2025-09-05 01:08:00,077 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:08:00,077 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:08:00,077 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:08:00,077 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:08:00,077 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:08:00,077 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:08:00,077 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:08:00,082 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:08:00,082 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:08:00,083 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:08:00,083 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:08:00,083 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:08:00,083 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:08:00,599 - src.llm_wrapper - INFO - Model ready in 0.52s
2025-09-05 01:08:00,599 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:08:00,599 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:08:00,599 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:08:00,599 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:08:00,599 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:08:00,599 - src.rag_pipeline - INFO - Processing validated query: 'What is natural language processing?'
2025-09-05 01:08:00,605 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is natural language processing?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.14it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.12it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.01it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.01it/s, batch=1/1, memory=N/A]
2025-09-05 01:08:00,885 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:08:00,886 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:08:01,268 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.662s
2025-09-05 01:08:01,270 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:08:01,270 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1749 tokens) in 0.002s
2025-09-05 01:08:08,165 - src.rag_pipeline - INFO - Generated response (55 tokens) in 6.894s
2025-09-05 01:08:08,305 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:08:08,357 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:08:08,363 - src.experiment_runner - INFO - Completed run 116/510: 8.09s
2025-09-05 01:08:08,363 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:08:08,363 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:08:08,363 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:08:08,363 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:08:08,363 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:08:08,363 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:08:08,363 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:08:08,369 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:08:08,369 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:08:08,369 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:08:08,369 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:08:08,370 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:08:08,370 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:08:08,867 - src.llm_wrapper - INFO - Model ready in 0.50s
2025-09-05 01:08:08,867 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:08:08,867 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:08:08,867 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:08:08,867 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:08:08,867 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:08:08,867 - src.rag_pipeline - INFO - Processing validated query: 'Explain computer vision techniques.'
2025-09-05 01:08:08,875 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain computer vision techniques....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.98it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.89it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.64it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.64it/s, batch=1/1, memory=N/A]
2025-09-05 01:08:09,176 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:08:09,178 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:08:09,621 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.746s
2025-09-05 01:08:09,623 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:08:09,624 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1297 tokens) in 0.002s
2025-09-05 01:08:27,885 - src.rag_pipeline - INFO - Generated response (318 tokens) in 18.258s
2025-09-05 01:08:28,033 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:08:28,090 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:08:28,104 - src.experiment_runner - INFO - Completed run 117/510: 19.52s
2025-09-05 01:08:28,105 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:08:28,105 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:08:28,105 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:08:28,105 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:08:28,105 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:08:28,105 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:08:28,105 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:08:28,110 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:08:28,110 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:08:28,110 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:08:28,110 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:08:28,110 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:08:28,110 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:08:28,654 - src.llm_wrapper - INFO - Model ready in 0.54s
2025-09-05 01:08:28,654 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:08:28,654 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:08:28,654 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:08:28,654 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:08:28,654 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:08:28,654 - src.rag_pipeline - INFO - Processing validated query: 'What is reinforcement learning?'
2025-09-05 01:08:28,661 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is reinforcement learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.09it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.08it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.03it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.03it/s, batch=1/1, memory=N/A]
2025-09-05 01:08:29,033 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:08:29,034 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:08:29,545 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.884s
2025-09-05 01:08:29,548 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:08:29,549 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1236 tokens) in 0.003s
2025-09-05 01:08:41,149 - src.rag_pipeline - INFO - Generated response (180 tokens) in 11.599s
2025-09-05 01:08:41,337 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:08:41,387 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:08:41,392 - src.experiment_runner - INFO - Completed run 118/510: 13.05s
2025-09-05 01:08:41,392 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:08:41,392 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:08:41,392 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:08:41,392 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:08:41,392 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:08:41,392 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:08:41,392 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:08:41,398 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:08:41,398 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:08:41,398 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:08:41,398 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:08:41,398 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:08:41,398 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:08:41,893 - src.llm_wrapper - INFO - Model ready in 0.49s
2025-09-05 01:08:41,893 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:08:41,893 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:08:41,893 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:08:41,893 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:08:41,893 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:08:41,893 - src.rag_pipeline - INFO - Processing validated query: 'How does data preprocessing work?'
2025-09-05 01:08:41,900 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does data preprocessing work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.22it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.20it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.14it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.14it/s, batch=1/1, memory=N/A]
2025-09-05 01:08:42,255 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:08:42,256 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:08:42,622 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.722s
2025-09-05 01:08:42,623 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:08:42,624 - src.rag_pipeline - INFO - Built prompt with 5 contexts (909 tokens) in 0.002s
2025-09-05 01:08:51,389 - src.rag_pipeline - INFO - Generated response (139 tokens) in 8.764s
2025-09-05 01:08:51,545 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:08:51,595 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:08:51,617 - src.experiment_runner - INFO - Completed run 119/510: 10.00s
2025-09-05 01:08:51,617 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:08:51,617 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:08:51,617 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:08:51,617 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:08:51,617 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:08:51,617 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:08:51,617 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:08:51,624 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:08:51,625 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:08:51,625 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:08:51,625 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:08:51,625 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:08:51,625 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:08:52,167 - src.llm_wrapper - INFO - Model ready in 0.54s
2025-09-05 01:08:52,167 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:08:52,167 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:08:52,167 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:08:52,167 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:08:52,167 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:08:52,167 - src.rag_pipeline - INFO - Processing validated query: 'What are the challenges in AI development?'
2025-09-05 01:08:52,174 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are the challenges in AI development?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  8.53it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  8.49it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  8.37it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  8.37it/s, batch=1/1, memory=N/A]
2025-09-05 01:08:52,450 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:08:52,452 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:08:52,878 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.704s
2025-09-05 01:08:52,880 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:08:52,880 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1445 tokens) in 0.002s
2025-09-05 01:09:08,098 - src.rag_pipeline - INFO - Generated response (237 tokens) in 15.216s
2025-09-05 01:09:08,261 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:09:08,322 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:09:08,347 - src.experiment_runner - INFO - Completed run 120/510: 16.48s
2025-09-05 01:09:08,347 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:09:08,347 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:09:08,347 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:09:08,347 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:09:08,347 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:09:08,347 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:09:08,347 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:09:08,353 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:09:08,354 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:09:08,354 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:09:08,354 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:09:08,354 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:09:08,354 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:09:08,874 - src.llm_wrapper - INFO - Model ready in 0.52s
2025-09-05 01:09:08,874 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:09:08,874 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:09:08,874 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:09:08,874 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:09:08,874 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:09:08,874 - src.rag_pipeline - INFO - Processing validated query: 'What is machine learning?'
2025-09-05 01:09:08,902 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is machine learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.98it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.86it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.79it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.79it/s, batch=1/1, memory=N/A]
2025-09-05 01:09:09,301 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:09:09,332 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:09:09,717 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.815s
2025-09-05 01:09:09,721 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:09:09,722 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1162 tokens) in 0.003s
2025-09-05 01:09:15,371 - src.rag_pipeline - INFO - Generated response (59 tokens) in 5.648s
2025-09-05 01:09:15,502 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:09:15,547 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:09:15,572 - src.experiment_runner - INFO - Completed run 121/510: 7.02s
2025-09-05 01:09:15,572 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:09:15,572 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:09:15,572 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:09:15,572 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:09:15,572 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:09:15,572 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:09:15,572 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:09:15,580 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:09:15,580 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:09:15,580 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:09:15,580 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:09:15,581 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:09:15,581 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:09:16,152 - src.llm_wrapper - INFO - Model ready in 0.57s
2025-09-05 01:09:16,152 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:09:16,152 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:09:16,152 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:09:16,152 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:09:16,152 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:09:16,152 - src.rag_pipeline - INFO - Processing validated query: 'How does artificial intelligence work?'
2025-09-05 01:09:16,159 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does artificial intelligence work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.04it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.01it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.96it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.96it/s, batch=1/1, memory=N/A]
2025-09-05 01:09:16,524 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:09:16,526 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:09:16,918 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.759s
2025-09-05 01:09:16,921 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:09:16,922 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1211 tokens) in 0.003s
2025-09-05 01:09:33,435 - src.rag_pipeline - INFO - Generated response (287 tokens) in 16.511s
2025-09-05 01:09:33,576 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:09:33,628 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:09:33,633 - src.experiment_runner - INFO - Completed run 122/510: 17.86s
2025-09-05 01:09:33,634 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:09:33,634 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:09:33,634 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:09:33,634 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:09:33,634 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:09:33,634 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:09:33,634 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:09:33,639 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:09:33,639 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:09:33,639 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:09:33,639 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:09:33,639 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:09:33,639 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:09:34,139 - src.llm_wrapper - INFO - Model ready in 0.50s
2025-09-05 01:09:34,139 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:09:34,139 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:09:34,139 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:09:34,139 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:09:34,139 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:09:34,139 - src.rag_pipeline - INFO - Processing validated query: 'Explain deep learning algorithms.'
2025-09-05 01:09:34,147 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain deep learning algorithms....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.44it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.43it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.41it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.41it/s, batch=1/1, memory=N/A]
2025-09-05 01:09:34,569 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:09:34,573 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:09:34,934 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.787s
2025-09-05 01:09:34,936 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:09:34,936 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1198 tokens) in 0.002s
2025-09-05 01:09:40,097 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.159s
2025-09-05 01:09:40,275 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:09:40,324 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:09:40,330 - src.experiment_runner - INFO - Completed run 123/510: 6.47s
2025-09-05 01:09:40,330 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:09:40,330 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:09:40,330 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:09:40,330 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:09:40,330 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:09:40,330 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:09:40,330 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:09:40,335 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:09:40,336 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:09:40,336 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:09:40,336 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:09:40,336 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:09:40,336 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:09:40,840 - src.llm_wrapper - INFO - Model ready in 0.50s
2025-09-05 01:09:40,840 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:09:40,840 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:09:40,840 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:09:40,840 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:09:40,840 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:09:40,840 - src.rag_pipeline - INFO - Processing validated query: 'What are neural networks?'
2025-09-05 01:09:40,846 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are neural networks?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.25it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.16it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.07it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.07it/s, batch=1/1, memory=N/A]
2025-09-05 01:09:41,285 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:09:41,286 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:09:41,756 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.910s
2025-09-05 01:09:41,759 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:09:41,760 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1353 tokens) in 0.002s
2025-09-05 01:09:56,450 - src.rag_pipeline - INFO - Generated response (238 tokens) in 14.683s
2025-09-05 01:09:56,687 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:09:56,740 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:09:56,757 - src.experiment_runner - INFO - Completed run 124/510: 16.12s
2025-09-05 01:09:56,757 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:09:56,757 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:09:56,757 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:09:56,758 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:09:56,758 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:09:56,758 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:09:56,758 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:09:56,765 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:09:56,766 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:09:56,766 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:09:56,766 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:09:56,766 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:09:56,766 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:09:57,277 - src.llm_wrapper - INFO - Model ready in 0.51s
2025-09-05 01:09:57,277 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:09:57,277 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:09:57,277 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:09:57,277 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:09:57,277 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:09:57,277 - src.rag_pipeline - INFO - Processing validated query: 'How do large language models work?'
2025-09-05 01:09:57,286 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How do large language models work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.38it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.36it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.32it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.32it/s, batch=1/1, memory=N/A]
2025-09-05 01:09:57,851 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:09:57,855 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:09:58,259 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.973s
2025-09-05 01:09:58,266 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:09:58,267 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1337 tokens) in 0.007s
2025-09-05 01:10:03,752 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.483s
2025-09-05 01:10:03,904 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:10:03,961 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:10:03,973 - src.experiment_runner - INFO - Completed run 125/510: 6.99s
2025-09-05 01:10:03,973 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:10:03,973 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:10:03,973 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:10:03,974 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:10:03,974 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:10:03,974 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:10:03,974 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:10:03,979 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:10:03,979 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:10:03,979 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:10:03,979 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:10:03,979 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:10:03,979 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:10:04,502 - src.llm_wrapper - INFO - Model ready in 0.52s
2025-09-05 01:10:04,502 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:10:04,502 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:10:04,502 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:10:04,502 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:10:04,502 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:10:04,502 - src.rag_pipeline - INFO - Processing validated query: 'What is natural language processing?'
2025-09-05 01:10:04,511 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is natural language processing?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  7.74it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  7.73it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  7.63it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  7.63it/s, batch=1/1, memory=N/A]
2025-09-05 01:10:04,755 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:10:04,757 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:10:05,215 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.704s
2025-09-05 01:10:05,217 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:10:05,218 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1749 tokens) in 0.002s
2025-09-05 01:10:12,176 - src.rag_pipeline - INFO - Generated response (55 tokens) in 6.956s
2025-09-05 01:10:12,379 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:10:12,430 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:10:12,458 - src.experiment_runner - INFO - Completed run 126/510: 8.20s
2025-09-05 01:10:12,458 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:10:12,458 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:10:12,458 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:10:12,458 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:10:12,458 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:10:12,458 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:10:12,458 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:10:12,464 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:10:12,464 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:10:12,464 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:10:12,464 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:10:12,465 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:10:12,465 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:10:13,008 - src.llm_wrapper - INFO - Model ready in 0.54s
2025-09-05 01:10:13,008 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:10:13,008 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:10:13,008 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:10:13,008 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:10:13,008 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:10:13,008 - src.rag_pipeline - INFO - Processing validated query: 'Explain computer vision techniques.'
2025-09-05 01:10:13,015 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain computer vision techniques....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.21it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.19it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.14it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.13it/s, batch=1/1, memory=N/A]
2025-09-05 01:10:13,381 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:10:13,390 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:10:13,917 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.902s
2025-09-05 01:10:13,921 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:10:13,924 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1297 tokens) in 0.005s
2025-09-05 01:10:32,483 - src.rag_pipeline - INFO - Generated response (318 tokens) in 18.557s
2025-09-05 01:10:32,667 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:10:32,718 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:10:32,726 - src.experiment_runner - INFO - Completed run 127/510: 20.03s
2025-09-05 01:10:32,726 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:10:32,726 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:10:32,726 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:10:32,726 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:10:32,726 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:10:32,726 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:10:32,726 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:10:32,732 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:10:32,732 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:10:32,732 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:10:32,732 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:10:32,732 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:10:32,732 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:10:33,245 - src.llm_wrapper - INFO - Model ready in 0.51s
2025-09-05 01:10:33,245 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:10:33,245 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:10:33,245 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:10:33,245 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:10:33,245 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:10:33,245 - src.rag_pipeline - INFO - Processing validated query: 'What is reinforcement learning?'
2025-09-05 01:10:33,253 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is reinforcement learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.43it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.41it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.39it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.39it/s, batch=1/1, memory=N/A]
2025-09-05 01:10:33,683 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:10:33,684 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:10:34,060 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.806s
2025-09-05 01:10:34,062 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:10:34,062 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1236 tokens) in 0.002s
2025-09-05 01:10:45,771 - src.rag_pipeline - INFO - Generated response (180 tokens) in 11.702s
2025-09-05 01:10:45,949 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:10:46,011 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:10:46,017 - src.experiment_runner - INFO - Completed run 128/510: 13.05s
2025-09-05 01:10:46,017 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:10:46,017 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:10:46,017 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:10:46,017 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:10:46,017 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:10:46,017 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:10:46,017 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:10:46,026 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:10:46,026 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:10:46,027 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:10:46,027 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:10:46,027 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:10:46,027 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:10:46,533 - src.llm_wrapper - INFO - Model ready in 0.51s
2025-09-05 01:10:46,533 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:10:46,533 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:10:46,533 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:10:46,533 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:10:46,533 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:10:46,533 - src.rag_pipeline - INFO - Processing validated query: 'How does data preprocessing work?'
2025-09-05 01:10:46,540 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does data preprocessing work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.69it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.68it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.63it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.63it/s, batch=1/1, memory=N/A]
2025-09-05 01:10:46,872 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:10:46,874 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:10:47,295 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.755s
2025-09-05 01:10:47,298 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:10:47,298 - src.rag_pipeline - INFO - Built prompt with 5 contexts (909 tokens) in 0.002s
2025-09-05 01:10:56,137 - src.rag_pipeline - INFO - Generated response (139 tokens) in 8.837s
2025-09-05 01:10:56,307 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:10:56,354 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:10:56,376 - src.experiment_runner - INFO - Completed run 129/510: 10.12s
2025-09-05 01:10:56,377 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:10:56,377 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:10:56,377 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:10:56,377 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:10:56,377 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:10:56,377 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:10:56,377 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:10:56,383 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:10:56,383 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:10:56,383 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:10:56,383 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:10:56,383 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:10:56,383 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:10:56,930 - src.llm_wrapper - INFO - Model ready in 0.55s
2025-09-05 01:10:56,930 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:10:56,930 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:10:56,930 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:10:56,930 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:10:56,930 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:10:56,930 - src.rag_pipeline - INFO - Processing validated query: 'What are the challenges in AI development?'
2025-09-05 01:10:56,940 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are the challenges in AI development?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.96it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.95it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.92it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.92it/s, batch=1/1, memory=N/A]
2025-09-05 01:10:57,435 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:10:57,436 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:10:57,931 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.991s
2025-09-05 01:10:57,936 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:10:57,937 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1445 tokens) in 0.003s
2025-09-05 01:28:45,924 - src.rag_pipeline - INFO - Generated response (237 tokens) in 1067.978s
2025-09-05 01:28:46,194 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:28:46,251 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:28:46,264 - src.experiment_runner - INFO - Completed run 130/510: 1069.55s
2025-09-05 01:28:46,264 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:28:46,264 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:28:46,264 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:28:46,264 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:28:46,264 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:28:46,264 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:28:46,264 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:28:46,271 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:28:46,271 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:28:46,271 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:28:46,271 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:28:46,272 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:28:46,272 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:28:46,891 - src.llm_wrapper - INFO - Model ready in 0.62s
2025-09-05 01:28:46,891 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:28:46,891 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:28:46,891 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:28:46,891 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:28:46,891 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:28:46,891 - src.rag_pipeline - INFO - Processing validated query: 'What is machine learning?'
2025-09-05 01:28:46,898 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is machine learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.18it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.17it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.13it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.13it/s, batch=1/1, memory=N/A]
2025-09-05 01:28:47,460 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:28:47,462 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:28:48,002 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.104s
2025-09-05 01:28:48,010 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:28:48,011 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1162 tokens) in 0.008s
2025-09-05 01:28:53,575 - src.rag_pipeline - INFO - Generated response (59 tokens) in 5.558s
2025-09-05 01:28:53,840 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:28:53,905 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:28:53,920 - src.experiment_runner - INFO - Completed run 131/510: 7.31s
2025-09-05 01:28:53,920 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:28:53,920 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:28:53,920 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:28:53,920 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:28:53,920 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:28:53,920 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:28:53,920 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:28:53,925 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:28:53,926 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:28:53,926 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:28:53,926 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:28:53,926 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:28:53,926 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:28:54,499 - src.llm_wrapper - INFO - Model ready in 0.57s
2025-09-05 01:28:54,499 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:28:54,499 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:28:54,499 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:28:54,499 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:28:54,500 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:28:54,500 - src.rag_pipeline - INFO - Processing validated query: 'How does artificial intelligence work?'
2025-09-05 01:28:54,507 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does artificial intelligence work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.22it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.20it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.12it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.12it/s, batch=1/1, memory=N/A]
2025-09-05 01:28:55,288 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:28:55,290 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:28:55,701 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.193s
2025-09-05 01:28:55,713 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:28:55,713 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1211 tokens) in 0.011s
2025-09-05 01:29:13,588 - src.rag_pipeline - INFO - Generated response (287 tokens) in 17.864s
2025-09-05 01:29:13,910 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:29:13,980 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:29:13,997 - src.experiment_runner - INFO - Completed run 132/510: 19.67s
2025-09-05 01:29:13,997 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:29:13,997 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:29:13,997 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:29:13,997 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:29:13,997 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:29:13,997 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:29:13,998 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:29:14,003 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:29:14,004 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:29:14,004 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:29:14,004 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:29:14,004 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:29:14,004 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:29:14,631 - src.llm_wrapper - INFO - Model ready in 0.63s
2025-09-05 01:29:14,631 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:29:14,631 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:29:14,631 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:29:14,631 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:29:14,632 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:29:14,632 - src.rag_pipeline - INFO - Processing validated query: 'Explain deep learning algorithms.'
2025-09-05 01:29:14,641 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain deep learning algorithms....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.98it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.97it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.92it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.92it/s, batch=1/1, memory=N/A]
2025-09-05 01:29:15,298 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:29:15,302 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:29:15,858 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.217s
2025-09-05 01:29:15,866 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:29:15,868 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1198 tokens) in 0.008s
2025-09-05 01:29:21,408 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.533s
2025-09-05 01:29:21,692 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:29:21,776 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:29:21,786 - src.experiment_runner - INFO - Completed run 133/510: 7.41s
2025-09-05 01:29:21,786 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:29:21,786 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:29:21,786 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:29:21,786 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:29:21,786 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:29:21,786 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:29:21,786 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:29:21,796 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:29:21,797 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:29:21,797 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:29:21,797 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:29:21,797 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:29:21,797 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:29:22,412 - src.llm_wrapper - INFO - Model ready in 0.62s
2025-09-05 01:29:22,413 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:29:22,413 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:29:22,413 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:29:22,413 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:29:22,413 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:29:22,413 - src.rag_pipeline - INFO - Processing validated query: 'What are neural networks?'
2025-09-05 01:29:22,421 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are neural networks?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.54it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.54it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.52it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.52it/s, batch=1/1, memory=N/A]
2025-09-05 01:29:22,831 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:29:22,835 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:29:23,282 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.861s
2025-09-05 01:29:23,292 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:29:23,292 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1353 tokens) in 0.008s
2025-09-05 01:29:38,566 - src.rag_pipeline - INFO - Generated response (238 tokens) in 15.261s
2025-09-05 01:29:38,908 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:29:38,990 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:29:39,018 - src.experiment_runner - INFO - Completed run 134/510: 16.78s
2025-09-05 01:29:39,019 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:29:39,020 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:29:39,020 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:29:39,020 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:29:39,021 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:29:39,021 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:29:39,021 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:29:39,038 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:29:39,039 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:29:39,039 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:29:39,039 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:29:39,039 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:29:39,040 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:29:39,873 - src.llm_wrapper - INFO - Model ready in 0.83s
2025-09-05 01:29:39,877 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:29:39,877 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:29:39,877 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:29:39,877 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:29:39,877 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:29:39,877 - src.rag_pipeline - INFO - Processing validated query: 'How do large language models work?'
2025-09-05 01:29:39,893 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How do large language models work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.12it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.11it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.09it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.09it/s, batch=1/1, memory=N/A]
2025-09-05 01:29:40,487 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:29:40,493 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:29:41,014 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.120s
2025-09-05 01:29:41,018 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:29:41,019 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1337 tokens) in 0.004s
2025-09-05 01:29:47,637 - src.rag_pipeline - INFO - Generated response (47 tokens) in 6.609s
2025-09-05 01:29:47,997 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:29:48,086 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:29:48,096 - src.experiment_runner - INFO - Completed run 135/510: 8.62s
2025-09-05 01:29:48,096 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:29:48,097 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:29:48,097 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:29:48,097 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:29:48,097 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:29:48,097 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:29:48,097 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:29:48,108 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:29:48,108 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:29:48,108 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:29:48,108 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:29:48,109 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:29:48,109 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:29:48,992 - src.llm_wrapper - INFO - Model ready in 0.88s
2025-09-05 01:29:48,993 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:29:48,993 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:29:48,993 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:29:48,993 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:29:48,993 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:29:48,993 - src.rag_pipeline - INFO - Processing validated query: 'What is natural language processing?'
2025-09-05 01:29:49,005 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is natural language processing?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.69it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.69it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.68it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.67it/s, batch=1/1, memory=N/A]
2025-09-05 01:29:49,546 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:29:49,548 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:29:49,975 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.970s
2025-09-05 01:29:49,980 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:29:49,980 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1749 tokens) in 0.003s
2025-09-05 01:29:58,905 - src.rag_pipeline - INFO - Generated response (55 tokens) in 8.915s
2025-09-05 01:29:59,479 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:29:59,576 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:29:59,608 - src.experiment_runner - INFO - Completed run 136/510: 10.81s
2025-09-05 01:29:59,609 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:29:59,609 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:29:59,609 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:29:59,609 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:29:59,609 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:29:59,609 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:29:59,609 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:29:59,626 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:29:59,627 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:29:59,627 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:29:59,627 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:29:59,628 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:29:59,628 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:30:00,617 - src.llm_wrapper - INFO - Model ready in 0.99s
2025-09-05 01:30:00,618 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:30:00,618 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:30:00,618 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:30:00,618 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:30:00,618 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:30:00,618 - src.rag_pipeline - INFO - Processing validated query: 'Explain computer vision techniques.'
2025-09-05 01:30:00,631 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain computer vision techniques....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.16it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.15it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.12it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.12it/s, batch=1/1, memory=N/A]
2025-09-05 01:30:01,158 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:30:01,161 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:30:01,637 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.006s
2025-09-05 01:30:01,640 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:30:01,640 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1297 tokens) in 0.003s
2025-09-05 01:30:24,743 - src.rag_pipeline - INFO - Generated response (318 tokens) in 23.091s
2025-09-05 01:30:25,477 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:30:25,649 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:30:25,678 - src.experiment_runner - INFO - Completed run 137/510: 25.14s
2025-09-05 01:30:25,678 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:30:25,678 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:30:25,679 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:30:25,679 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:30:25,679 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:30:25,679 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:30:25,679 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:30:25,695 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:30:25,696 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:30:25,696 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:30:25,696 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:30:25,697 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:30:25,697 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:30:26,838 - src.llm_wrapper - INFO - Model ready in 1.14s
2025-09-05 01:30:26,839 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:30:26,839 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:30:26,839 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:30:26,839 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:30:26,840 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:30:26,840 - src.rag_pipeline - INFO - Processing validated query: 'What is reinforcement learning?'
2025-09-05 01:30:26,857 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is reinforcement learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.07it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.07it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.06it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.06it/s, batch=1/1, memory=N/A]
2025-09-05 01:30:27,725 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:30:27,728 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:30:28,280 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.423s
2025-09-05 01:30:28,294 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:30:28,295 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1236 tokens) in 0.012s
2025-09-05 01:30:48,899 - src.rag_pipeline - INFO - Generated response (180 tokens) in 20.556s
2025-09-05 01:30:50,767 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:30:50,905 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:30:51,054 - src.experiment_runner - INFO - Completed run 138/510: 23.23s
2025-09-05 01:30:51,055 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:30:51,055 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:30:51,055 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:30:51,056 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:30:51,056 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:30:51,056 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:30:51,056 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:30:51,077 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:30:51,078 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:30:51,078 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:30:51,078 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:30:51,078 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:30:51,079 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:30:52,314 - src.llm_wrapper - INFO - Model ready in 1.24s
2025-09-05 01:30:52,315 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:30:52,315 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:30:52,315 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:30:52,315 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:30:52,315 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:30:52,315 - src.rag_pipeline - INFO - Processing validated query: 'How does data preprocessing work?'
2025-09-05 01:30:52,334 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does data preprocessing work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.32it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.31it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.29it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.29it/s, batch=1/1, memory=N/A]
2025-09-05 01:30:53,590 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:30:53,593 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:30:54,218 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.884s
2025-09-05 01:30:54,228 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:30:54,229 - src.rag_pipeline - INFO - Built prompt with 5 contexts (909 tokens) in 0.009s
2025-09-05 01:31:15,687 - src.rag_pipeline - INFO - Generated response (139 tokens) in 21.455s
2025-09-05 01:31:16,367 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:31:16,512 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:31:16,529 - src.experiment_runner - INFO - Completed run 139/510: 24.63s
2025-09-05 01:31:16,529 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:31:16,529 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:31:16,529 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:31:16,530 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:31:16,530 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:31:16,530 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:31:16,530 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:31:16,543 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:31:16,544 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:31:16,544 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:31:16,544 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:31:16,545 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:31:16,545 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:31:17,859 - src.llm_wrapper - INFO - Model ready in 1.31s
2025-09-05 01:31:17,859 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:31:17,859 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:31:17,859 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:31:17,859 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:31:17,859 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:31:17,859 - src.rag_pipeline - INFO - Processing validated query: 'What are the challenges in AI development?'
2025-09-05 01:31:17,880 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are the challenges in AI development?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.34it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.33it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.32it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.32it/s, batch=1/1, memory=N/A]
2025-09-05 01:31:19,041 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:31:19,044 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:31:20,223 - src.rag_pipeline - INFO - Retrieved 5 contexts in 2.343s
2025-09-05 01:31:20,237 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:31:20,240 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1445 tokens) in 0.012s
2025-09-05 01:47:17,961 - src.rag_pipeline - INFO - Generated response (237 tokens) in 957.704s
2025-09-05 01:47:18,182 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:47:18,251 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:47:18,270 - src.experiment_runner - INFO - Completed run 140/510: 961.43s
2025-09-05 01:47:18,270 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:47:18,271 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:47:18,271 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:47:18,271 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:47:18,271 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:47:18,271 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:47:18,271 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:47:18,279 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:47:18,279 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:47:18,279 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:47:18,279 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:47:18,279 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:47:18,279 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:47:18,898 - src.llm_wrapper - INFO - Model ready in 0.62s
2025-09-05 01:47:18,899 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:47:18,899 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:47:18,899 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:47:18,899 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:47:18,900 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:47:18,900 - src.rag_pipeline - INFO - Processing validated query: 'What is machine learning?'
2025-09-05 01:47:18,921 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is machine learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.02it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.01it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.96it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.96it/s, batch=1/1, memory=N/A]
2025-09-05 01:47:19,506 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:47:19,508 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:47:19,991 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.070s
2025-09-05 01:47:19,994 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:47:19,995 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1162 tokens) in 0.003s
2025-09-05 01:47:25,620 - src.rag_pipeline - INFO - Generated response (59 tokens) in 5.619s
2025-09-05 01:47:25,828 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:47:25,890 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:47:25,895 - src.experiment_runner - INFO - Completed run 141/510: 7.35s
2025-09-05 01:47:25,896 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:47:25,896 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:47:25,896 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:47:25,896 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:47:25,896 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:47:25,896 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:47:25,896 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:47:25,901 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:47:25,901 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:47:25,901 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:47:25,901 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:47:25,901 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:47:25,901 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:47:26,443 - src.llm_wrapper - INFO - Model ready in 0.54s
2025-09-05 01:47:26,443 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:47:26,443 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:47:26,443 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:47:26,443 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:47:26,443 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:47:26,443 - src.rag_pipeline - INFO - Processing validated query: 'How does artificial intelligence work?'
2025-09-05 01:47:26,453 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does artificial intelligence work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.00it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.98it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.91it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.91it/s, batch=1/1, memory=N/A]
2025-09-05 01:47:27,062 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:47:27,063 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:47:27,482 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.028s
2025-09-05 01:47:27,488 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:47:27,489 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1211 tokens) in 0.005s
2025-09-05 01:47:45,249 - src.rag_pipeline - INFO - Generated response (287 tokens) in 17.753s
2025-09-05 01:47:45,483 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:47:45,545 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:47:45,572 - src.experiment_runner - INFO - Completed run 142/510: 19.35s
2025-09-05 01:47:45,572 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:47:45,572 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:47:45,572 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:47:45,572 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:47:45,572 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:47:45,572 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:47:45,572 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:47:45,582 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:47:45,583 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:47:45,583 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:47:45,583 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:47:45,584 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:47:45,584 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:47:46,165 - src.llm_wrapper - INFO - Model ready in 0.58s
2025-09-05 01:47:46,165 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:47:46,165 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:47:46,165 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:47:46,165 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:47:46,165 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:47:46,165 - src.rag_pipeline - INFO - Processing validated query: 'Explain deep learning algorithms.'
2025-09-05 01:47:46,172 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain deep learning algorithms....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.07it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.06it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.02it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.02it/s, batch=1/1, memory=N/A]
2025-09-05 01:47:46,966 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:47:46,967 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:47:47,424 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.251s
2025-09-05 01:47:47,428 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:47:47,428 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1198 tokens) in 0.004s
2025-09-05 01:47:52,750 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.318s
2025-09-05 01:47:52,994 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:47:53,068 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:47:53,076 - src.experiment_runner - INFO - Completed run 143/510: 7.18s
2025-09-05 01:47:53,076 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:47:53,076 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:47:53,076 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:47:53,076 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:47:53,076 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:47:53,076 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:47:53,077 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:47:53,083 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:47:53,083 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:47:53,083 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:47:53,083 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:47:53,083 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:47:53,083 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:47:53,645 - src.llm_wrapper - INFO - Model ready in 0.56s
2025-09-05 01:47:53,645 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:47:53,645 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:47:53,645 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:47:53,645 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:47:53,645 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:47:53,645 - src.rag_pipeline - INFO - Processing validated query: 'What are neural networks?'
2025-09-05 01:47:53,654 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are neural networks?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.64it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.60it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.57it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.57it/s, batch=1/1, memory=N/A]
2025-09-05 01:47:54,088 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:47:54,089 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:47:54,543 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.889s
2025-09-05 01:47:54,546 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:47:54,546 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1353 tokens) in 0.003s
2025-09-05 01:48:09,610 - src.rag_pipeline - INFO - Generated response (238 tokens) in 15.046s
2025-09-05 01:48:09,934 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:48:10,022 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:48:10,047 - src.experiment_runner - INFO - Completed run 144/510: 16.53s
2025-09-05 01:48:10,047 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:48:10,047 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:48:10,048 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:48:10,048 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:48:10,048 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:48:10,048 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:48:10,048 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:48:10,058 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:48:10,058 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:48:10,059 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:48:10,059 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:48:10,059 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:48:10,059 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:48:10,728 - src.llm_wrapper - INFO - Model ready in 0.67s
2025-09-05 01:48:10,728 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:48:10,728 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:48:10,728 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:48:10,728 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:48:10,728 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:48:10,728 - src.rag_pipeline - INFO - Processing validated query: 'How do large language models work?'
2025-09-05 01:48:10,738 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How do large language models work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.31it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.31it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.28it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.28it/s, batch=1/1, memory=N/A]
2025-09-05 01:48:11,179 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:48:11,183 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:48:11,595 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.857s
2025-09-05 01:48:11,601 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:48:11,601 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1337 tokens) in 0.006s
2025-09-05 01:48:17,925 - src.rag_pipeline - INFO - Generated response (47 tokens) in 6.323s
2025-09-05 01:48:18,240 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:48:18,318 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:48:18,338 - src.experiment_runner - INFO - Completed run 145/510: 7.88s
2025-09-05 01:48:18,338 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:48:18,338 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:48:18,339 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:48:18,339 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:48:18,339 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:48:18,339 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:48:18,339 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:48:18,349 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:48:18,349 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:48:18,349 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:48:18,349 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:48:18,350 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:48:18,350 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:48:19,189 - src.llm_wrapper - INFO - Model ready in 0.84s
2025-09-05 01:48:19,189 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:48:19,189 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:48:19,189 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:48:19,189 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:48:19,189 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:48:19,189 - src.rag_pipeline - INFO - Processing validated query: 'What is natural language processing?'
2025-09-05 01:48:19,199 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is natural language processing?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.42it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.41it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.39it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.39it/s, batch=1/1, memory=N/A]
2025-09-05 01:48:19,658 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:48:19,660 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:48:20,098 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.898s
2025-09-05 01:48:20,102 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:48:20,102 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1749 tokens) in 0.003s
2025-09-05 01:48:28,719 - src.rag_pipeline - INFO - Generated response (55 tokens) in 8.608s
2025-09-05 01:48:29,185 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:48:29,282 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:48:29,293 - src.experiment_runner - INFO - Completed run 146/510: 10.38s
2025-09-05 01:48:29,293 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:48:29,294 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:48:29,294 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:48:29,294 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:48:29,294 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:48:29,294 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:48:29,294 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:48:29,306 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:48:29,306 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:48:29,306 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:48:29,306 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:48:29,307 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:48:29,307 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:48:30,225 - src.llm_wrapper - INFO - Model ready in 0.92s
2025-09-05 01:48:30,226 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:48:30,226 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:48:30,226 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:48:30,226 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:48:30,226 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:48:30,226 - src.rag_pipeline - INFO - Processing validated query: 'Explain computer vision techniques.'
2025-09-05 01:48:30,237 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain computer vision techniques....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.01it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.00it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.95it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.95it/s, batch=1/1, memory=N/A]
2025-09-05 01:48:30,757 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:48:30,758 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:48:31,171 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.933s
2025-09-05 01:48:31,176 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:48:31,178 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1297 tokens) in 0.005s
2025-09-05 01:48:52,447 - src.rag_pipeline - INFO - Generated response (318 tokens) in 21.260s
2025-09-05 01:48:53,006 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:48:53,135 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:48:53,150 - src.experiment_runner - INFO - Completed run 147/510: 23.16s
2025-09-05 01:48:53,151 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:48:53,151 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:48:53,151 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:48:53,151 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:48:53,151 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:48:53,151 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:48:53,151 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:48:53,164 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:48:53,165 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:48:53,165 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:48:53,165 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:48:53,165 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:48:53,166 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:48:54,112 - src.llm_wrapper - INFO - Model ready in 0.95s
2025-09-05 01:48:54,112 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:48:54,112 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:48:54,112 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:48:54,112 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:48:54,112 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:48:54,112 - src.rag_pipeline - INFO - Processing validated query: 'What is reinforcement learning?'
2025-09-05 01:48:54,125 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is reinforcement learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.24it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.22it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.20it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.19it/s, batch=1/1, memory=N/A]
2025-09-05 01:48:54,901 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:48:54,904 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:48:55,307 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.182s
2025-09-05 01:48:55,310 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:48:55,311 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1236 tokens) in 0.003s
2025-09-05 01:49:13,242 - src.rag_pipeline - INFO - Generated response (180 tokens) in 17.922s
2025-09-05 01:49:13,681 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:49:13,824 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:49:13,839 - src.experiment_runner - INFO - Completed run 148/510: 20.09s
2025-09-05 01:49:13,839 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:49:13,839 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:49:13,840 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:49:13,840 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:49:13,840 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:49:13,840 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:49:13,840 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:49:13,854 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:49:13,855 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:49:13,855 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:49:13,855 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:49:13,856 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:49:13,856 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:49:14,983 - src.llm_wrapper - INFO - Model ready in 1.13s
2025-09-05 01:49:14,983 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:49:14,983 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:49:14,983 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:49:14,983 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:49:14,984 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:49:14,984 - src.rag_pipeline - INFO - Processing validated query: 'How does data preprocessing work?'
2025-09-05 01:49:14,997 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does data preprocessing work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.50it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.48it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.47it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.46it/s, batch=1/1, memory=N/A]
2025-09-05 01:49:15,587 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:49:15,589 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:49:16,027 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.030s
2025-09-05 01:49:16,030 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:49:16,030 - src.rag_pipeline - INFO - Built prompt with 5 contexts (909 tokens) in 0.002s
2025-09-05 01:49:33,001 - src.rag_pipeline - INFO - Generated response (139 tokens) in 16.957s
2025-09-05 01:49:33,656 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:49:33,866 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:49:33,880 - src.experiment_runner - INFO - Completed run 149/510: 19.16s
2025-09-05 01:49:33,881 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:49:33,881 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:49:33,881 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:49:33,881 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:49:33,881 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:49:33,881 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:49:33,881 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:49:33,903 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:49:33,904 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:49:33,904 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:49:33,905 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:49:33,905 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:49:33,906 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:49:35,255 - src.llm_wrapper - INFO - Model ready in 1.35s
2025-09-05 01:49:35,255 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:49:35,255 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:49:35,255 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:49:35,255 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:49:35,255 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:49:35,255 - src.rag_pipeline - INFO - Processing validated query: 'What are the challenges in AI development?'
2025-09-05 01:49:35,275 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are the challenges in AI development?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.60it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.60it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.59it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.59it/s, batch=1/1, memory=N/A]
2025-09-05 01:49:36,199 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:49:36,206 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:49:36,786 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.511s
2025-09-05 01:49:36,798 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:49:36,799 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1445 tokens) in 0.010s
2025-09-05 01:50:15,210 - src.rag_pipeline - INFO - Generated response (237 tokens) in 38.356s
2025-09-05 01:50:16,223 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:50:16,426 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:50:16,450 - src.experiment_runner - INFO - Completed run 150/510: 41.41s
2025-09-05 01:50:16,451 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:50:16,451 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:50:16,452 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:50:16,452 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:50:16,453 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:50:16,453 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:50:16,453 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:50:16,474 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:50:16,475 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:50:16,475 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:50:16,475 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:50:16,476 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:50:16,476 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 01:50:18,498 - src.llm_wrapper - INFO - Model ready in 2.02s
2025-09-05 01:50:18,498 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 01:50:18,498 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 01:50:18,499 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 01:50:18,499 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 01:50:18,499 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 01:50:18,499 - src.rag_pipeline - INFO - Processing validated query: 'What is machine learning?'
2025-09-05 01:50:18,532 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is machine learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.10s/it][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.10s/it]
Generating embeddings:   0%|          | 0/1 [00:01<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.11s/it, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.12s/it, batch=1/1, memory=N/A]
2025-09-05 01:50:20,357 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:50:20,365 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:50:21,152 - src.rag_pipeline - INFO - Retrieved 5 contexts in 2.620s
2025-09-05 01:50:21,167 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 01:50:21,169 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1162 tokens) in 0.014s
2025-09-05 01:50:41,517 - src.rag_pipeline - INFO - Generated response (59 tokens) in 20.345s
2025-09-05 01:50:42,444 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 01:50:42,677 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 01:50:42,725 - src.experiment_runner - INFO - Completed run 151/510: 25.07s
2025-09-05 01:50:42,725 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 01:50:42,725 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 01:50:42,726 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 01:50:42,726 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 01:50:42,726 - src.embedding_service - INFO -   Device: mps
2025-09-05 01:50:42,726 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 01:50:42,726 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 01:50:42,756 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 01:50:42,764 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 01:50:42,764 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 01:50:42,765 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 01:50:42,768 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 01:50:42,769 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 02:07:03,829 - src.llm_wrapper - INFO - Model ready in 981.06s
2025-09-05 02:07:03,829 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 02:07:03,829 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 02:07:03,829 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 02:07:03,829 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 02:07:03,829 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 02:07:03,830 - src.rag_pipeline - INFO - Processing validated query: 'How does artificial intelligence work?'
2025-09-05 02:07:03,851 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does artificial intelligence work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.91it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.91it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s, batch=1/1, memory=N/A]
2025-09-05 02:07:04,725 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:07:04,728 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:07:05,585 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.733s
2025-09-05 02:07:05,616 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 02:07:05,619 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1211 tokens) in 0.029s
2025-09-05 02:07:23,839 - src.rag_pipeline - INFO - Generated response (287 tokens) in 18.211s
2025-09-05 02:07:24,080 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 02:07:24,141 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 02:07:24,158 - src.experiment_runner - INFO - Completed run 152/510: 1001.12s
2025-09-05 02:07:24,158 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 02:07:24,158 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 02:07:24,158 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 02:07:24,158 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 02:07:24,158 - src.embedding_service - INFO -   Device: mps
2025-09-05 02:07:24,158 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 02:07:24,158 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 02:07:24,163 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:07:24,164 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 02:07:24,164 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 02:07:24,164 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 02:07:24,164 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 02:07:24,164 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 02:07:24,765 - src.llm_wrapper - INFO - Model ready in 0.60s
2025-09-05 02:07:24,765 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 02:07:24,765 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 02:07:24,765 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 02:07:24,765 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 02:07:24,765 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 02:07:24,765 - src.rag_pipeline - INFO - Processing validated query: 'Explain deep learning algorithms.'
2025-09-05 02:07:24,773 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain deep learning algorithms....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.40it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.39it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.36it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.36it/s, batch=1/1, memory=N/A]
2025-09-05 02:07:25,307 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:07:25,310 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:07:25,803 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.030s
2025-09-05 02:07:25,813 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 02:07:25,813 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1198 tokens) in 0.008s
2025-09-05 02:07:31,054 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.237s
2025-09-05 02:07:31,267 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 02:07:31,329 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 02:07:31,336 - src.experiment_runner - INFO - Completed run 153/510: 6.90s
2025-09-05 02:07:31,336 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 02:07:31,336 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 02:07:31,336 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 02:07:31,336 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 02:07:31,336 - src.embedding_service - INFO -   Device: mps
2025-09-05 02:07:31,336 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 02:07:31,337 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 02:07:31,344 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:07:31,345 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 02:07:31,345 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 02:07:31,345 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 02:07:31,345 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 02:07:31,345 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 02:07:31,933 - src.llm_wrapper - INFO - Model ready in 0.59s
2025-09-05 02:07:31,933 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 02:07:31,933 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 02:07:31,933 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 02:07:31,933 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 02:07:31,933 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 02:07:31,933 - src.rag_pipeline - INFO - Processing validated query: 'What are neural networks?'
2025-09-05 02:07:31,943 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are neural networks?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.82it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.81it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.78it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.78it/s, batch=1/1, memory=N/A]
2025-09-05 02:07:32,620 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:07:32,623 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:07:33,154 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.211s
2025-09-05 02:07:33,161 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 02:07:33,161 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1353 tokens) in 0.006s
2025-09-05 02:07:48,214 - src.rag_pipeline - INFO - Generated response (238 tokens) in 15.044s
2025-09-05 02:07:48,521 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 02:07:48,598 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 02:07:48,619 - src.experiment_runner - INFO - Completed run 154/510: 16.88s
2025-09-05 02:07:48,619 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 02:07:48,619 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 02:07:48,619 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 02:07:48,619 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 02:07:48,619 - src.embedding_service - INFO -   Device: mps
2025-09-05 02:07:48,619 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 02:07:48,619 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 02:07:48,628 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:07:48,628 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 02:07:48,628 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 02:07:48,628 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 02:07:48,629 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 02:07:48,629 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 02:07:49,205 - src.llm_wrapper - INFO - Model ready in 0.58s
2025-09-05 02:07:49,205 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 02:07:49,205 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 02:07:49,205 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 02:07:49,205 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 02:07:49,205 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 02:07:49,205 - src.rag_pipeline - INFO - Processing validated query: 'How do large language models work?'
2025-09-05 02:07:49,219 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How do large language models work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.25it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.24it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.22it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.22it/s, batch=1/1, memory=N/A]
2025-09-05 02:07:49,713 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:07:49,715 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:07:50,157 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.938s
2025-09-05 02:07:50,160 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 02:07:50,160 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1337 tokens) in 0.003s
2025-09-05 02:07:56,171 - src.rag_pipeline - INFO - Generated response (47 tokens) in 6.005s
2025-09-05 02:07:56,464 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 02:07:56,550 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 02:07:56,569 - src.experiment_runner - INFO - Completed run 155/510: 7.55s
2025-09-05 02:07:56,569 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 02:07:56,569 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 02:07:56,569 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 02:07:56,569 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 02:07:56,569 - src.embedding_service - INFO -   Device: mps
2025-09-05 02:07:56,569 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 02:07:56,570 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 02:07:56,578 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:07:56,579 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 02:07:56,579 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 02:07:56,579 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 02:07:56,580 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 02:07:56,580 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 02:07:57,278 - src.llm_wrapper - INFO - Model ready in 0.70s
2025-09-05 02:07:57,278 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 02:07:57,278 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 02:07:57,278 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 02:07:57,278 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 02:07:57,278 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 02:07:57,278 - src.rag_pipeline - INFO - Processing validated query: 'What is natural language processing?'
2025-09-05 02:07:57,287 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is natural language processing?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.11it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.07it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.03it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.03it/s, batch=1/1, memory=N/A]
2025-09-05 02:07:57,666 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:07:57,668 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:07:58,096 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.809s
2025-09-05 02:07:58,103 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 02:07:58,104 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1749 tokens) in 0.006s
2025-09-05 02:08:06,100 - src.rag_pipeline - INFO - Generated response (55 tokens) in 7.985s
2025-09-05 02:08:06,436 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 02:08:06,525 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 02:08:06,551 - src.experiment_runner - INFO - Completed run 156/510: 9.53s
2025-09-05 02:08:06,552 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 02:08:06,552 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 02:08:06,552 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 02:08:06,552 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 02:08:06,552 - src.embedding_service - INFO -   Device: mps
2025-09-05 02:08:06,552 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 02:08:06,552 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 02:08:06,563 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:08:06,564 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 02:08:06,564 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 02:08:06,564 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 02:08:06,564 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 02:08:06,565 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 02:08:07,441 - src.llm_wrapper - INFO - Model ready in 0.88s
2025-09-05 02:08:07,441 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 02:08:07,441 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 02:08:07,441 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 02:08:07,441 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 02:08:07,441 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 02:08:07,441 - src.rag_pipeline - INFO - Processing validated query: 'Explain computer vision techniques.'
2025-09-05 02:08:07,455 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain computer vision techniques....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.25it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.24it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.22it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.21it/s, batch=1/1, memory=N/A]
2025-09-05 02:08:07,970 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:08:07,973 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:08:08,423 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.968s
2025-09-05 02:08:08,426 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 02:08:08,427 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1297 tokens) in 0.002s
2025-09-05 02:08:28,136 - src.rag_pipeline - INFO - Generated response (318 tokens) in 19.699s
2025-09-05 02:08:28,542 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 02:08:28,632 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 02:08:28,653 - src.experiment_runner - INFO - Completed run 157/510: 21.59s
2025-09-05 02:08:28,654 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 02:08:28,654 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 02:08:28,654 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 02:08:28,654 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 02:08:28,654 - src.embedding_service - INFO -   Device: mps
2025-09-05 02:08:28,654 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 02:08:28,654 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 02:08:28,666 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:08:28,666 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 02:08:28,667 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 02:08:28,667 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 02:08:28,667 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 02:08:28,667 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 02:08:29,548 - src.llm_wrapper - INFO - Model ready in 0.88s
2025-09-05 02:08:29,548 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 02:08:29,549 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 02:08:29,549 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 02:08:29,549 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 02:08:29,549 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 02:08:29,549 - src.rag_pipeline - INFO - Processing validated query: 'What is reinforcement learning?'
2025-09-05 02:08:29,560 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is reinforcement learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.59it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.59it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.56it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.56it/s, batch=1/1, memory=N/A]
2025-09-05 02:08:30,118 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:08:30,121 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:08:30,546 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.986s
2025-09-05 02:08:30,549 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 02:08:30,550 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1236 tokens) in 0.002s
2025-09-05 02:08:45,282 - src.rag_pipeline - INFO - Generated response (180 tokens) in 14.722s
2025-09-05 02:08:45,909 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 02:08:46,085 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 02:08:46,097 - src.experiment_runner - INFO - Completed run 158/510: 16.63s
2025-09-05 02:08:46,098 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 02:08:46,098 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 02:08:46,098 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 02:08:46,098 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 02:08:46,098 - src.embedding_service - INFO -   Device: mps
2025-09-05 02:08:46,098 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 02:08:46,099 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 02:08:46,119 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:08:46,120 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 02:08:46,120 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 02:08:46,120 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 02:08:46,121 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 02:08:46,121 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 02:08:47,092 - src.llm_wrapper - INFO - Model ready in 0.97s
2025-09-05 02:08:47,092 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 02:08:47,093 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 02:08:47,093 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 02:08:47,093 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 02:08:47,093 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 02:08:47,093 - src.rag_pipeline - INFO - Processing validated query: 'How does data preprocessing work?'
2025-09-05 02:08:47,108 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does data preprocessing work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.36it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.35it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.33it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.33it/s, batch=1/1, memory=N/A]
2025-09-05 02:08:47,727 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:08:47,729 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:08:48,211 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.102s
2025-09-05 02:08:48,220 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 02:08:48,221 - src.rag_pipeline - INFO - Built prompt with 5 contexts (909 tokens) in 0.008s
2025-09-05 02:09:01,321 - src.rag_pipeline - INFO - Generated response (139 tokens) in 13.096s
2025-09-05 02:09:01,932 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 02:09:02,073 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 02:09:02,084 - src.experiment_runner - INFO - Completed run 159/510: 15.22s
2025-09-05 02:09:02,084 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 02:09:02,084 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 02:09:02,084 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 02:09:02,084 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 02:09:02,085 - src.embedding_service - INFO -   Device: mps
2025-09-05 02:09:02,085 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 02:09:02,085 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 02:09:02,098 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:09:02,099 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 02:09:02,099 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 02:09:02,099 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 02:09:02,100 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 02:09:02,100 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 02:09:03,185 - src.llm_wrapper - INFO - Model ready in 1.08s
2025-09-05 02:09:03,185 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 02:09:03,185 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 02:09:03,185 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 02:09:03,185 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 02:09:03,185 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 02:09:03,186 - src.rag_pipeline - INFO - Processing validated query: 'What are the challenges in AI development?'
2025-09-05 02:09:03,201 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are the challenges in AI development?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.98it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.93it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.92it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.92it/s, batch=1/1, memory=N/A]
2025-09-05 02:09:03,994 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:09:03,996 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:09:04,593 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.391s
2025-09-05 02:09:04,599 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 02:09:04,600 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1445 tokens) in 0.006s
2025-09-05 02:09:33,575 - src.rag_pipeline - INFO - Generated response (237 tokens) in 28.969s
2025-09-05 02:09:34,163 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 02:09:34,379 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 02:09:34,392 - src.experiment_runner - INFO - Completed run 160/510: 31.49s
2025-09-05 02:09:34,392 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 02:09:34,392 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 02:09:34,393 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 02:09:34,393 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 02:09:34,393 - src.embedding_service - INFO -   Device: mps
2025-09-05 02:09:34,393 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 02:09:34,393 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 02:09:34,415 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:09:34,415 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 02:09:34,415 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 02:09:34,415 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 02:09:34,420 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 02:09:34,420 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 02:09:35,766 - src.llm_wrapper - INFO - Model ready in 1.35s
2025-09-05 02:09:35,766 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 02:09:35,766 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 02:09:35,766 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 02:09:35,766 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 02:09:35,766 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 02:09:35,767 - src.rag_pipeline - INFO - Processing validated query: 'What is machine learning?'
2025-09-05 02:09:35,788 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is machine learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.61it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.61it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.60it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.60it/s, batch=1/1, memory=N/A]
2025-09-05 02:09:36,700 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:09:36,703 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:09:37,440 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.651s
2025-09-05 02:09:37,445 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 02:09:37,446 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1162 tokens) in 0.005s
2025-09-05 02:09:52,751 - src.rag_pipeline - INFO - Generated response (59 tokens) in 15.303s
2025-09-05 02:09:53,487 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 02:09:53,659 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 02:09:53,688 - src.experiment_runner - INFO - Completed run 161/510: 18.36s
2025-09-05 02:09:53,689 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 02:09:53,689 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 02:09:53,689 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 02:09:53,689 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 02:09:53,689 - src.embedding_service - INFO -   Device: mps
2025-09-05 02:09:53,689 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 02:09:53,689 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 02:09:53,707 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:09:53,708 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 02:09:53,708 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 02:09:53,708 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 02:09:53,709 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 02:09:53,709 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 02:09:54,923 - src.llm_wrapper - INFO - Model ready in 1.21s
2025-09-05 02:09:54,923 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 02:09:54,923 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 02:09:54,923 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 02:09:54,924 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 02:09:54,924 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 02:09:54,924 - src.rag_pipeline - INFO - Processing validated query: 'How does artificial intelligence work?'
2025-09-05 02:09:54,942 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does artificial intelligence work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.37it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.36it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.33it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.33it/s, batch=1/1, memory=N/A]
2025-09-05 02:09:55,624 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:09:55,630 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:09:56,204 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.262s
2025-09-05 02:09:56,208 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 02:09:56,209 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1211 tokens) in 0.004s
2025-09-05 02:10:39,943 - src.rag_pipeline - INFO - Generated response (287 tokens) in 43.710s
2025-09-05 02:10:41,437 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 02:10:41,614 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 02:10:41,666 - src.experiment_runner - INFO - Completed run 162/510: 46.26s
2025-09-05 02:10:41,667 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 02:10:41,667 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 02:10:41,667 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 02:10:41,667 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 02:10:41,667 - src.embedding_service - INFO -   Device: mps
2025-09-05 02:10:41,667 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 02:10:41,668 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 02:10:41,689 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:10:41,690 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 02:10:41,690 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 02:10:41,690 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 02:10:41,691 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 02:10:41,691 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 02:25:47,714 - src.llm_wrapper - INFO - Model ready in 906.02s
2025-09-05 02:25:47,715 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 02:25:47,715 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 02:25:47,715 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 02:25:47,715 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 02:25:47,715 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 02:25:47,715 - src.rag_pipeline - INFO - Processing validated query: 'Explain deep learning algorithms.'
2025-09-05 02:25:47,724 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain deep learning algorithms....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.40it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.39it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.38it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.38it/s, batch=1/1, memory=N/A]
2025-09-05 02:25:48,939 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:25:48,951 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:25:50,285 - src.rag_pipeline - INFO - Retrieved 5 contexts in 2.561s
2025-09-05 02:25:50,295 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 02:25:50,296 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1198 tokens) in 0.009s
2025-09-05 02:25:55,828 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.526s
2025-09-05 02:25:56,068 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 02:25:56,131 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 02:25:56,146 - src.experiment_runner - INFO - Completed run 163/510: 914.17s
2025-09-05 02:25:56,146 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 02:25:56,146 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 02:25:56,146 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 02:25:56,146 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 02:25:56,146 - src.embedding_service - INFO -   Device: mps
2025-09-05 02:25:56,146 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 02:25:56,146 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 02:25:56,153 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:25:56,154 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 02:25:56,155 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 02:25:56,155 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 02:25:56,155 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 02:25:56,155 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 02:25:56,748 - src.llm_wrapper - INFO - Model ready in 0.59s
2025-09-05 02:25:56,748 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 02:25:56,749 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 02:25:56,749 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 02:25:56,749 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 02:25:56,749 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 02:25:56,749 - src.rag_pipeline - INFO - Processing validated query: 'What are neural networks?'
2025-09-05 02:25:56,760 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are neural networks?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.54it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.46it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.42it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.42it/s, batch=1/1, memory=N/A]
2025-09-05 02:25:57,356 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:25:57,357 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:25:57,952 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.191s
2025-09-05 02:25:57,955 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 02:25:57,955 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1353 tokens) in 0.003s
2025-09-05 02:26:12,981 - src.rag_pipeline - INFO - Generated response (238 tokens) in 15.017s
2025-09-05 02:26:13,205 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 02:26:13,268 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 02:26:13,276 - src.experiment_runner - INFO - Completed run 164/510: 16.84s
2025-09-05 02:26:13,276 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 02:26:13,276 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 02:26:13,277 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 02:26:13,277 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 02:26:13,277 - src.embedding_service - INFO -   Device: mps
2025-09-05 02:26:13,277 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 02:26:13,277 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 02:26:13,282 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:26:13,282 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 02:26:13,282 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 02:26:13,282 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 02:26:13,282 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 02:26:13,282 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 02:26:13,854 - src.llm_wrapper - INFO - Model ready in 0.57s
2025-09-05 02:26:13,854 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 02:26:13,854 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 02:26:13,854 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 02:26:13,854 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 02:26:13,854 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 02:26:13,854 - src.rag_pipeline - INFO - Processing validated query: 'How do large language models work?'
2025-09-05 02:26:13,863 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How do large language models work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.39it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.38it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.29it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.29it/s, batch=1/1, memory=N/A]
2025-09-05 02:26:14,641 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:26:14,643 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:26:15,292 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.429s
2025-09-05 02:26:15,297 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 02:26:15,298 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1337 tokens) in 0.004s
2025-09-05 02:26:20,952 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.652s
2025-09-05 02:26:21,163 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 02:26:21,223 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 02:26:21,242 - src.experiment_runner - INFO - Completed run 165/510: 7.68s
2025-09-05 02:26:21,242 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 02:26:21,242 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 02:26:21,242 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 02:26:21,242 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 02:26:21,242 - src.embedding_service - INFO -   Device: mps
2025-09-05 02:26:21,242 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 02:26:21,242 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 02:26:21,251 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:26:21,251 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 02:26:21,251 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 02:26:21,251 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 02:26:21,251 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 02:26:21,251 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 02:26:21,838 - src.llm_wrapper - INFO - Model ready in 0.59s
2025-09-05 02:26:21,839 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 02:26:21,839 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 02:26:21,839 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 02:26:21,839 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 02:26:21,839 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 02:26:21,839 - src.rag_pipeline - INFO - Processing validated query: 'What is natural language processing?'
2025-09-05 02:26:21,853 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is natural language processing?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.59it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.58it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.55it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.55it/s, batch=1/1, memory=N/A]
2025-09-05 02:26:22,415 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:26:22,416 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:26:23,018 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.164s
2025-09-05 02:26:23,021 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 02:26:23,024 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1749 tokens) in 0.005s
2025-09-05 02:26:30,469 - src.rag_pipeline - INFO - Generated response (55 tokens) in 7.443s
2025-09-05 02:26:30,739 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 02:26:30,819 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 02:26:30,828 - src.experiment_runner - INFO - Completed run 166/510: 9.23s
2025-09-05 02:26:30,828 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 02:26:30,828 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 02:26:30,828 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 02:26:30,828 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 02:26:30,828 - src.embedding_service - INFO -   Device: mps
2025-09-05 02:26:30,828 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 02:26:30,828 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 02:26:30,837 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:26:30,838 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 02:26:30,838 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 02:26:30,838 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 02:26:30,838 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 02:26:30,838 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 02:26:31,441 - src.llm_wrapper - INFO - Model ready in 0.60s
2025-09-05 02:26:31,442 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 02:26:31,442 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 02:26:31,442 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 02:26:31,442 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 02:26:31,442 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 02:26:31,442 - src.rag_pipeline - INFO - Processing validated query: 'Explain computer vision techniques.'
2025-09-05 02:26:31,450 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain computer vision techniques....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.07it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.03it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.00it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.00it/s, batch=1/1, memory=N/A]
2025-09-05 02:26:31,848 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:26:31,859 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:26:32,248 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.798s
2025-09-05 02:26:32,256 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 02:26:32,257 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1297 tokens) in 0.007s
2025-09-05 02:26:51,215 - src.rag_pipeline - INFO - Generated response (318 tokens) in 18.942s
2025-09-05 02:26:51,537 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 02:26:51,626 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 02:26:51,649 - src.experiment_runner - INFO - Completed run 167/510: 20.39s
2025-09-05 02:26:51,649 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 02:26:51,649 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 02:26:51,649 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 02:26:51,650 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 02:26:51,650 - src.embedding_service - INFO -   Device: mps
2025-09-05 02:26:51,650 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 02:26:51,650 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 02:26:51,662 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:26:51,663 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 02:26:51,663 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 02:26:51,663 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 02:26:51,663 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 02:26:51,663 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 02:26:52,425 - src.llm_wrapper - INFO - Model ready in 0.76s
2025-09-05 02:26:52,425 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 02:26:52,425 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 02:26:52,425 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 02:26:52,425 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 02:26:52,425 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 02:26:52,425 - src.rag_pipeline - INFO - Processing validated query: 'What is reinforcement learning?'
2025-09-05 02:26:52,435 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is reinforcement learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.98it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.98it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.96it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.96it/s, batch=1/1, memory=N/A]
2025-09-05 02:26:52,922 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:26:52,923 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:26:53,340 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.904s
2025-09-05 02:26:53,344 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 02:26:53,348 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1236 tokens) in 0.006s
2025-09-05 02:27:06,119 - src.rag_pipeline - INFO - Generated response (180 tokens) in 12.751s
2025-09-05 02:27:06,539 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 02:27:06,630 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 02:27:06,642 - src.experiment_runner - INFO - Completed run 168/510: 14.47s
2025-09-05 02:27:06,642 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 02:27:06,642 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 02:27:06,642 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 02:27:06,643 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 02:27:06,643 - src.embedding_service - INFO -   Device: mps
2025-09-05 02:27:06,643 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 02:27:06,643 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 02:27:06,655 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:27:06,656 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 02:27:06,656 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 02:27:06,656 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 02:27:06,656 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 02:27:06,656 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 02:27:07,578 - src.llm_wrapper - INFO - Model ready in 0.92s
2025-09-05 02:27:07,578 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 02:27:07,578 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 02:27:07,578 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 02:27:07,578 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 02:27:07,578 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 02:27:07,578 - src.rag_pipeline - INFO - Processing validated query: 'How does data preprocessing work?'
2025-09-05 02:27:07,592 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does data preprocessing work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.55it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.53it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.48it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.48it/s, batch=1/1, memory=N/A]
2025-09-05 02:27:08,254 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:27:08,255 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:27:08,710 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.118s
2025-09-05 02:27:08,714 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 02:27:08,715 - src.rag_pipeline - INFO - Built prompt with 5 contexts (909 tokens) in 0.002s
2025-09-05 02:27:19,006 - src.rag_pipeline - INFO - Generated response (139 tokens) in 10.282s
2025-09-05 02:27:19,452 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 02:27:19,541 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 02:27:19,566 - src.experiment_runner - INFO - Completed run 169/510: 12.37s
2025-09-05 02:27:19,566 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 02:27:19,566 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 02:27:19,567 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 02:27:19,567 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 02:27:19,567 - src.embedding_service - INFO -   Device: mps
2025-09-05 02:27:19,567 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 02:27:19,567 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 02:27:19,579 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:27:19,580 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 02:27:19,580 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 02:27:19,580 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 02:27:19,581 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 02:27:19,581 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 02:27:20,602 - src.llm_wrapper - INFO - Model ready in 1.02s
2025-09-05 02:27:20,602 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 02:27:20,602 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 02:27:20,602 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 02:27:20,602 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 02:27:20,602 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 02:27:20,602 - src.rag_pipeline - INFO - Processing validated query: 'What are the challenges in AI development?'
2025-09-05 02:27:20,618 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are the challenges in AI development?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.65it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.64it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.63it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.63it/s, batch=1/1, memory=N/A]
2025-09-05 02:27:21,209 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:27:21,211 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:27:21,647 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.029s
2025-09-05 02:27:21,650 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 02:27:21,651 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1445 tokens) in 0.004s
2025-09-05 02:27:42,809 - src.rag_pipeline - INFO - Generated response (237 tokens) in 21.141s
2025-09-05 02:27:43,504 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 02:27:43,631 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 02:27:43,660 - src.experiment_runner - INFO - Completed run 170/510: 23.24s
2025-09-05 02:27:43,660 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 02:27:43,660 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 02:27:43,661 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 02:27:43,661 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 02:27:43,661 - src.embedding_service - INFO -   Device: mps
2025-09-05 02:27:43,661 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 02:27:43,661 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 02:27:43,679 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:27:43,680 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 02:27:43,680 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 02:27:43,680 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 02:27:43,681 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 02:27:43,682 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 02:27:44,646 - src.llm_wrapper - INFO - Model ready in 0.96s
2025-09-05 02:27:44,647 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 02:27:44,647 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 02:27:44,648 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 02:27:44,648 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 02:27:44,652 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 02:27:44,652 - src.rag_pipeline - INFO - Processing validated query: 'What is machine learning?'
2025-09-05 02:27:44,671 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is machine learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.83it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.83it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.82it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.82it/s, batch=1/1, memory=N/A]
2025-09-05 02:27:45,594 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:27:45,596 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:27:46,082 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.411s
2025-09-05 02:27:46,100 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 02:27:46,102 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1162 tokens) in 0.017s
2025-09-05 02:27:57,186 - src.rag_pipeline - INFO - Generated response (59 tokens) in 11.082s
2025-09-05 02:27:57,822 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 02:27:57,980 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 02:27:58,002 - src.experiment_runner - INFO - Completed run 171/510: 13.53s
2025-09-05 02:27:58,003 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 02:27:58,003 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 02:27:58,003 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 02:27:58,004 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 02:27:58,004 - src.embedding_service - INFO -   Device: mps
2025-09-05 02:27:58,004 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 02:27:58,004 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 02:27:58,021 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:27:58,021 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 02:27:58,021 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 02:27:58,021 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 02:27:58,022 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 02:27:58,022 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 02:27:59,131 - src.llm_wrapper - INFO - Model ready in 1.11s
2025-09-05 02:27:59,131 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 02:27:59,131 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 02:27:59,132 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 02:27:59,132 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 02:27:59,132 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 02:27:59,132 - src.rag_pipeline - INFO - Processing validated query: 'How does artificial intelligence work?'
2025-09-05 02:27:59,145 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does artificial intelligence work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.41it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.40it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.36it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.35it/s, batch=1/1, memory=N/A]
2025-09-05 02:27:59,668 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:27:59,672 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:28:00,137 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.992s
2025-09-05 02:28:00,142 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 02:28:00,142 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1211 tokens) in 0.004s
2025-09-05 02:28:36,069 - src.rag_pipeline - INFO - Generated response (287 tokens) in 35.914s
2025-09-05 02:28:36,781 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 02:28:36,995 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 02:28:37,019 - src.experiment_runner - INFO - Completed run 172/510: 38.07s
2025-09-05 02:28:37,019 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 02:28:37,019 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 02:28:37,019 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 02:28:37,020 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 02:28:37,020 - src.embedding_service - INFO -   Device: mps
2025-09-05 02:28:37,020 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 02:28:37,020 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 02:28:37,038 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:28:37,040 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 02:28:37,040 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 02:28:37,040 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 02:28:37,041 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 02:28:37,041 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 02:28:38,581 - src.llm_wrapper - INFO - Model ready in 1.54s
2025-09-05 02:28:38,581 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 02:28:38,581 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 02:28:38,581 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 02:28:38,581 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 02:28:38,581 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 02:28:38,581 - src.rag_pipeline - INFO - Processing validated query: 'Explain deep learning algorithms.'
2025-09-05 02:28:38,600 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain deep learning algorithms....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.50it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.49it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.47it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.47it/s, batch=1/1, memory=N/A]
2025-09-05 02:28:39,888 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:28:39,902 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:28:40,667 - src.rag_pipeline - INFO - Retrieved 5 contexts in 2.067s
2025-09-05 02:28:40,676 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 02:28:40,680 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1198 tokens) in 0.008s
2025-09-05 02:28:56,426 - src.rag_pipeline - INFO - Generated response (47 tokens) in 15.743s
2025-09-05 02:28:57,300 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 02:28:57,551 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 02:28:57,580 - src.experiment_runner - INFO - Completed run 173/510: 19.41s
2025-09-05 02:28:57,580 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 02:28:57,580 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 02:28:57,581 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 02:28:57,581 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 02:28:57,581 - src.embedding_service - INFO -   Device: mps
2025-09-05 02:28:57,581 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 02:28:57,581 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 02:28:57,600 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:28:57,601 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 02:28:57,601 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 02:28:57,601 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 02:28:57,602 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 02:28:57,602 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 02:28:59,599 - src.llm_wrapper - INFO - Model ready in 2.00s
2025-09-05 02:28:59,600 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 02:28:59,600 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 02:28:59,600 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 02:28:59,600 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 02:28:59,600 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 02:28:59,600 - src.rag_pipeline - INFO - Processing validated query: 'What are neural networks?'
2025-09-05 02:28:59,632 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are neural networks?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.12it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.11it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.10it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.10it/s, batch=1/1, memory=N/A]
2025-09-05 02:29:00,906 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:29:00,910 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:29:01,577 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.945s
2025-09-05 02:29:01,593 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 02:29:01,595 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1353 tokens) in 0.015s
2025-09-05 02:44:55,008 - src.rag_pipeline - INFO - Generated response (238 tokens) in 953.405s
2025-09-05 02:44:55,264 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 02:44:55,321 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 02:44:55,328 - src.experiment_runner - INFO - Completed run 174/510: 957.43s
2025-09-05 02:44:55,328 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 02:44:55,328 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 02:44:55,328 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 02:44:55,328 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 02:44:55,328 - src.embedding_service - INFO -   Device: mps
2025-09-05 02:44:55,328 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 02:44:55,328 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 02:44:55,334 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:44:55,334 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 02:44:55,334 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 02:44:55,334 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 02:44:55,334 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 02:44:55,335 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 02:44:55,920 - src.llm_wrapper - INFO - Model ready in 0.59s
2025-09-05 02:44:55,920 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 02:44:55,920 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 02:44:55,920 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 02:44:55,920 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 02:44:55,920 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 02:44:55,920 - src.rag_pipeline - INFO - Processing validated query: 'How do large language models work?'
2025-09-05 02:44:55,929 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How do large language models work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.41it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.39it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.37it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.37it/s, batch=1/1, memory=N/A]
2025-09-05 02:44:56,714 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:44:56,717 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:44:57,191 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.260s
2025-09-05 02:44:57,200 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 02:44:57,200 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1337 tokens) in 0.007s
2025-09-05 02:45:02,487 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.283s
2025-09-05 02:45:02,676 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 02:45:02,735 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 02:45:02,751 - src.experiment_runner - INFO - Completed run 175/510: 7.16s
2025-09-05 02:45:02,751 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 02:45:02,751 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 02:45:02,751 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 02:45:02,751 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 02:45:02,751 - src.embedding_service - INFO -   Device: mps
2025-09-05 02:45:02,751 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 02:45:02,751 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 02:45:02,756 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:45:02,756 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 02:45:02,756 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 02:45:02,756 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 02:45:02,757 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 02:45:02,757 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 02:45:03,299 - src.llm_wrapper - INFO - Model ready in 0.54s
2025-09-05 02:45:03,299 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 02:45:03,299 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 02:45:03,299 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 02:45:03,299 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 02:45:03,299 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 02:45:03,299 - src.rag_pipeline - INFO - Processing validated query: 'What is natural language processing?'
2025-09-05 02:45:03,306 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is natural language processing?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.68it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.67it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.64it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.64it/s, batch=1/1, memory=N/A]
2025-09-05 02:45:03,722 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:45:03,724 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:45:04,244 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.938s
2025-09-05 02:45:04,253 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 02:45:04,254 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1749 tokens) in 0.008s
2025-09-05 02:45:11,394 - src.rag_pipeline - INFO - Generated response (55 tokens) in 7.136s
2025-09-05 02:45:11,711 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 02:45:11,767 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 02:45:11,772 - src.experiment_runner - INFO - Completed run 176/510: 8.64s
2025-09-05 02:45:11,773 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 02:45:11,773 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 02:45:11,773 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 02:45:11,773 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 02:45:11,773 - src.embedding_service - INFO -   Device: mps
2025-09-05 02:45:11,773 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 02:45:11,773 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 02:45:11,778 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:45:11,779 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 02:45:11,779 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 02:45:11,779 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 02:45:11,779 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 02:45:11,779 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 02:45:12,341 - src.llm_wrapper - INFO - Model ready in 0.56s
2025-09-05 02:45:12,342 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 02:45:12,342 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 02:45:12,342 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 02:45:12,342 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 02:45:12,342 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 02:45:12,342 - src.rag_pipeline - INFO - Processing validated query: 'Explain computer vision techniques.'
2025-09-05 02:45:12,348 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain computer vision techniques....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.25it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.24it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.23it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.23it/s, batch=1/1, memory=N/A]
2025-09-05 02:45:13,174 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:45:13,179 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:45:13,706 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.357s
2025-09-05 02:45:13,716 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 02:45:13,717 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1297 tokens) in 0.008s
2025-09-05 02:45:33,156 - src.rag_pipeline - INFO - Generated response (318 tokens) in 19.421s
2025-09-05 02:45:33,506 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 02:45:33,572 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 02:45:33,590 - src.experiment_runner - INFO - Completed run 177/510: 21.39s
2025-09-05 02:45:33,591 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 02:45:33,591 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 02:45:33,591 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 02:45:33,591 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 02:45:33,591 - src.embedding_service - INFO -   Device: mps
2025-09-05 02:45:33,591 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 02:45:33,591 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 02:45:33,600 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:45:33,600 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 02:45:33,600 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 02:45:33,600 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 02:45:33,601 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 02:45:33,601 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 02:45:34,199 - src.llm_wrapper - INFO - Model ready in 0.60s
2025-09-05 02:45:34,199 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 02:45:34,199 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 02:45:34,199 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 02:45:34,199 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 02:45:34,199 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 02:45:34,199 - src.rag_pipeline - INFO - Processing validated query: 'What is reinforcement learning?'
2025-09-05 02:45:34,207 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is reinforcement learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.21it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.19it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.16it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.16it/s, batch=1/1, memory=N/A]
2025-09-05 02:45:34,800 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:45:34,801 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:45:35,255 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.046s
2025-09-05 02:45:35,261 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 02:45:35,262 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1236 tokens) in 0.006s
2025-09-05 02:45:47,177 - src.rag_pipeline - INFO - Generated response (180 tokens) in 11.910s
2025-09-05 02:45:47,435 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 02:45:47,510 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 02:45:47,528 - src.experiment_runner - INFO - Completed run 178/510: 13.59s
2025-09-05 02:45:47,528 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 02:45:47,528 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 02:45:47,528 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 02:45:47,528 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 02:45:47,528 - src.embedding_service - INFO -   Device: mps
2025-09-05 02:45:47,529 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 02:45:47,529 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 02:45:47,539 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:45:47,539 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 02:45:47,539 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 02:45:47,539 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 02:45:47,540 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 02:45:47,540 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 02:45:48,256 - src.llm_wrapper - INFO - Model ready in 0.72s
2025-09-05 02:45:48,256 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 02:45:48,256 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 02:45:48,256 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 02:45:48,256 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 02:45:48,256 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 02:45:48,256 - src.rag_pipeline - INFO - Processing validated query: 'How does data preprocessing work?'
2025-09-05 02:45:48,265 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does data preprocessing work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.93it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.92it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.89it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.89it/s, batch=1/1, memory=N/A]
2025-09-05 02:45:48,634 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:45:48,635 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:45:49,381 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.116s
2025-09-05 02:45:49,388 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 02:45:49,389 - src.rag_pipeline - INFO - Built prompt with 5 contexts (909 tokens) in 0.007s
2025-09-05 02:45:58,621 - src.rag_pipeline - INFO - Generated response (139 tokens) in 9.227s
2025-09-05 02:45:58,915 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 02:45:59,007 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 02:45:59,019 - src.experiment_runner - INFO - Completed run 179/510: 11.10s
2025-09-05 02:45:59,019 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 02:45:59,019 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 02:45:59,019 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 02:45:59,019 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 02:45:59,019 - src.embedding_service - INFO -   Device: mps
2025-09-05 02:45:59,019 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 02:45:59,019 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 02:45:59,032 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:45:59,032 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 02:45:59,032 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 02:45:59,032 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 02:45:59,033 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 02:45:59,033 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 02:45:59,889 - src.llm_wrapper - INFO - Model ready in 0.86s
2025-09-05 02:45:59,889 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 02:45:59,889 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 02:45:59,889 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 02:45:59,889 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 02:45:59,889 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 02:45:59,889 - src.rag_pipeline - INFO - Processing validated query: 'What are the challenges in AI development?'
2025-09-05 02:45:59,900 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are the challenges in AI development?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.14it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.13it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.11it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.11it/s, batch=1/1, memory=N/A]
2025-09-05 02:46:00,375 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:46:00,376 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:46:00,852 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.952s
2025-09-05 02:46:00,857 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 02:46:00,857 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1445 tokens) in 0.005s
2025-09-05 02:46:17,693 - src.rag_pipeline - INFO - Generated response (237 tokens) in 16.829s
2025-09-05 02:46:18,233 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 02:46:18,323 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 02:46:18,344 - src.experiment_runner - INFO - Completed run 180/510: 18.68s
2025-09-05 02:46:18,344 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 02:46:18,344 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 02:46:18,344 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 02:46:18,344 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 02:46:18,344 - src.embedding_service - INFO -   Device: mps
2025-09-05 02:46:18,344 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 02:46:18,345 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 02:46:18,355 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:46:18,358 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 02:46:18,358 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 02:46:18,358 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 02:46:18,359 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 02:46:18,359 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 02:46:19,283 - src.llm_wrapper - INFO - Model ready in 0.92s
2025-09-05 02:46:19,283 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 02:46:19,283 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 02:46:19,283 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 02:46:19,283 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 02:46:19,283 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 02:46:19,283 - src.rag_pipeline - INFO - Processing validated query: 'What is machine learning?'
2025-09-05 02:46:19,294 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is machine learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.91it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.89it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.86it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.86it/s, batch=1/1, memory=N/A]
2025-09-05 02:46:19,882 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:46:19,884 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:46:20,329 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.035s
2025-09-05 02:46:20,333 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 02:46:20,333 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1162 tokens) in 0.003s
2025-09-05 02:46:28,344 - src.rag_pipeline - INFO - Generated response (59 tokens) in 8.006s
2025-09-05 02:46:28,957 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 02:46:29,121 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 02:46:29,144 - src.experiment_runner - INFO - Completed run 181/510: 10.00s
2025-09-05 02:46:29,145 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 02:46:29,145 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 02:46:29,145 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 02:46:29,145 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 02:46:29,145 - src.embedding_service - INFO -   Device: mps
2025-09-05 02:46:29,145 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 02:46:29,145 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 02:46:29,161 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:46:29,162 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 02:46:29,162 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 02:46:29,162 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 02:46:29,162 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 02:46:29,162 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 02:46:30,107 - src.llm_wrapper - INFO - Model ready in 0.95s
2025-09-05 02:46:30,108 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 02:46:30,108 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 02:46:30,108 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 02:46:30,108 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 02:46:30,108 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 02:46:30,108 - src.rag_pipeline - INFO - Processing validated query: 'How does artificial intelligence work?'
2025-09-05 02:46:30,119 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does artificial intelligence work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.41it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.41it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.38it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.38it/s, batch=1/1, memory=N/A]
2025-09-05 02:46:30,667 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:46:30,669 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:46:31,404 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.284s
2025-09-05 02:46:31,414 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 02:46:31,415 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1211 tokens) in 0.010s
2025-09-05 02:46:56,403 - src.rag_pipeline - INFO - Generated response (287 tokens) in 24.974s
2025-09-05 02:46:57,002 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 02:46:57,150 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 02:46:57,171 - src.experiment_runner - INFO - Completed run 182/510: 27.26s
2025-09-05 02:46:57,173 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 02:46:57,173 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 02:46:57,174 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 02:46:57,174 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 02:46:57,174 - src.embedding_service - INFO -   Device: mps
2025-09-05 02:46:57,174 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 02:46:57,174 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 02:46:57,189 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:46:57,190 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 02:46:57,190 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 02:46:57,190 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 02:46:57,190 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 02:46:57,191 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 02:46:58,530 - src.llm_wrapper - INFO - Model ready in 1.34s
2025-09-05 02:46:58,530 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 02:46:58,530 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 02:46:58,530 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 02:46:58,530 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 02:46:58,530 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 02:46:58,530 - src.rag_pipeline - INFO - Processing validated query: 'Explain deep learning algorithms.'
2025-09-05 02:46:58,548 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain deep learning algorithms....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.86it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.86it/s, batch=1/1, memory=N/A]
2025-09-05 02:46:59,515 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:46:59,519 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:47:00,081 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.533s
2025-09-05 02:47:00,092 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 02:47:00,094 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1198 tokens) in 0.010s
2025-09-05 02:47:12,362 - src.rag_pipeline - INFO - Generated response (47 tokens) in 12.266s
2025-09-05 02:47:13,070 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 02:47:13,341 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 02:47:13,378 - src.experiment_runner - INFO - Completed run 183/510: 15.21s
2025-09-05 02:47:13,379 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 02:47:13,379 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 02:47:13,379 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 02:47:13,380 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 02:47:13,380 - src.embedding_service - INFO -   Device: mps
2025-09-05 02:47:13,380 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 02:47:13,380 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 02:47:13,397 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:47:13,398 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 02:47:13,398 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 02:47:13,398 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 02:47:13,398 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 02:47:13,398 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 02:47:14,623 - src.llm_wrapper - INFO - Model ready in 1.22s
2025-09-05 02:47:14,623 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 02:47:14,623 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 02:47:14,623 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 02:47:14,623 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 02:47:14,623 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 02:47:14,623 - src.rag_pipeline - INFO - Processing validated query: 'What are neural networks?'
2025-09-05 02:47:14,645 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are neural networks?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.49it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.49it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.48it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.48it/s, batch=1/1, memory=N/A]
2025-09-05 02:47:15,587 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:47:15,592 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:47:16,129 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.484s
2025-09-05 02:47:16,146 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 02:47:16,146 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1353 tokens) in 0.012s
2025-09-05 02:47:53,224 - src.rag_pipeline - INFO - Generated response (238 tokens) in 37.066s
2025-09-05 02:47:54,196 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 02:47:54,416 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 02:47:54,455 - src.experiment_runner - INFO - Completed run 184/510: 39.85s
2025-09-05 02:47:54,456 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 02:47:54,456 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 02:47:54,456 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 02:47:54,456 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 02:47:54,456 - src.embedding_service - INFO -   Device: mps
2025-09-05 02:47:54,456 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 02:47:54,456 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 02:47:54,481 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:47:54,482 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 02:47:54,482 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 02:47:54,482 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 02:47:54,482 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 02:47:54,483 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 02:47:56,404 - src.llm_wrapper - INFO - Model ready in 1.92s
2025-09-05 02:47:56,405 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 02:47:56,405 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 02:47:56,405 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 02:47:56,405 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 02:47:56,406 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 02:47:56,406 - src.rag_pipeline - INFO - Processing validated query: 'How do large language models work?'
2025-09-05 02:47:56,436 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How do large language models work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.10it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.10it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.08it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.08it/s, batch=1/1, memory=N/A]
2025-09-05 02:47:58,036 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:47:58,041 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:47:58,803 - src.rag_pipeline - INFO - Retrieved 5 contexts in 2.367s
2025-09-05 02:47:58,815 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 02:47:58,817 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1337 tokens) in 0.011s
2025-09-05 02:48:20,318 - src.rag_pipeline - INFO - Generated response (47 tokens) in 21.498s
2025-09-05 02:48:20,700 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 02:48:20,851 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 02:48:20,871 - src.experiment_runner - INFO - Completed run 185/510: 25.87s
2025-09-05 02:48:20,872 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 02:48:20,872 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 02:48:20,872 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 02:48:20,872 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 02:48:20,872 - src.embedding_service - INFO -   Device: mps
2025-09-05 02:48:20,872 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 02:48:20,872 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 02:48:20,892 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 02:48:20,893 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 02:48:20,893 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 02:48:20,893 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 02:48:20,893 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 02:48:20,894 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 02:48:22,856 - src.llm_wrapper - INFO - Model ready in 1.96s
2025-09-05 02:48:22,856 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 02:48:22,856 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 02:48:22,856 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 02:48:22,856 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 02:48:22,856 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 02:48:22,856 - src.rag_pipeline - INFO - Processing validated query: 'What is natural language processing?'
2025-09-05 02:48:22,912 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is natural language processing?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [16:43<00:00, 1003.21s/it][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [16:43<00:00, 1003.21s/it]
Generating embeddings:   0%|          | 0/1 [16:43<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [16:43<00:00, 1003.22s/it, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [16:43<00:00, 1003.22s/it, batch=1/1, memory=N/A]
2025-09-05 03:05:06,646 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:05:06,649 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:05:07,463 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1004.551s
2025-09-05 03:05:07,469 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:05:07,471 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1749 tokens) in 0.005s
2025-09-05 03:05:15,652 - src.rag_pipeline - INFO - Generated response (55 tokens) in 8.176s
2025-09-05 03:05:15,913 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:05:15,974 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:05:15,985 - src.experiment_runner - INFO - Completed run 186/510: 1014.78s
2025-09-05 03:05:15,985 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:05:15,985 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:05:15,985 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:05:15,985 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:05:15,985 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:05:15,985 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:05:15,985 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:05:15,992 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:05:15,992 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:05:15,993 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:05:15,993 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:05:15,993 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:05:15,993 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:05:16,566 - src.llm_wrapper - INFO - Model ready in 0.57s
2025-09-05 03:05:16,567 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:05:16,567 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:05:16,567 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:05:16,567 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:05:16,567 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:05:16,567 - src.rag_pipeline - INFO - Processing validated query: 'Explain computer vision techniques.'
2025-09-05 03:05:16,573 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain computer vision techniques....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.39it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.37it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.33it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.33it/s, batch=1/1, memory=N/A]
2025-09-05 03:05:17,156 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:05:17,157 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:05:17,661 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.088s
2025-09-05 03:05:17,669 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:05:17,670 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1297 tokens) in 0.007s
2025-09-05 03:05:37,352 - src.rag_pipeline - INFO - Generated response (318 tokens) in 19.675s
2025-09-05 03:05:37,595 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:05:37,665 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:05:37,671 - src.experiment_runner - INFO - Completed run 187/510: 21.37s
2025-09-05 03:05:37,671 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:05:37,671 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:05:37,671 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:05:37,671 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:05:37,671 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:05:37,671 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:05:37,671 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:05:37,676 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:05:37,676 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:05:37,677 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:05:37,677 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:05:37,677 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:05:37,677 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:05:38,291 - src.llm_wrapper - INFO - Model ready in 0.61s
2025-09-05 03:05:38,291 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:05:38,291 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:05:38,291 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:05:38,291 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:05:38,291 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:05:38,291 - src.rag_pipeline - INFO - Processing validated query: 'What is reinforcement learning?'
2025-09-05 03:05:38,299 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is reinforcement learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.28it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.28it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.24it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.24it/s, batch=1/1, memory=N/A]
2025-09-05 03:05:39,130 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:05:39,133 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:05:39,603 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.304s
2025-09-05 03:05:39,618 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:05:39,619 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1236 tokens) in 0.015s
2025-09-05 03:05:51,742 - src.rag_pipeline - INFO - Generated response (180 tokens) in 12.108s
2025-09-05 03:05:52,067 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:05:52,144 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:05:52,152 - src.experiment_runner - INFO - Completed run 188/510: 14.07s
2025-09-05 03:05:52,152 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:05:52,152 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:05:52,152 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:05:52,152 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:05:52,152 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:05:52,152 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:05:52,152 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:05:52,160 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:05:52,160 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:05:52,160 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:05:52,160 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:05:52,161 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:05:52,161 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:05:52,778 - src.llm_wrapper - INFO - Model ready in 0.62s
2025-09-05 03:05:52,778 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:05:52,779 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:05:52,779 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:05:52,779 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:05:52,779 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:05:52,779 - src.rag_pipeline - INFO - Processing validated query: 'How does data preprocessing work?'
2025-09-05 03:05:52,786 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does data preprocessing work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.22it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.21it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.19it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.19it/s, batch=1/1, memory=N/A]
2025-09-05 03:05:53,316 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:05:53,317 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:05:53,788 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.001s
2025-09-05 03:05:53,792 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:05:53,795 - src.rag_pipeline - INFO - Built prompt with 5 contexts (909 tokens) in 0.005s
2025-09-05 03:06:02,730 - src.rag_pipeline - INFO - Generated response (139 tokens) in 8.917s
2025-09-05 03:06:03,035 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:06:03,130 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:06:03,141 - src.experiment_runner - INFO - Completed run 189/510: 10.58s
2025-09-05 03:06:03,141 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:06:03,141 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:06:03,141 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:06:03,141 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:06:03,141 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:06:03,141 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:06:03,141 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:06:03,152 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:06:03,153 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:06:03,154 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:06:03,154 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:06:03,154 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:06:03,154 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:06:03,864 - src.llm_wrapper - INFO - Model ready in 0.71s
2025-09-05 03:06:03,864 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:06:03,864 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:06:03,864 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:06:03,864 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:06:03,864 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:06:03,864 - src.rag_pipeline - INFO - Processing validated query: 'What are the challenges in AI development?'
2025-09-05 03:06:03,874 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are the challenges in AI development?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.91it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.90it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.85it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.85it/s, batch=1/1, memory=N/A]
2025-09-05 03:06:04,249 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:06:04,251 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:06:04,738 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.865s
2025-09-05 03:06:04,744 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:06:04,745 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1445 tokens) in 0.006s
2025-09-05 03:06:20,492 - src.rag_pipeline - INFO - Generated response (237 tokens) in 15.733s
2025-09-05 03:06:20,785 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:06:20,862 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:06:20,886 - src.experiment_runner - INFO - Completed run 190/510: 17.36s
2025-09-05 03:06:20,887 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:06:20,887 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:06:20,887 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:06:20,887 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:06:20,887 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:06:20,887 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:06:20,887 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:06:20,897 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:06:20,898 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:06:20,898 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:06:20,898 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:06:20,898 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:06:20,898 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:06:21,728 - src.llm_wrapper - INFO - Model ready in 0.83s
2025-09-05 03:06:21,728 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:06:21,728 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:06:21,728 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:06:21,728 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:06:21,728 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:06:21,728 - src.rag_pipeline - INFO - Processing validated query: 'What is machine learning?'
2025-09-05 03:06:21,738 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is machine learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.12it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.11it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.03it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.03it/s, batch=1/1, memory=N/A]
2025-09-05 03:06:22,048 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:06:22,053 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:06:22,464 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.726s
2025-09-05 03:06:22,478 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:06:22,478 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1162 tokens) in 0.013s
2025-09-05 03:06:29,360 - src.rag_pipeline - INFO - Generated response (59 tokens) in 6.880s
2025-09-05 03:06:29,645 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:06:29,762 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:06:29,800 - src.experiment_runner - INFO - Completed run 191/510: 8.48s
2025-09-05 03:06:29,800 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:06:29,800 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:06:29,800 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:06:29,800 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:06:29,800 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:06:29,801 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:06:29,801 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:06:29,814 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:06:29,815 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:06:29,815 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:06:29,815 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:06:29,815 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:06:29,816 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:06:30,768 - src.llm_wrapper - INFO - Model ready in 0.95s
2025-09-05 03:06:30,768 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:06:30,768 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:06:30,768 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:06:30,768 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:06:30,768 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:06:30,768 - src.rag_pipeline - INFO - Processing validated query: 'How does artificial intelligence work?'
2025-09-05 03:06:30,780 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does artificial intelligence work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 16.57it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 15.70it/s, batch=1/1, memory=N/A]
2025-09-05 03:06:30,989 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:06:30,991 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:06:31,378 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.598s
2025-09-05 03:06:31,381 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:06:31,382 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1211 tokens) in 0.003s
2025-09-05 03:06:51,266 - src.rag_pipeline - INFO - Generated response (287 tokens) in 19.880s
2025-09-05 03:06:51,609 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:06:51,736 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:06:51,751 - src.experiment_runner - INFO - Completed run 192/510: 21.47s
2025-09-05 03:06:51,751 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:06:51,751 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:06:51,751 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:06:51,751 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:06:51,751 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:06:51,751 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:06:51,751 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:06:51,765 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:06:51,765 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:06:51,765 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:06:51,765 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:06:51,766 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:06:51,766 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:06:52,710 - src.llm_wrapper - INFO - Model ready in 0.94s
2025-09-05 03:06:52,710 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:06:52,710 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:06:52,710 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:06:52,710 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:06:52,710 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:06:52,710 - src.rag_pipeline - INFO - Processing validated query: 'Explain deep learning algorithms.'
2025-09-05 03:06:52,729 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain deep learning algorithms....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.30it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.28it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.22it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.22it/s, batch=1/1, memory=N/A]
2025-09-05 03:06:53,115 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:06:53,117 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:06:53,532 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.803s
2025-09-05 03:06:53,535 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:06:53,535 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1198 tokens) in 0.003s
2025-09-05 03:07:02,240 - src.rag_pipeline - INFO - Generated response (47 tokens) in 8.699s
2025-09-05 03:07:02,707 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:07:02,839 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:07:02,850 - src.experiment_runner - INFO - Completed run 193/510: 10.49s
2025-09-05 03:07:02,851 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:07:02,851 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:07:02,851 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:07:02,851 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:07:02,851 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:07:02,851 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:07:02,851 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:07:02,867 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:07:02,868 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:07:02,868 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:07:02,868 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:07:02,868 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:07:02,868 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:07:03,870 - src.llm_wrapper - INFO - Model ready in 1.00s
2025-09-05 03:07:03,870 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:07:03,870 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:07:03,870 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:07:03,870 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:07:03,870 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:07:03,870 - src.rag_pipeline - INFO - Processing validated query: 'What are neural networks?'
2025-09-05 03:07:03,884 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are neural networks?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  9.30it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  9.25it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  9.01it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  8.99it/s, batch=1/1, memory=N/A]
2025-09-05 03:07:04,165 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:07:04,167 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:07:04,542 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.658s
2025-09-05 03:07:04,543 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:07:04,544 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1353 tokens) in 0.001s
2025-09-05 03:07:30,784 - src.rag_pipeline - INFO - Generated response (238 tokens) in 26.233s
2025-09-05 03:07:31,239 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:07:31,435 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:07:31,457 - src.experiment_runner - INFO - Completed run 194/510: 27.93s
2025-09-05 03:07:31,457 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:07:31,457 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:07:31,457 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:07:31,458 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:07:31,458 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:07:31,458 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:07:31,458 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:07:31,475 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:07:31,476 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:07:31,476 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:07:31,476 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:07:31,476 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:07:31,477 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:07:32,633 - src.llm_wrapper - INFO - Model ready in 1.16s
2025-09-05 03:07:32,633 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:07:32,633 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:07:32,633 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:07:32,633 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:07:32,633 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:07:32,634 - src.rag_pipeline - INFO - Processing validated query: 'How do large language models work?'
2025-09-05 03:07:32,649 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How do large language models work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.59it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.58it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.52it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.51it/s, batch=1/1, memory=N/A]
2025-09-05 03:07:33,226 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:07:33,228 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:07:33,703 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.054s
2025-09-05 03:07:33,707 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:07:33,708 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1337 tokens) in 0.004s
2025-09-05 03:07:47,744 - src.rag_pipeline - INFO - Generated response (47 tokens) in 14.029s
2025-09-05 03:07:48,205 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:07:48,394 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:07:48,407 - src.experiment_runner - INFO - Completed run 195/510: 16.29s
2025-09-05 03:07:48,408 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:07:48,408 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:07:48,408 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:07:48,408 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:07:48,408 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:07:48,408 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:07:48,408 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:07:48,425 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:07:48,426 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:07:48,426 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:07:48,426 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:07:48,427 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:07:48,427 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:07:49,609 - src.llm_wrapper - INFO - Model ready in 1.18s
2025-09-05 03:07:49,609 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:07:49,609 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:07:49,609 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:07:49,609 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:07:49,609 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:07:49,609 - src.rag_pipeline - INFO - Processing validated query: 'What is natural language processing?'
2025-09-05 03:07:49,625 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is natural language processing?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.88it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.87it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.78it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.78it/s, batch=1/1, memory=N/A]
2025-09-05 03:07:50,303 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:07:50,305 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:07:50,800 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.175s
2025-09-05 03:07:50,804 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:07:50,805 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1749 tokens) in 0.004s
2025-09-05 03:08:10,428 - src.rag_pipeline - INFO - Generated response (55 tokens) in 19.620s
2025-09-05 03:08:10,885 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:08:11,091 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:08:11,104 - src.experiment_runner - INFO - Completed run 196/510: 22.02s
2025-09-05 03:08:11,104 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:08:11,104 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:08:11,104 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:08:11,105 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:08:11,105 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:08:11,105 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:08:11,105 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:08:11,120 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:08:11,121 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:08:11,121 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:08:11,121 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:08:11,122 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:08:11,122 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:08:12,352 - src.llm_wrapper - INFO - Model ready in 1.23s
2025-09-05 03:08:12,352 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:08:12,352 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:08:12,352 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:08:12,352 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:08:12,352 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:08:12,352 - src.rag_pipeline - INFO - Processing validated query: 'Explain computer vision techniques.'
2025-09-05 03:08:12,368 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain computer vision techniques....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.06it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.05it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.98it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.98it/s, batch=1/1, memory=N/A]
2025-09-05 03:08:13,156 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:08:13,159 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:08:13,633 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.264s
2025-09-05 03:08:13,640 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:08:13,641 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1297 tokens) in 0.007s
2025-09-05 03:09:00,544 - src.rag_pipeline - INFO - Generated response (318 tokens) in 46.891s
2025-09-05 03:09:01,122 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:09:01,435 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:09:01,453 - src.experiment_runner - INFO - Completed run 197/510: 49.44s
2025-09-05 03:09:01,453 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:09:01,453 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:09:01,454 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:09:01,455 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:09:01,455 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:09:01,455 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:09:01,455 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:24:30,526 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:24:30,527 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:24:30,527 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:24:30,527 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:24:30,527 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:24:30,528 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:24:31,760 - src.llm_wrapper - INFO - Model ready in 1.23s
2025-09-05 03:24:31,760 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:24:31,760 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:24:31,760 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:24:31,760 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:24:31,760 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:24:31,760 - src.rag_pipeline - INFO - Processing validated query: 'What is reinforcement learning?'
2025-09-05 03:24:31,780 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is reinforcement learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.59it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.59it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.58it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.58it/s, batch=1/1, memory=N/A]
2025-09-05 03:24:32,846 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:24:32,850 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:24:33,405 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.625s
2025-09-05 03:24:33,420 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:24:33,422 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1236 tokens) in 0.014s
2025-09-05 03:24:45,216 - src.rag_pipeline - INFO - Generated response (180 tokens) in 11.793s
2025-09-05 03:24:45,398 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:24:45,455 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:24:45,468 - src.experiment_runner - INFO - Completed run 198/510: 943.76s
2025-09-05 03:24:45,468 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:24:45,468 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:24:45,468 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:24:45,469 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:24:45,469 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:24:45,469 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:24:45,469 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:24:45,473 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:24:45,474 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:24:45,474 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:24:45,474 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:24:45,474 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:24:45,474 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:24:45,986 - src.llm_wrapper - INFO - Model ready in 0.51s
2025-09-05 03:24:45,986 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:24:45,986 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:24:45,986 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:24:45,986 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:24:45,986 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:24:45,986 - src.rag_pipeline - INFO - Processing validated query: 'How does data preprocessing work?'
2025-09-05 03:24:45,996 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does data preprocessing work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.19it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.19it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.16it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.16it/s, batch=1/1, memory=N/A]
2025-09-05 03:24:46,448 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:24:46,449 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:24:46,910 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.914s
2025-09-05 03:24:46,917 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:24:46,917 - src.rag_pipeline - INFO - Built prompt with 5 contexts (909 tokens) in 0.007s
2025-09-05 03:24:56,174 - src.rag_pipeline - INFO - Generated response (139 tokens) in 9.252s
2025-09-05 03:24:56,409 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:24:56,474 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:24:56,481 - src.experiment_runner - INFO - Completed run 199/510: 10.71s
2025-09-05 03:24:56,481 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:24:56,481 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:24:56,481 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:24:56,482 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:24:56,482 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:24:56,482 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:24:56,482 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:24:56,487 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:24:56,487 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:24:56,487 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:24:56,487 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:24:56,487 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:24:56,488 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:24:57,021 - src.llm_wrapper - INFO - Model ready in 0.53s
2025-09-05 03:24:57,021 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:24:57,021 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:24:57,021 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:24:57,021 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:24:57,021 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:24:57,021 - src.rag_pipeline - INFO - Processing validated query: 'What are the challenges in AI development?'
2025-09-05 03:24:57,029 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are the challenges in AI development?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.31it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.30it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.28it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.28it/s, batch=1/1, memory=N/A]
2025-09-05 03:24:57,497 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:24:57,498 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:24:57,888 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.858s
2025-09-05 03:24:57,890 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:24:57,890 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1445 tokens) in 0.002s
2025-09-05 03:25:13,276 - src.rag_pipeline - INFO - Generated response (237 tokens) in 15.384s
2025-09-05 03:25:13,535 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:25:13,608 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:25:13,629 - src.experiment_runner - INFO - Completed run 200/510: 16.80s
2025-09-05 03:25:13,629 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:25:13,629 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:25:13,630 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:25:13,630 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:25:13,630 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:25:13,630 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:25:13,630 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:25:13,636 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:25:13,636 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:25:13,636 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:25:13,636 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:25:13,636 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:25:13,636 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:25:14,221 - src.llm_wrapper - INFO - Model ready in 0.58s
2025-09-05 03:25:14,221 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:25:14,221 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:25:14,221 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:25:14,221 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:25:14,221 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:25:14,221 - src.rag_pipeline - INFO - Processing validated query: 'What is machine learning?'
2025-09-05 03:25:14,230 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is machine learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.28it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.27it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.22it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.22it/s, batch=1/1, memory=N/A]
2025-09-05 03:25:14,649 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:25:14,657 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:25:15,463 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.233s
2025-09-05 03:25:15,465 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:25:15,465 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1162 tokens) in 0.002s
2025-09-05 03:25:21,368 - src.rag_pipeline - INFO - Generated response (59 tokens) in 5.902s
2025-09-05 03:25:21,625 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:25:21,709 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:25:21,728 - src.experiment_runner - INFO - Completed run 201/510: 7.74s
2025-09-05 03:25:21,728 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:25:21,728 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:25:21,729 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:25:21,729 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:25:21,729 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:25:21,729 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:25:21,729 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:25:21,739 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:25:21,739 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:25:21,739 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:25:21,739 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:25:21,740 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:25:21,740 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:25:22,411 - src.llm_wrapper - INFO - Model ready in 0.67s
2025-09-05 03:25:22,411 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:25:22,411 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:25:22,411 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:25:22,411 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:25:22,411 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:25:22,411 - src.rag_pipeline - INFO - Processing validated query: 'How does artificial intelligence work?'
2025-09-05 03:25:22,423 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does artificial intelligence work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.34it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.96it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.83it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.82it/s, batch=1/1, memory=N/A]
2025-09-05 03:25:22,718 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:25:22,719 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:25:23,559 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.135s
2025-09-05 03:25:23,561 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:25:23,562 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1211 tokens) in 0.002s
2025-09-05 03:25:40,742 - src.rag_pipeline - INFO - Generated response (287 tokens) in 17.177s
2025-09-05 03:25:41,045 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:25:41,135 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:25:41,147 - src.experiment_runner - INFO - Completed run 202/510: 19.01s
2025-09-05 03:25:41,147 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:25:41,147 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:25:41,147 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:25:41,147 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:25:41,147 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:25:41,147 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:25:41,147 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:25:41,158 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:25:41,159 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:25:41,159 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:25:41,159 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:25:41,159 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:25:41,159 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:25:42,012 - src.llm_wrapper - INFO - Model ready in 0.85s
2025-09-05 03:25:42,012 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:25:42,012 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:25:42,012 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:25:42,012 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:25:42,012 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:25:42,012 - src.rag_pipeline - INFO - Processing validated query: 'Explain deep learning algorithms.'
2025-09-05 03:25:42,023 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain deep learning algorithms....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.07it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.05it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.02it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.02it/s, batch=1/1, memory=N/A]
2025-09-05 03:25:42,501 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:25:42,502 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:25:42,902 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.878s
2025-09-05 03:25:42,904 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:25:42,904 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1198 tokens) in 0.002s
2025-09-05 03:25:49,130 - src.rag_pipeline - INFO - Generated response (47 tokens) in 6.219s
2025-09-05 03:25:49,505 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:25:49,594 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:25:49,617 - src.experiment_runner - INFO - Completed run 203/510: 7.98s
2025-09-05 03:25:49,618 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:25:49,618 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:25:49,618 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:25:49,618 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:25:49,618 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:25:49,618 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:25:49,618 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:25:49,630 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:25:49,631 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:25:49,631 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:25:49,631 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:25:49,631 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:25:49,632 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:25:50,559 - src.llm_wrapper - INFO - Model ready in 0.93s
2025-09-05 03:25:50,560 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:25:50,560 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:25:50,560 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:25:50,560 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:25:50,560 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:25:50,560 - src.rag_pipeline - INFO - Processing validated query: 'What are neural networks?'
2025-09-05 03:25:50,576 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are neural networks?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.40it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.36it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.30it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.30it/s, batch=1/1, memory=N/A]
2025-09-05 03:25:50,971 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:25:50,974 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:25:51,422 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.846s
2025-09-05 03:25:51,425 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:25:51,426 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1353 tokens) in 0.003s
2025-09-05 03:26:08,489 - src.rag_pipeline - INFO - Generated response (238 tokens) in 17.054s
2025-09-05 03:26:08,925 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:26:09,015 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:26:09,032 - src.experiment_runner - INFO - Completed run 204/510: 18.87s
2025-09-05 03:26:09,033 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:26:09,033 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:26:09,033 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:26:09,034 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:26:09,034 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:26:09,034 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:26:09,034 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:26:09,068 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:26:09,069 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:26:09,069 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:26:09,069 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:26:09,070 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:26:09,070 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:26:10,114 - src.llm_wrapper - INFO - Model ready in 1.04s
2025-09-05 03:26:10,114 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:26:10,114 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:26:10,114 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:26:10,114 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:26:10,114 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:26:10,114 - src.rag_pipeline - INFO - Processing validated query: 'How do large language models work?'
2025-09-05 03:26:10,129 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How do large language models work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.15it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.12it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.09it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.09it/s, batch=1/1, memory=N/A]
2025-09-05 03:26:10,659 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:26:10,660 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:26:11,065 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.936s
2025-09-05 03:26:11,067 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:26:11,068 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1337 tokens) in 0.003s
2025-09-05 03:26:19,664 - src.rag_pipeline - INFO - Generated response (47 tokens) in 8.594s
2025-09-05 03:26:20,235 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:26:20,333 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:26:20,345 - src.experiment_runner - INFO - Completed run 205/510: 10.63s
2025-09-05 03:26:20,345 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:26:20,345 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:26:20,345 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:26:20,346 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:26:20,346 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:26:20,346 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:26:20,346 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:26:20,363 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:26:20,364 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:26:20,364 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:26:20,364 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:26:20,364 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:26:20,365 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:26:21,389 - src.llm_wrapper - INFO - Model ready in 1.02s
2025-09-05 03:26:21,389 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:26:21,389 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:26:21,389 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:26:21,390 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:26:21,390 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:26:21,390 - src.rag_pipeline - INFO - Processing validated query: 'What is natural language processing?'
2025-09-05 03:26:21,404 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is natural language processing?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.20it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.18it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.99it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.98it/s, batch=1/1, memory=N/A]
2025-09-05 03:26:21,882 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:26:21,884 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:26:22,334 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.930s
2025-09-05 03:26:22,337 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:26:22,338 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1749 tokens) in 0.003s
2025-09-05 03:26:35,226 - src.rag_pipeline - INFO - Generated response (55 tokens) in 12.882s
2025-09-05 03:26:35,755 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:26:35,904 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:26:35,925 - src.experiment_runner - INFO - Completed run 206/510: 14.88s
2025-09-05 03:26:35,925 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:26:35,925 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:26:35,926 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:26:35,926 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:26:35,926 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:26:35,926 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:26:35,926 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:26:35,948 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:26:35,949 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:26:35,949 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:26:35,949 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:26:35,949 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:26:35,949 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:26:37,098 - src.llm_wrapper - INFO - Model ready in 1.15s
2025-09-05 03:26:37,098 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:26:37,098 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:26:37,098 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:26:37,098 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:26:37,099 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:26:37,099 - src.rag_pipeline - INFO - Processing validated query: 'Explain computer vision techniques.'
2025-09-05 03:26:37,113 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain computer vision techniques....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.76it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.70it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.62it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.61it/s, batch=1/1, memory=N/A]
2025-09-05 03:26:37,615 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:26:37,618 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:26:38,117 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.004s
2025-09-05 03:26:38,121 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:26:38,122 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1297 tokens) in 0.004s
2025-09-05 03:27:14,753 - src.rag_pipeline - INFO - Generated response (318 tokens) in 36.621s
2025-09-05 03:27:15,448 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:27:15,691 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:27:15,710 - src.experiment_runner - INFO - Completed run 207/510: 38.83s
2025-09-05 03:27:15,710 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:27:15,711 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:27:15,711 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:27:15,711 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:27:15,711 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:27:15,711 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:27:15,711 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:27:15,727 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:27:15,728 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:27:15,728 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:27:15,728 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:27:15,729 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:27:15,729 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:27:16,944 - src.llm_wrapper - INFO - Model ready in 1.22s
2025-09-05 03:27:16,948 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:27:16,949 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:27:16,949 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:27:16,949 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:27:16,949 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:27:16,949 - src.rag_pipeline - INFO - Processing validated query: 'What is reinforcement learning?'
2025-09-05 03:27:16,968 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is reinforcement learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.77it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.76it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.73it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.73it/s, batch=1/1, memory=N/A]
2025-09-05 03:27:17,773 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:27:17,774 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:27:18,296 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.328s
2025-09-05 03:27:18,301 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:27:18,302 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1236 tokens) in 0.004s
2025-09-05 03:27:53,556 - src.rag_pipeline - INFO - Generated response (180 tokens) in 35.245s
2025-09-05 03:27:54,246 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:27:54,470 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:27:54,495 - src.experiment_runner - INFO - Completed run 208/510: 37.85s
2025-09-05 03:27:54,495 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:27:54,496 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:27:54,496 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:27:54,496 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:27:54,496 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:27:54,496 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:27:54,496 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:27:54,533 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:27:54,535 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:27:54,535 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:27:54,535 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:27:54,536 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:27:54,537 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:27:56,506 - src.llm_wrapper - INFO - Model ready in 1.97s
2025-09-05 03:27:56,507 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:27:56,507 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:27:56,507 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:27:56,507 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:27:56,507 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:27:56,507 - src.rag_pipeline - INFO - Processing validated query: 'How does data preprocessing work?'
2025-09-05 03:27:56,546 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does data preprocessing work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.28it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.28it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.27it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.27it/s, batch=1/1, memory=N/A]
2025-09-05 03:27:57,645 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:27:57,653 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:27:58,347 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.801s
2025-09-05 03:27:58,356 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:27:58,358 - src.rag_pipeline - INFO - Built prompt with 5 contexts (909 tokens) in 0.009s
2025-09-05 03:45:30,074 - src.rag_pipeline - INFO - Generated response (139 tokens) in 1051.714s
2025-09-05 03:45:30,404 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:45:30,496 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:45:30,521 - src.experiment_runner - INFO - Completed run 209/510: 1055.58s
2025-09-05 03:45:30,521 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:45:30,521 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:45:30,521 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:45:30,521 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:45:30,521 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:45:30,521 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:45:30,521 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:45:30,532 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:45:30,533 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:45:30,533 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:45:30,533 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:45:30,534 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:45:30,534 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:45:32,022 - src.llm_wrapper - INFO - Model ready in 1.49s
2025-09-05 03:45:32,023 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:45:32,023 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:45:32,023 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:45:32,023 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:45:32,023 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:45:32,023 - src.rag_pipeline - INFO - Processing validated query: 'What are the challenges in AI development?'
2025-09-05 03:45:32,048 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are the challenges in AI development?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.25it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.25it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.24it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.24it/s, batch=1/1, memory=N/A]
2025-09-05 03:45:33,239 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:45:33,245 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:45:33,736 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.688s
2025-09-05 03:45:33,746 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:45:33,748 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1445 tokens) in 0.009s
2025-09-05 03:45:49,460 - src.rag_pipeline - INFO - Generated response (237 tokens) in 15.705s
2025-09-05 03:45:49,754 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:45:49,813 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:45:49,832 - src.experiment_runner - INFO - Completed run 210/510: 18.94s
2025-09-05 03:45:49,832 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:45:49,832 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:45:49,832 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:45:49,832 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:45:49,832 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:45:49,832 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:45:49,833 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:45:49,838 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:45:49,838 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:45:49,839 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:45:49,839 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:45:49,839 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:45:49,839 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:45:50,470 - src.llm_wrapper - INFO - Model ready in 0.63s
2025-09-05 03:45:50,471 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:45:50,471 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:45:50,471 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:45:50,471 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:45:50,471 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:45:50,471 - src.rag_pipeline - INFO - Processing validated query: 'What is machine learning?'
2025-09-05 03:45:50,482 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is machine learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.85it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.85it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.83it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.83it/s, batch=1/1, memory=N/A]
2025-09-05 03:45:51,442 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:45:51,447 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:45:52,069 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.587s
2025-09-05 03:45:52,077 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:45:52,078 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1162 tokens) in 0.006s
2025-09-05 03:45:57,889 - src.rag_pipeline - INFO - Generated response (59 tokens) in 5.805s
2025-09-05 03:45:58,147 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:45:58,212 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:45:58,226 - src.experiment_runner - INFO - Completed run 211/510: 8.06s
2025-09-05 03:45:58,226 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:45:58,226 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:45:58,226 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:45:58,226 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:45:58,226 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:45:58,227 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:45:58,227 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:45:58,234 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:45:58,235 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:45:58,235 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:45:58,235 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:45:58,235 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:45:58,235 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:45:58,778 - src.llm_wrapper - INFO - Model ready in 0.54s
2025-09-05 03:45:58,778 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:45:58,778 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:45:58,778 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:45:58,778 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:45:58,778 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:45:58,778 - src.rag_pipeline - INFO - Processing validated query: 'How does artificial intelligence work?'
2025-09-05 03:45:58,786 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does artificial intelligence work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.76it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.75it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.74it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.74it/s, batch=1/1, memory=N/A]
2025-09-05 03:45:59,467 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:45:59,471 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:46:00,331 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.545s
2025-09-05 03:46:00,334 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:46:00,335 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1211 tokens) in 0.002s
2025-09-05 03:46:17,854 - src.rag_pipeline - INFO - Generated response (287 tokens) in 17.507s
2025-09-05 03:46:18,158 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:46:18,231 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:46:18,256 - src.experiment_runner - INFO - Completed run 212/510: 19.63s
2025-09-05 03:46:18,256 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:46:18,256 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:46:18,256 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:46:18,256 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:46:18,256 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:46:18,256 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:46:18,257 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:46:18,269 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:46:18,270 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:46:18,271 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:46:18,271 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:46:18,272 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:46:18,272 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:46:18,864 - src.llm_wrapper - INFO - Model ready in 0.59s
2025-09-05 03:46:18,864 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:46:18,864 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:46:18,864 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:46:18,864 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:46:18,864 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:46:18,864 - src.rag_pipeline - INFO - Processing validated query: 'Explain deep learning algorithms.'
2025-09-05 03:46:18,873 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain deep learning algorithms....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.18it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.16it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.11it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.11it/s, batch=1/1, memory=N/A]
2025-09-05 03:46:19,432 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:46:19,435 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:46:20,188 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.315s
2025-09-05 03:46:20,191 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:46:20,191 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1198 tokens) in 0.002s
2025-09-05 03:46:25,947 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.755s
2025-09-05 03:46:26,225 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:46:26,308 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:46:26,332 - src.experiment_runner - INFO - Completed run 213/510: 7.70s
2025-09-05 03:46:26,332 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:46:26,332 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:46:26,332 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:46:26,332 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:46:26,332 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:46:26,332 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:46:26,332 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:46:26,344 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:46:26,344 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:46:26,344 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:46:26,345 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:46:26,345 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:46:26,345 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:46:27,109 - src.llm_wrapper - INFO - Model ready in 0.76s
2025-09-05 03:46:27,109 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:46:27,109 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:46:27,109 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:46:27,109 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:46:27,109 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:46:27,110 - src.rag_pipeline - INFO - Processing validated query: 'What are neural networks?'
2025-09-05 03:46:27,120 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are neural networks?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.66it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.65it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.60it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.59it/s, batch=1/1, memory=N/A]
2025-09-05 03:46:27,505 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:46:27,507 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:46:27,954 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.834s
2025-09-05 03:46:27,958 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:46:27,958 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1353 tokens) in 0.003s
2025-09-05 03:46:43,439 - src.rag_pipeline - INFO - Generated response (238 tokens) in 15.473s
2025-09-05 03:46:43,769 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:46:43,857 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:46:43,892 - src.experiment_runner - INFO - Completed run 214/510: 17.11s
2025-09-05 03:46:43,892 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:46:43,892 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:46:43,893 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:46:43,893 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:46:43,893 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:46:43,893 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:46:43,893 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:46:43,903 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:46:43,904 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:46:43,904 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:46:43,904 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:46:43,904 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:46:43,905 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:46:44,811 - src.llm_wrapper - INFO - Model ready in 0.91s
2025-09-05 03:46:44,812 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:46:44,812 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:46:44,812 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:46:44,812 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:46:44,812 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:46:44,812 - src.rag_pipeline - INFO - Processing validated query: 'How do large language models work?'
2025-09-05 03:46:44,833 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How do large language models work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.53it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.52it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.50it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.49it/s, batch=1/1, memory=N/A]
2025-09-05 03:46:45,262 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:46:45,263 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:46:45,725 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.892s
2025-09-05 03:46:45,727 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:46:45,728 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1337 tokens) in 0.002s
2025-09-05 03:46:52,592 - src.rag_pipeline - INFO - Generated response (47 tokens) in 6.861s
2025-09-05 03:46:53,126 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:46:53,222 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:46:53,248 - src.experiment_runner - INFO - Completed run 215/510: 8.70s
2025-09-05 03:46:53,248 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:46:53,248 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:46:53,248 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:46:53,248 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:46:53,249 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:46:53,249 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:46:53,249 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:46:53,265 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:46:53,265 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:46:53,265 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:46:53,265 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:46:53,266 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:46:53,266 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:46:54,221 - src.llm_wrapper - INFO - Model ready in 0.96s
2025-09-05 03:46:54,221 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:46:54,221 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:46:54,221 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:46:54,221 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:46:54,221 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:46:54,221 - src.rag_pipeline - INFO - Processing validated query: 'What is natural language processing?'
2025-09-05 03:46:54,229 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is natural language processing?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.55it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.53it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.49it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.49it/s, batch=1/1, memory=N/A]
2025-09-05 03:46:54,575 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:46:54,578 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:46:54,994 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.766s
2025-09-05 03:46:54,996 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:46:54,997 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1749 tokens) in 0.002s
2025-09-05 03:47:02,015 - src.rag_pipeline - INFO - Generated response (55 tokens) in 7.016s
2025-09-05 03:47:02,180 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:47:02,236 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:47:02,267 - src.experiment_runner - INFO - Completed run 216/510: 8.77s
2025-09-05 03:47:02,267 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:47:02,267 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:47:02,267 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:47:02,267 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:47:02,267 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:47:02,267 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:47:02,267 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:47:02,274 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:47:02,275 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:47:02,275 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:47:02,275 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:47:02,275 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:47:02,275 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:47:02,816 - src.llm_wrapper - INFO - Model ready in 0.54s
2025-09-05 03:47:02,816 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:47:02,816 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:47:02,816 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:47:02,816 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:47:02,816 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:47:02,816 - src.rag_pipeline - INFO - Processing validated query: 'Explain computer vision techniques.'
2025-09-05 03:47:02,823 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain computer vision techniques....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.05it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.03it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.00it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.00it/s, batch=1/1, memory=N/A]
2025-09-05 03:47:03,203 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:47:03,206 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:47:03,626 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.802s
2025-09-05 03:47:03,627 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:47:03,628 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1297 tokens) in 0.002s
2025-09-05 03:47:22,637 - src.rag_pipeline - INFO - Generated response (318 tokens) in 19.003s
2025-09-05 03:47:22,849 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:47:22,906 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:47:22,922 - src.experiment_runner - INFO - Completed run 217/510: 20.37s
2025-09-05 03:47:22,922 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:47:22,922 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:47:22,922 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:47:22,922 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:47:22,922 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:47:22,922 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:47:22,922 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:47:22,928 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:47:22,929 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:47:22,929 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:47:22,929 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:47:22,929 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:47:22,929 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:47:23,439 - src.llm_wrapper - INFO - Model ready in 0.51s
2025-09-05 03:47:23,439 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:47:23,439 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:47:23,439 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:47:23,439 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:47:23,439 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:47:23,439 - src.rag_pipeline - INFO - Processing validated query: 'What is reinforcement learning?'
2025-09-05 03:47:23,447 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is reinforcement learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.07it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.06it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.99it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.98it/s, batch=1/1, memory=N/A]
2025-09-05 03:47:24,113 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:47:24,116 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:47:25,051 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.604s
2025-09-05 03:47:25,054 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:47:25,054 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1236 tokens) in 0.002s
2025-09-05 03:47:36,581 - src.rag_pipeline - INFO - Generated response (180 tokens) in 11.526s
2025-09-05 03:47:36,703 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:47:36,760 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:47:36,772 - src.experiment_runner - INFO - Completed run 218/510: 13.66s
2025-09-05 03:47:36,772 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:47:36,772 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:47:36,772 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:47:36,772 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:47:36,772 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:47:36,772 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:47:36,772 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:47:36,780 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:47:36,780 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:47:36,780 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:47:36,780 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:47:36,780 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:47:36,781 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:47:37,291 - src.llm_wrapper - INFO - Model ready in 0.51s
2025-09-05 03:47:37,291 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:47:37,291 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:47:37,291 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:47:37,291 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:47:37,291 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:47:37,291 - src.rag_pipeline - INFO - Processing validated query: 'How does data preprocessing work?'
2025-09-05 03:47:37,299 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does data preprocessing work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.25it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.24it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.20it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.20it/s, batch=1/1, memory=N/A]
2025-09-05 03:47:37,758 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:47:37,761 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:47:38,343 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.044s
2025-09-05 03:47:38,345 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:47:38,345 - src.rag_pipeline - INFO - Built prompt with 5 contexts (909 tokens) in 0.002s
2025-09-05 03:47:47,074 - src.rag_pipeline - INFO - Generated response (139 tokens) in 8.728s
2025-09-05 03:47:47,194 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:47:47,242 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:47:47,249 - src.experiment_runner - INFO - Completed run 219/510: 10.30s
2025-09-05 03:47:47,249 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:47:47,249 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:47:47,249 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:47:47,250 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:47:47,250 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:47:47,250 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:47:47,250 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:47:47,255 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:47:47,255 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:47:47,255 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:47:47,255 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:47:47,255 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:47:47,256 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:47:47,833 - src.llm_wrapper - INFO - Model ready in 0.58s
2025-09-05 03:47:47,833 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:47:47,833 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:47:47,833 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:47:47,833 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:47:47,833 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:47:47,833 - src.rag_pipeline - INFO - Processing validated query: 'What are the challenges in AI development?'
2025-09-05 03:47:47,844 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are the challenges in AI development?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.02it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.97it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.92it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.91it/s, batch=1/1, memory=N/A]
2025-09-05 03:47:48,187 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:47:48,189 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:47:48,817 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.972s
2025-09-05 03:47:48,822 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:47:48,823 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1445 tokens) in 0.004s
2025-09-05 03:48:03,842 - src.rag_pipeline - INFO - Generated response (237 tokens) in 15.018s
2025-09-05 03:48:03,983 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:48:04,034 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:48:04,040 - src.experiment_runner - INFO - Completed run 220/510: 16.59s
2025-09-05 03:48:04,040 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:48:04,040 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:48:04,040 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:48:04,040 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:48:04,040 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:48:04,040 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:48:04,040 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:48:04,046 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:48:04,046 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:48:04,046 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:48:04,046 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:48:04,046 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:48:04,047 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:48:04,554 - src.llm_wrapper - INFO - Model ready in 0.51s
2025-09-05 03:48:04,554 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:48:04,554 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:48:04,554 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:48:04,554 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:48:04,554 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:48:04,554 - src.rag_pipeline - INFO - Processing validated query: 'What is machine learning?'
2025-09-05 03:48:04,564 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is machine learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.95it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.94it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.80it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.80it/s, batch=1/1, memory=N/A]
2025-09-05 03:48:04,900 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:48:04,901 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:48:05,479 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.915s
2025-09-05 03:48:05,480 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:48:05,481 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1162 tokens) in 0.002s
2025-09-05 03:48:11,132 - src.rag_pipeline - INFO - Generated response (59 tokens) in 5.650s
2025-09-05 03:48:11,281 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:48:11,331 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:48:11,339 - src.experiment_runner - INFO - Completed run 221/510: 7.09s
2025-09-05 03:48:11,339 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:48:11,339 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:48:11,339 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:48:11,339 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:48:11,339 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:48:11,339 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:48:11,339 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:48:11,344 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:48:11,344 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:48:11,344 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:48:11,344 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:48:11,345 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:48:11,345 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:48:11,876 - src.llm_wrapper - INFO - Model ready in 0.53s
2025-09-05 03:48:11,876 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:48:11,876 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:48:11,876 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:48:11,876 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:48:11,876 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:48:11,876 - src.rag_pipeline - INFO - Processing validated query: 'How does artificial intelligence work?'
2025-09-05 03:48:11,886 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does artificial intelligence work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.24it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.17it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.07it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.07it/s, batch=1/1, memory=N/A]
2025-09-05 03:48:12,256 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:48:12,257 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:48:12,808 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.921s
2025-09-05 03:48:12,812 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:48:12,813 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1211 tokens) in 0.003s
2025-09-05 03:48:29,580 - src.rag_pipeline - INFO - Generated response (287 tokens) in 16.764s
2025-09-05 03:48:29,708 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:48:29,768 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:48:29,792 - src.experiment_runner - INFO - Completed run 222/510: 18.24s
2025-09-05 03:48:29,792 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:48:29,792 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:48:29,792 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:48:29,792 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:48:29,792 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:48:29,792 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:48:29,792 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:48:29,797 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:48:29,798 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:48:29,798 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:48:29,798 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:48:29,798 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:48:29,798 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:48:30,315 - src.llm_wrapper - INFO - Model ready in 0.52s
2025-09-05 03:48:30,315 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:48:30,315 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:48:30,315 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:48:30,315 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:48:30,315 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:48:30,315 - src.rag_pipeline - INFO - Processing validated query: 'Explain deep learning algorithms.'
2025-09-05 03:48:30,322 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain deep learning algorithms....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.85it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.82it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.72it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.72it/s, batch=1/1, memory=N/A]
2025-09-05 03:48:30,684 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:48:30,685 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:48:31,076 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.754s
2025-09-05 03:48:31,079 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:48:31,080 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1198 tokens) in 0.003s
2025-09-05 03:48:36,229 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.149s
2025-09-05 03:48:36,345 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:48:36,401 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:48:36,408 - src.experiment_runner - INFO - Completed run 223/510: 6.44s
2025-09-05 03:48:36,409 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:48:36,409 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:48:36,409 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:48:36,409 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:48:36,409 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:48:36,409 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:48:36,409 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:48:36,416 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:48:36,416 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:48:36,417 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:48:36,417 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:48:36,417 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:48:36,418 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:48:36,926 - src.llm_wrapper - INFO - Model ready in 0.51s
2025-09-05 03:48:36,926 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:48:36,926 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:48:36,926 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:48:36,926 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:48:36,926 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:48:36,926 - src.rag_pipeline - INFO - Processing validated query: 'What are neural networks?'
2025-09-05 03:48:36,934 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are neural networks?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 10.48it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 10.28it/s, batch=1/1, memory=N/A]
2025-09-05 03:48:37,139 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:48:37,140 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:48:37,522 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.588s
2025-09-05 03:48:37,524 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:48:37,524 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1353 tokens) in 0.002s
2025-09-05 03:48:52,144 - src.rag_pipeline - INFO - Generated response (238 tokens) in 14.616s
2025-09-05 03:48:52,280 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:48:52,335 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:48:52,344 - src.experiment_runner - INFO - Completed run 224/510: 15.74s
2025-09-05 03:48:52,345 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:48:52,345 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:48:52,345 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:48:52,345 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:48:52,345 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:48:52,345 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:48:52,345 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:48:52,350 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:48:52,350 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:48:52,351 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:48:52,351 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:48:52,351 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:48:52,351 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:48:52,858 - src.llm_wrapper - INFO - Model ready in 0.51s
2025-09-05 03:48:52,858 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:48:52,858 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:48:52,858 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:48:52,858 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:48:52,858 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:48:52,858 - src.rag_pipeline - INFO - Processing validated query: 'How do large language models work?'
2025-09-05 03:48:52,865 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How do large language models work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.80it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.79it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.74it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.74it/s, batch=1/1, memory=N/A]
2025-09-05 03:48:53,245 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:48:53,248 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:48:53,624 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.759s
2025-09-05 03:48:53,626 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:48:53,627 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1337 tokens) in 0.002s
2025-09-05 03:48:59,081 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.454s
2025-09-05 03:48:59,209 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:48:59,260 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:48:59,271 - src.experiment_runner - INFO - Completed run 225/510: 6.74s
2025-09-05 03:48:59,272 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:48:59,272 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:48:59,272 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:48:59,272 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:48:59,272 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:48:59,272 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:48:59,272 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:48:59,279 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:48:59,279 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:48:59,279 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:48:59,279 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:48:59,280 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:48:59,280 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:48:59,775 - src.llm_wrapper - INFO - Model ready in 0.50s
2025-09-05 03:48:59,775 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:48:59,775 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:48:59,775 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:48:59,775 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:48:59,775 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:48:59,775 - src.rag_pipeline - INFO - Processing validated query: 'What is natural language processing?'
2025-09-05 03:48:59,782 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is natural language processing?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.15it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 10.97it/s, batch=1/1, memory=N/A]
2025-09-05 03:48:59,980 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:48:59,981 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:49:00,487 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.705s
2025-09-05 03:49:00,489 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:49:00,491 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1749 tokens) in 0.004s
2025-09-05 03:49:07,429 - src.rag_pipeline - INFO - Generated response (55 tokens) in 6.937s
2025-09-05 03:49:07,557 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:49:07,613 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:49:07,626 - src.experiment_runner - INFO - Completed run 226/510: 8.16s
2025-09-05 03:49:07,626 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:49:07,626 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:49:07,626 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:49:07,626 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:49:07,626 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:49:07,627 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:49:07,627 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:49:07,631 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:49:07,631 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:49:07,632 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:49:07,632 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:49:07,632 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:49:07,632 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:49:08,135 - src.llm_wrapper - INFO - Model ready in 0.50s
2025-09-05 03:49:08,136 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:49:08,136 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:49:08,136 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:49:08,136 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:49:08,136 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:49:08,136 - src.rag_pipeline - INFO - Processing validated query: 'Explain computer vision techniques.'
2025-09-05 03:49:08,143 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain computer vision techniques....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  7.59it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  7.58it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  7.49it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  7.48it/s, batch=1/1, memory=N/A]
2025-09-05 03:49:08,451 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:49:08,452 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:49:09,011 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.868s
2025-09-05 03:49:09,013 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:49:09,013 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1297 tokens) in 0.002s
2025-09-05 03:49:27,524 - src.rag_pipeline - INFO - Generated response (318 tokens) in 18.508s
2025-09-05 03:49:27,656 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:49:27,708 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:49:27,724 - src.experiment_runner - INFO - Completed run 227/510: 19.90s
2025-09-05 03:49:27,724 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:49:27,724 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:49:27,724 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:49:27,724 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:49:27,724 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:49:27,724 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:49:27,724 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:49:27,730 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:49:27,730 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:49:27,730 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:49:27,730 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:49:27,730 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:49:27,730 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:49:28,237 - src.llm_wrapper - INFO - Model ready in 0.51s
2025-09-05 03:49:28,237 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:49:28,237 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:49:28,237 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:49:28,237 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:49:28,237 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:49:28,237 - src.rag_pipeline - INFO - Processing validated query: 'What is reinforcement learning?'
2025-09-05 03:49:28,247 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is reinforcement learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.82it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.81it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.76it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.76it/s, batch=1/1, memory=N/A]
2025-09-05 03:49:28,620 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:49:28,621 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:49:29,186 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.939s
2025-09-05 03:49:29,193 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:49:29,193 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1236 tokens) in 0.007s
2025-09-05 03:49:40,845 - src.rag_pipeline - INFO - Generated response (180 tokens) in 11.650s
2025-09-05 03:49:40,990 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:49:41,044 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:49:41,059 - src.experiment_runner - INFO - Completed run 228/510: 13.13s
2025-09-05 03:49:41,059 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:49:41,060 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:49:41,060 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:49:41,060 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:49:41,060 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:49:41,060 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:49:41,060 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:49:41,066 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:49:41,066 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:49:41,066 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:49:41,067 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:49:41,067 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:49:41,067 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:49:41,593 - src.llm_wrapper - INFO - Model ready in 0.53s
2025-09-05 03:49:41,593 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:49:41,593 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:49:41,594 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:49:41,594 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:49:41,594 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:49:41,594 - src.rag_pipeline - INFO - Processing validated query: 'How does data preprocessing work?'
2025-09-05 03:49:41,603 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does data preprocessing work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.76it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.67it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.60it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.60it/s, batch=1/1, memory=N/A]
2025-09-05 03:49:42,002 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:49:42,004 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:49:42,652 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.049s
2025-09-05 03:49:42,654 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:49:42,654 - src.rag_pipeline - INFO - Built prompt with 5 contexts (909 tokens) in 0.002s
2025-09-05 03:49:51,508 - src.rag_pipeline - INFO - Generated response (139 tokens) in 8.853s
2025-09-05 03:49:51,656 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:49:51,702 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:49:51,722 - src.experiment_runner - INFO - Completed run 229/510: 10.45s
2025-09-05 03:49:51,722 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:49:51,722 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:49:51,722 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:49:51,722 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:49:51,722 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:49:51,722 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:49:51,722 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:49:51,728 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:49:51,728 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:49:51,728 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:49:51,728 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:49:51,728 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:49:51,728 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:49:52,237 - src.llm_wrapper - INFO - Model ready in 0.51s
2025-09-05 03:49:52,237 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:49:52,237 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:49:52,237 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:49:52,237 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:49:52,237 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:49:52,237 - src.rag_pipeline - INFO - Processing validated query: 'What are the challenges in AI development?'
2025-09-05 03:49:52,246 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are the challenges in AI development?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.22it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.18it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.15it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.15it/s, batch=1/1, memory=N/A]
2025-09-05 03:49:52,629 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:49:52,630 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:49:53,276 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.030s
2025-09-05 03:49:53,278 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:49:53,279 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1445 tokens) in 0.002s
2025-09-05 03:50:08,367 - src.rag_pipeline - INFO - Generated response (237 tokens) in 15.086s
2025-09-05 03:50:08,531 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:50:08,580 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:50:08,608 - src.experiment_runner - INFO - Completed run 230/510: 16.65s
2025-09-05 03:50:08,608 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:50:08,608 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:50:08,608 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:50:08,609 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:50:08,609 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:50:08,609 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:50:08,609 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:50:08,614 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:50:08,615 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:50:08,615 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:50:08,615 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:50:08,615 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:50:08,615 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:50:09,147 - src.llm_wrapper - INFO - Model ready in 0.53s
2025-09-05 03:50:09,147 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:50:09,147 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:50:09,147 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:50:09,147 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:50:09,147 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:50:09,147 - src.rag_pipeline - INFO - Processing validated query: 'What is machine learning?'
2025-09-05 03:50:09,154 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is machine learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.98it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.88it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.82it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.82it/s, batch=1/1, memory=N/A]
2025-09-05 03:50:09,583 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:50:09,584 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:50:10,036 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.882s
2025-09-05 03:50:10,040 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:50:10,042 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1162 tokens) in 0.004s
2025-09-05 03:50:15,697 - src.rag_pipeline - INFO - Generated response (59 tokens) in 5.654s
2025-09-05 03:50:15,849 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:50:15,916 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:50:15,931 - src.experiment_runner - INFO - Completed run 231/510: 7.09s
2025-09-05 03:50:15,931 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:50:15,931 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:50:15,931 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:50:15,931 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:50:15,931 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:50:15,931 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:50:15,931 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:50:15,937 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:50:15,937 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:50:15,937 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:50:15,937 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:50:15,937 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:50:15,937 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:50:16,470 - src.llm_wrapper - INFO - Model ready in 0.53s
2025-09-05 03:50:16,470 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:50:16,470 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:50:16,470 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:50:16,470 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:50:16,470 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:50:16,470 - src.rag_pipeline - INFO - Processing validated query: 'How does artificial intelligence work?'
2025-09-05 03:50:16,477 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does artificial intelligence work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.42it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.39it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.29it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.29it/s, batch=1/1, memory=N/A]
2025-09-05 03:50:16,722 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:50:16,723 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:50:17,117 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.640s
2025-09-05 03:50:17,118 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:50:17,119 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1211 tokens) in 0.002s
2025-09-05 03:50:33,820 - src.rag_pipeline - INFO - Generated response (287 tokens) in 16.699s
2025-09-05 03:50:34,032 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:50:34,091 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:50:34,106 - src.experiment_runner - INFO - Completed run 232/510: 17.89s
2025-09-05 03:50:34,106 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:50:34,106 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:50:34,106 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:50:34,106 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:50:34,106 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:50:34,106 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:50:34,106 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:50:34,112 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:50:34,112 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:50:34,112 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:50:34,112 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:50:34,113 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:50:34,113 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:50:34,623 - src.llm_wrapper - INFO - Model ready in 0.51s
2025-09-05 03:50:34,623 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:50:34,623 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:50:34,623 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:50:34,623 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:50:34,623 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:50:34,623 - src.rag_pipeline - INFO - Processing validated query: 'Explain deep learning algorithms.'
2025-09-05 03:50:34,630 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain deep learning algorithms....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.71it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.70it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.67it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.67it/s, batch=1/1, memory=N/A]
2025-09-05 03:50:34,970 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:50:34,971 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:50:35,416 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.786s
2025-09-05 03:50:35,419 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:50:35,419 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1198 tokens) in 0.002s
2025-09-05 03:50:40,543 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.123s
2025-09-05 03:50:40,709 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:50:40,767 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:50:40,782 - src.experiment_runner - INFO - Completed run 233/510: 6.44s
2025-09-05 03:50:40,782 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:50:40,783 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:50:40,783 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:50:40,783 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:50:40,783 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:50:40,783 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:50:40,783 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:50:40,788 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:50:40,788 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:50:40,788 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:50:40,788 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:50:40,788 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:50:40,788 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:50:41,344 - src.llm_wrapper - INFO - Model ready in 0.56s
2025-09-05 03:50:41,344 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:50:41,344 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:50:41,344 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:50:41,344 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:50:41,344 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:50:41,344 - src.rag_pipeline - INFO - Processing validated query: 'What are neural networks?'
2025-09-05 03:50:41,351 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are neural networks?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.58it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.56it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.51it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.50it/s, batch=1/1, memory=N/A]
2025-09-05 03:50:41,653 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:50:41,654 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:50:42,075 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.724s
2025-09-05 03:50:42,077 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:50:42,078 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1353 tokens) in 0.002s
2025-09-05 03:50:57,492 - src.rag_pipeline - INFO - Generated response (238 tokens) in 15.407s
2025-09-05 03:50:57,730 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:50:57,789 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:50:57,808 - src.experiment_runner - INFO - Completed run 234/510: 16.71s
2025-09-05 03:50:57,808 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:50:57,808 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:50:57,808 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:50:57,808 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:50:57,808 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:50:57,808 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:50:57,808 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:50:57,815 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:50:57,815 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:50:57,815 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:50:57,815 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:50:57,815 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:50:57,815 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:50:58,380 - src.llm_wrapper - INFO - Model ready in 0.56s
2025-09-05 03:50:58,380 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:50:58,380 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:50:58,380 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:50:58,380 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:50:58,380 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:50:58,380 - src.rag_pipeline - INFO - Processing validated query: 'How do large language models work?'
2025-09-05 03:50:58,386 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How do large language models work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.27it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.25it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.22it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.22it/s, batch=1/1, memory=N/A]
2025-09-05 03:50:58,829 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:50:58,830 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:50:59,221 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.834s
2025-09-05 03:50:59,223 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:50:59,223 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1337 tokens) in 0.002s
2025-09-05 03:51:04,805 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.580s
2025-09-05 03:51:04,979 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:51:05,039 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:51:05,056 - src.experiment_runner - INFO - Completed run 235/510: 7.00s
2025-09-05 03:51:05,056 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:51:05,056 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:51:05,057 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:51:05,057 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:51:05,057 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:51:05,057 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:51:05,057 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:51:05,062 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:51:05,063 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:51:05,063 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:51:05,063 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:51:05,063 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:51:05,063 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:51:05,575 - src.llm_wrapper - INFO - Model ready in 0.51s
2025-09-05 03:51:05,575 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:51:05,575 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:51:05,575 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:51:05,576 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:51:05,576 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:51:05,576 - src.rag_pipeline - INFO - Processing validated query: 'What is natural language processing?'
2025-09-05 03:51:05,587 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is natural language processing?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.76it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.75it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.70it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.70it/s, batch=1/1, memory=N/A]
2025-09-05 03:51:05,920 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:51:05,926 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:51:06,341 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.754s
2025-09-05 03:51:06,343 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:51:06,344 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1749 tokens) in 0.002s
2025-09-05 03:51:13,471 - src.rag_pipeline - INFO - Generated response (55 tokens) in 7.126s
2025-09-05 03:51:13,668 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:51:13,720 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:51:13,734 - src.experiment_runner - INFO - Completed run 236/510: 8.42s
2025-09-05 03:51:13,735 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:51:13,735 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:51:13,735 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:51:13,735 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:51:13,735 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:51:13,735 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:51:13,735 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:51:13,740 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:51:13,740 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:51:13,740 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:51:13,740 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:51:13,740 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:51:13,741 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:51:14,283 - src.llm_wrapper - INFO - Model ready in 0.54s
2025-09-05 03:51:14,283 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:51:14,283 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:51:14,283 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:51:14,283 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:51:14,283 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:51:14,283 - src.rag_pipeline - INFO - Processing validated query: 'Explain computer vision techniques.'
2025-09-05 03:51:14,292 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain computer vision techniques....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.96it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.94it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.86it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.86it/s, batch=1/1, memory=N/A]
2025-09-05 03:51:14,659 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:51:14,660 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:51:15,128 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.836s
2025-09-05 03:51:15,130 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:51:15,131 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1297 tokens) in 0.002s
2025-09-05 03:51:34,334 - src.rag_pipeline - INFO - Generated response (318 tokens) in 19.202s
2025-09-05 03:51:34,509 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:51:34,568 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:51:34,585 - src.experiment_runner - INFO - Completed run 237/510: 20.60s
2025-09-05 03:51:34,585 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:51:34,585 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:51:34,585 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:51:34,585 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:51:34,585 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:51:34,585 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:51:34,585 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:51:34,590 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:51:34,591 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:51:34,591 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:51:34,591 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:51:34,591 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:51:34,591 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:51:35,153 - src.llm_wrapper - INFO - Model ready in 0.56s
2025-09-05 03:51:35,156 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:51:35,156 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:51:35,156 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:51:35,156 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:51:35,156 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:51:35,156 - src.rag_pipeline - INFO - Processing validated query: 'What is reinforcement learning?'
2025-09-05 03:51:35,170 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is reinforcement learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.59it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.59it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.57it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.57it/s, batch=1/1, memory=N/A]
2025-09-05 03:51:35,586 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:51:35,587 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:51:36,053 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.883s
2025-09-05 03:51:36,057 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:51:36,057 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1236 tokens) in 0.004s
2025-09-05 03:51:47,801 - src.rag_pipeline - INFO - Generated response (180 tokens) in 11.741s
2025-09-05 03:51:48,009 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:51:48,071 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:51:48,086 - src.experiment_runner - INFO - Completed run 238/510: 13.22s
2025-09-05 03:51:48,086 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:51:48,086 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:51:48,086 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:51:48,086 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:51:48,086 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:51:48,086 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:51:48,086 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:51:48,092 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:51:48,092 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:51:48,092 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:51:48,092 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:51:48,092 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:51:48,092 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:51:48,611 - src.llm_wrapper - INFO - Model ready in 0.52s
2025-09-05 03:51:48,611 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:51:48,611 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:51:48,611 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:51:48,611 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:51:48,611 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:51:48,611 - src.rag_pipeline - INFO - Processing validated query: 'How does data preprocessing work?'
2025-09-05 03:51:48,618 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does data preprocessing work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.08it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.08it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.02it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.02it/s, batch=1/1, memory=N/A]
2025-09-05 03:51:48,987 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:51:48,990 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:51:49,383 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.765s
2025-09-05 03:51:49,385 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:51:49,385 - src.rag_pipeline - INFO - Built prompt with 5 contexts (909 tokens) in 0.002s
2025-09-05 03:51:58,143 - src.rag_pipeline - INFO - Generated response (139 tokens) in 8.757s
2025-09-05 03:51:58,264 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:51:58,316 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:51:58,323 - src.experiment_runner - INFO - Completed run 239/510: 10.06s
2025-09-05 03:51:58,323 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:51:58,323 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:51:58,323 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:51:58,323 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:51:58,323 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:51:58,323 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:51:58,323 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:51:58,329 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:51:58,329 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:51:58,329 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:51:58,329 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:51:58,329 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:51:58,330 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:51:58,834 - src.llm_wrapper - INFO - Model ready in 0.50s
2025-09-05 03:51:58,834 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:51:58,834 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:51:58,834 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:51:58,834 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:51:58,834 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:51:58,834 - src.rag_pipeline - INFO - Processing validated query: 'What are the challenges in AI development?'
2025-09-05 03:51:58,841 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are the challenges in AI development?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.54it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.46it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.37it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.36it/s, batch=1/1, memory=N/A]
2025-09-05 03:51:59,179 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:51:59,185 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:51:59,552 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.711s
2025-09-05 03:51:59,554 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:51:59,555 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1445 tokens) in 0.002s
2025-09-05 03:52:14,561 - src.rag_pipeline - INFO - Generated response (237 tokens) in 15.004s
2025-09-05 03:52:14,716 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:52:14,769 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:52:14,788 - src.experiment_runner - INFO - Completed run 240/510: 16.24s
2025-09-05 03:52:14,788 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:52:14,788 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:52:14,788 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:52:14,788 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:52:14,788 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:52:14,788 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:52:14,788 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:52:14,795 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:52:14,802 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:52:14,802 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:52:14,802 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:52:14,802 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:52:14,802 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:52:15,324 - src.llm_wrapper - INFO - Model ready in 0.52s
2025-09-05 03:52:15,324 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:52:15,324 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:52:15,324 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:52:15,324 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:52:15,324 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:52:15,324 - src.rag_pipeline - INFO - Processing validated query: 'What is machine learning?'
2025-09-05 03:52:15,332 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is machine learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.47it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.46it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.42it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.42it/s, batch=1/1, memory=N/A]
2025-09-05 03:52:15,757 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:52:15,760 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:52:16,192 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.860s
2025-09-05 03:52:16,194 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:52:16,194 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1162 tokens) in 0.002s
2025-09-05 03:52:21,849 - src.rag_pipeline - INFO - Generated response (59 tokens) in 5.654s
2025-09-05 03:52:21,965 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:52:22,019 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:52:22,026 - src.experiment_runner - INFO - Completed run 241/510: 7.06s
2025-09-05 03:52:22,026 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:52:22,026 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:52:22,026 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:52:22,026 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:52:22,026 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:52:22,026 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:52:22,026 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:52:22,031 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:52:22,032 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:52:22,032 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:52:22,032 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:52:22,032 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:52:22,032 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:52:22,549 - src.llm_wrapper - INFO - Model ready in 0.52s
2025-09-05 03:52:22,549 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:52:22,549 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:52:22,549 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:52:22,549 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:52:22,549 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:52:22,549 - src.rag_pipeline - INFO - Processing validated query: 'How does artificial intelligence work?'
2025-09-05 03:52:22,557 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does artificial intelligence work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  7.78it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  7.77it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  7.64it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  7.64it/s, batch=1/1, memory=N/A]
2025-09-05 03:52:22,882 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:52:22,884 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:52:23,356 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.799s
2025-09-05 03:52:23,358 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:52:23,359 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1211 tokens) in 0.002s
2025-09-05 03:52:40,039 - src.rag_pipeline - INFO - Generated response (287 tokens) in 16.677s
2025-09-05 03:52:40,185 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:52:40,244 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:52:40,276 - src.experiment_runner - INFO - Completed run 242/510: 18.01s
2025-09-05 03:52:40,276 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:52:40,276 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:52:40,276 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:52:40,276 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:52:40,276 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:52:40,276 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:52:40,276 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:52:40,285 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:52:40,286 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:52:40,286 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:52:40,286 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:52:40,286 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:52:40,286 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:52:40,811 - src.llm_wrapper - INFO - Model ready in 0.52s
2025-09-05 03:52:40,811 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:52:40,811 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:52:40,811 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:52:40,811 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:52:40,811 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:52:40,811 - src.rag_pipeline - INFO - Processing validated query: 'Explain deep learning algorithms.'
2025-09-05 03:52:40,821 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain deep learning algorithms....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.65it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.64it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.61it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.61it/s, batch=1/1, memory=N/A]
2025-09-05 03:52:41,209 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:52:41,212 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:52:41,786 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.965s
2025-09-05 03:52:41,788 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:52:41,789 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1198 tokens) in 0.002s
2025-09-05 03:52:46,933 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.143s
2025-09-05 03:52:47,065 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:52:47,128 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:52:47,156 - src.experiment_runner - INFO - Completed run 243/510: 6.66s
2025-09-05 03:52:47,156 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:52:47,156 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:52:47,156 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:52:47,156 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:52:47,156 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:52:47,156 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:52:47,156 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:52:47,165 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:52:47,165 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:52:47,165 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:52:47,165 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:52:47,165 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:52:47,165 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:52:47,664 - src.llm_wrapper - INFO - Model ready in 0.50s
2025-09-05 03:52:47,664 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:52:47,664 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:52:47,664 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:52:47,664 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:52:47,664 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:52:47,664 - src.rag_pipeline - INFO - Processing validated query: 'What are neural networks?'
2025-09-05 03:52:47,672 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are neural networks?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  8.41it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  8.39it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  8.28it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  8.28it/s, batch=1/1, memory=N/A]
2025-09-05 03:52:47,927 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:52:47,929 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:52:48,874 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.202s
2025-09-05 03:52:48,876 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:52:48,877 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1353 tokens) in 0.002s
2025-09-05 03:53:03,566 - src.rag_pipeline - INFO - Generated response (238 tokens) in 14.688s
2025-09-05 03:53:03,693 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:53:03,741 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:53:03,753 - src.experiment_runner - INFO - Completed run 244/510: 16.41s
2025-09-05 03:53:03,753 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:53:03,753 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:53:03,753 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:53:03,753 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:53:03,753 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:53:03,753 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:53:03,753 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:53:03,761 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:53:03,761 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:53:03,761 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:53:03,761 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:53:03,761 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:53:03,761 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:53:04,264 - src.llm_wrapper - INFO - Model ready in 0.50s
2025-09-05 03:53:04,265 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:53:04,265 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:53:04,265 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:53:04,265 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:53:04,265 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:53:04,265 - src.rag_pipeline - INFO - Processing validated query: 'How do large language models work?'
2025-09-05 03:53:04,271 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How do large language models work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.96it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.94it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.90it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.89it/s, batch=1/1, memory=N/A]
2025-09-05 03:53:04,600 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:53:04,603 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:53:05,907 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.636s
2025-09-05 03:53:05,909 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:53:05,909 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1337 tokens) in 0.002s
2025-09-05 03:53:11,378 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.468s
2025-09-05 03:53:11,512 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:53:11,559 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:53:11,565 - src.experiment_runner - INFO - Completed run 245/510: 7.63s
2025-09-05 03:53:11,565 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:53:11,565 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:53:11,565 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:53:11,565 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:53:11,565 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:53:11,565 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:53:11,565 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:53:11,570 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:53:11,570 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:53:11,571 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:53:11,571 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:53:11,571 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:53:11,571 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:53:12,076 - src.llm_wrapper - INFO - Model ready in 0.51s
2025-09-05 03:53:12,076 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:53:12,076 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:53:12,076 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:53:12,076 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:53:12,076 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:53:12,077 - src.rag_pipeline - INFO - Processing validated query: 'What is natural language processing?'
2025-09-05 03:53:12,084 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is natural language processing?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.35it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.33it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.26it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.26it/s, batch=1/1, memory=N/A]
2025-09-05 03:53:12,367 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:53:12,369 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:53:13,171 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.087s
2025-09-05 03:53:13,173 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:53:13,173 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1749 tokens) in 0.002s
2025-09-05 03:53:20,080 - src.rag_pipeline - INFO - Generated response (55 tokens) in 6.906s
2025-09-05 03:53:20,218 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:53:20,272 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:53:20,281 - src.experiment_runner - INFO - Completed run 246/510: 8.52s
2025-09-05 03:53:20,281 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:53:20,281 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:53:20,281 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:53:20,281 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:53:20,281 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:53:20,281 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:53:20,281 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:53:20,286 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:53:20,288 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:53:20,288 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:53:20,288 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:53:20,288 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:53:20,288 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:53:20,816 - src.llm_wrapper - INFO - Model ready in 0.53s
2025-09-05 03:53:20,816 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:53:20,816 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:53:20,816 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:53:20,816 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:53:20,816 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:53:20,816 - src.rag_pipeline - INFO - Processing validated query: 'Explain computer vision techniques.'
2025-09-05 03:53:20,826 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain computer vision techniques....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  7.52it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  7.49it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  7.31it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  7.31it/s, batch=1/1, memory=N/A]
2025-09-05 03:53:21,091 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:53:21,094 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:53:22,330 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.504s
2025-09-05 03:53:22,332 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:53:22,333 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1297 tokens) in 0.002s
2025-09-05 03:53:40,813 - src.rag_pipeline - INFO - Generated response (318 tokens) in 18.478s
2025-09-05 03:53:40,989 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:53:41,036 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:53:41,058 - src.experiment_runner - INFO - Completed run 247/510: 20.54s
2025-09-05 03:53:41,058 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:53:41,058 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:53:41,058 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:53:41,058 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:53:41,058 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:53:41,058 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:53:41,058 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:53:41,065 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:53:41,065 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:53:41,065 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:53:41,065 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:53:41,066 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:53:41,066 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:53:41,581 - src.llm_wrapper - INFO - Model ready in 0.52s
2025-09-05 03:53:41,581 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:53:41,581 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:53:41,581 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:53:41,581 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:53:41,582 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:53:41,582 - src.rag_pipeline - INFO - Processing validated query: 'What is reinforcement learning?'
2025-09-05 03:53:41,589 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is reinforcement learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.95it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.93it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.89it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.89it/s, batch=1/1, memory=N/A]
2025-09-05 03:53:41,902 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:53:41,903 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:53:43,038 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.449s
2025-09-05 03:53:43,040 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:53:43,040 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1236 tokens) in 0.002s
2025-09-05 03:53:54,591 - src.rag_pipeline - INFO - Generated response (180 tokens) in 11.550s
2025-09-05 03:53:54,731 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:53:54,788 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:53:54,797 - src.experiment_runner - INFO - Completed run 248/510: 13.53s
2025-09-05 03:53:54,797 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:53:54,797 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:53:54,797 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:53:54,797 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:53:54,797 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:53:54,797 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:53:54,797 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:53:54,804 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:53:54,804 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:53:54,804 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:53:54,804 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:53:54,804 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:53:54,805 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:53:55,284 - src.llm_wrapper - INFO - Model ready in 0.48s
2025-09-05 03:53:55,284 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:53:55,284 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:53:55,284 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:53:55,284 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:53:55,284 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:53:55,284 - src.rag_pipeline - INFO - Processing validated query: 'How does data preprocessing work?'
2025-09-05 03:53:55,295 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does data preprocessing work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.73it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.72it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.68it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.68it/s, batch=1/1, memory=N/A]
2025-09-05 03:53:55,621 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:53:55,625 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:53:56,834 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.539s
2025-09-05 03:53:56,836 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:53:56,836 - src.rag_pipeline - INFO - Built prompt with 5 contexts (909 tokens) in 0.002s
2025-09-05 03:54:05,587 - src.rag_pipeline - INFO - Generated response (139 tokens) in 8.750s
2025-09-05 03:54:05,724 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:54:05,774 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:54:05,786 - src.experiment_runner - INFO - Completed run 249/510: 10.79s
2025-09-05 03:54:05,786 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:54:05,786 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:54:05,786 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:54:05,786 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:54:05,786 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:54:05,786 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:54:05,786 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:54:05,791 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:54:05,791 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:54:05,791 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:54:05,791 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:54:05,792 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:54:05,792 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:54:06,304 - src.llm_wrapper - INFO - Model ready in 0.51s
2025-09-05 03:54:06,304 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:54:06,304 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:54:06,304 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:54:06,304 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:54:06,304 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:54:06,304 - src.rag_pipeline - INFO - Processing validated query: 'What are the challenges in AI development?'
2025-09-05 03:54:06,314 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are the challenges in AI development?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  7.79it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  7.76it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  7.65it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  7.65it/s, batch=1/1, memory=N/A]
2025-09-05 03:54:06,554 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:54:06,555 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:54:07,694 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.380s
2025-09-05 03:54:07,696 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:54:07,697 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1445 tokens) in 0.002s
2025-09-05 03:54:22,722 - src.rag_pipeline - INFO - Generated response (237 tokens) in 15.024s
2025-09-05 03:54:22,865 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:54:22,908 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:54:22,926 - src.experiment_runner - INFO - Completed run 250/510: 16.94s
2025-09-05 03:54:22,926 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:54:22,926 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:54:22,926 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:54:22,926 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:54:22,926 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:54:22,926 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:54:22,926 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:54:22,935 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:54:22,935 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:54:22,935 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:54:22,935 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:54:22,935 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:54:22,936 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:54:23,439 - src.llm_wrapper - INFO - Model ready in 0.50s
2025-09-05 03:54:23,439 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:54:23,439 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:54:23,439 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:54:23,439 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:54:23,439 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:54:23,439 - src.rag_pipeline - INFO - Processing validated query: 'What is machine learning?'
2025-09-05 03:54:23,448 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is machine learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.72it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.71it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.65it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.65it/s, batch=1/1, memory=N/A]
2025-09-05 03:54:23,786 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:54:23,787 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:54:24,860 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.411s
2025-09-05 03:54:24,861 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:54:24,862 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1162 tokens) in 0.002s
2025-09-05 03:54:30,451 - src.rag_pipeline - INFO - Generated response (59 tokens) in 5.588s
2025-09-05 03:54:30,596 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:54:30,651 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:54:30,663 - src.experiment_runner - INFO - Completed run 251/510: 7.53s
2025-09-05 03:54:30,663 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:54:30,663 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:54:30,663 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:54:30,663 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:54:30,663 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:54:30,663 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:54:30,663 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:54:30,668 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:54:30,669 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:54:30,669 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:54:30,669 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:54:30,669 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:54:30,669 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:54:31,188 - src.llm_wrapper - INFO - Model ready in 0.52s
2025-09-05 03:54:31,189 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:54:31,189 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:54:31,189 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:54:31,189 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:54:31,189 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:54:31,189 - src.rag_pipeline - INFO - Processing validated query: 'How does artificial intelligence work?'
2025-09-05 03:54:31,199 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does artificial intelligence work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.24it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.15it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.06it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.05it/s, batch=1/1, memory=N/A]
2025-09-05 03:54:31,486 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:54:31,487 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:54:32,873 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.675s
2025-09-05 03:54:32,875 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:54:32,876 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1211 tokens) in 0.002s
2025-09-05 03:54:49,475 - src.rag_pipeline - INFO - Generated response (287 tokens) in 16.597s
2025-09-05 03:54:49,626 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:54:49,676 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:54:49,688 - src.experiment_runner - INFO - Completed run 252/510: 18.82s
2025-09-05 03:54:49,688 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:54:49,688 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:54:49,688 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:54:49,688 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:54:49,688 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:54:49,688 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:54:49,688 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:54:49,694 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:54:49,694 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:54:49,694 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:54:49,694 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:54:49,694 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:54:49,694 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:54:50,175 - src.llm_wrapper - INFO - Model ready in 0.48s
2025-09-05 03:54:50,175 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:54:50,175 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:54:50,175 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:54:50,175 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:54:50,175 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:54:50,175 - src.rag_pipeline - INFO - Processing validated query: 'Explain deep learning algorithms.'
2025-09-05 03:54:50,182 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain deep learning algorithms....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.78it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.78it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.76it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.76it/s, batch=1/1, memory=N/A]
2025-09-05 03:54:50,550 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:54:50,551 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:54:51,103 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.920s
2025-09-05 03:54:51,105 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:54:51,105 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1198 tokens) in 0.002s
2025-09-05 03:54:56,234 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.129s
2025-09-05 03:54:56,380 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:54:56,425 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:54:56,434 - src.experiment_runner - INFO - Completed run 253/510: 6.55s
2025-09-05 03:54:56,434 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:54:56,434 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:54:56,435 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:54:56,435 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:54:56,435 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:54:56,435 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:54:56,435 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:54:56,441 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:54:56,442 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:54:56,442 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:54:56,442 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:54:56,442 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:54:56,442 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:54:56,977 - src.llm_wrapper - INFO - Model ready in 0.53s
2025-09-05 03:54:56,977 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:54:56,977 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:54:56,977 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:54:56,977 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:54:56,977 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:54:56,977 - src.rag_pipeline - INFO - Processing validated query: 'What are neural networks?'
2025-09-05 03:54:56,984 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are neural networks?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.41it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.40it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.29it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.29it/s, batch=1/1, memory=N/A]
2025-09-05 03:54:57,264 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:54:57,265 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:54:57,658 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.675s
2025-09-05 03:54:57,660 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:54:57,661 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1353 tokens) in 0.002s
2025-09-05 03:55:12,429 - src.rag_pipeline - INFO - Generated response (238 tokens) in 14.766s
2025-09-05 03:55:12,572 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:55:12,623 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:55:12,631 - src.experiment_runner - INFO - Completed run 254/510: 16.00s
2025-09-05 03:55:12,631 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:55:12,631 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:55:12,631 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:55:12,632 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:55:12,632 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:55:12,632 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:55:12,632 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:55:12,637 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:55:12,637 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:55:12,637 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:55:12,637 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:55:12,637 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:55:12,637 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:55:13,155 - src.llm_wrapper - INFO - Model ready in 0.52s
2025-09-05 03:55:13,155 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:55:13,155 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:55:13,155 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:55:13,155 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:55:13,155 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:55:13,155 - src.rag_pipeline - INFO - Processing validated query: 'How do large language models work?'
2025-09-05 03:55:13,162 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How do large language models work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.01it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.01it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.98it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.98it/s, batch=1/1, memory=N/A]
2025-09-05 03:55:13,559 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:55:13,560 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:55:13,995 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.833s
2025-09-05 03:55:13,997 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:55:13,997 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1337 tokens) in 0.002s
2025-09-05 03:55:19,471 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.473s
2025-09-05 03:55:19,591 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:55:19,641 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:55:19,649 - src.experiment_runner - INFO - Completed run 255/510: 6.84s
2025-09-05 03:55:19,649 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:55:19,649 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:55:19,649 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:55:19,649 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:55:19,649 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:55:19,649 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:55:19,649 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:55:19,655 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:55:19,655 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:55:19,655 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:55:19,655 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:55:19,655 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:55:19,655 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:55:20,181 - src.llm_wrapper - INFO - Model ready in 0.53s
2025-09-05 03:55:20,181 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:55:20,181 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:55:20,181 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:55:20,181 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:55:20,181 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:55:20,181 - src.rag_pipeline - INFO - Processing validated query: 'What is natural language processing?'
2025-09-05 03:55:20,188 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is natural language processing?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.45it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.42it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.36it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.36it/s, batch=1/1, memory=N/A]
2025-09-05 03:55:20,526 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:55:20,527 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:55:20,931 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.743s
2025-09-05 03:55:20,933 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:55:20,934 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1749 tokens) in 0.002s
2025-09-05 03:55:27,872 - src.rag_pipeline - INFO - Generated response (55 tokens) in 6.938s
2025-09-05 03:55:27,981 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:55:28,030 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:55:28,040 - src.experiment_runner - INFO - Completed run 256/510: 8.22s
2025-09-05 03:55:28,040 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:55:28,040 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:55:28,040 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:55:28,041 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:55:28,041 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:55:28,041 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:55:28,041 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:55:28,047 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:55:28,048 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:55:28,048 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:55:28,048 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:55:28,048 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:55:28,048 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:55:28,561 - src.llm_wrapper - INFO - Model ready in 0.51s
2025-09-05 03:55:28,561 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:55:28,561 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:55:28,561 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:55:28,561 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:55:28,561 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:55:28,561 - src.rag_pipeline - INFO - Processing validated query: 'Explain computer vision techniques.'
2025-09-05 03:55:28,568 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain computer vision techniques....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  9.52it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  9.48it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  9.33it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  9.33it/s, batch=1/1, memory=N/A]
2025-09-05 03:55:28,793 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:55:28,794 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:55:29,177 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.609s
2025-09-05 03:55:29,179 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:55:29,179 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1297 tokens) in 0.002s
2025-09-05 03:55:47,689 - src.rag_pipeline - INFO - Generated response (318 tokens) in 18.506s
2025-09-05 03:55:47,829 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:55:47,875 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:55:47,883 - src.experiment_runner - INFO - Completed run 257/510: 19.65s
2025-09-05 03:55:47,883 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:55:47,883 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:55:47,883 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:55:47,883 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:55:47,883 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:55:47,883 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:55:47,883 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:55:47,889 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:55:47,889 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:55:47,889 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:55:47,889 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:55:47,889 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:55:47,889 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:55:48,420 - src.llm_wrapper - INFO - Model ready in 0.53s
2025-09-05 03:55:48,420 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:55:48,420 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:55:48,420 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:55:48,420 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:55:48,420 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:55:48,420 - src.rag_pipeline - INFO - Processing validated query: 'What is reinforcement learning?'
2025-09-05 03:55:48,428 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is reinforcement learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.15it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.15it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.12it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.12it/s, batch=1/1, memory=N/A]
2025-09-05 03:55:48,793 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:55:48,794 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:55:49,266 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.838s
2025-09-05 03:55:49,272 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:55:49,273 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1236 tokens) in 0.006s
2025-09-05 03:56:00,851 - src.rag_pipeline - INFO - Generated response (180 tokens) in 11.577s
2025-09-05 03:56:01,000 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:56:01,055 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:56:01,065 - src.experiment_runner - INFO - Completed run 258/510: 12.97s
2025-09-05 03:56:01,065 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:56:01,065 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:56:01,065 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:56:01,065 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:56:01,065 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:56:01,065 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:56:01,065 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:56:01,070 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:56:01,070 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:56:01,070 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:56:01,070 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:56:01,071 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:56:01,071 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:56:01,577 - src.llm_wrapper - INFO - Model ready in 0.51s
2025-09-05 03:56:01,577 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:56:01,577 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:56:01,577 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:56:01,577 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:56:01,577 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:56:01,577 - src.rag_pipeline - INFO - Processing validated query: 'How does data preprocessing work?'
2025-09-05 03:56:01,589 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does data preprocessing work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.28it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.27it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.23it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.23it/s, batch=1/1, memory=N/A]
2025-09-05 03:56:01,917 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:56:01,921 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:56:02,323 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.734s
2025-09-05 03:56:02,325 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:56:02,325 - src.rag_pipeline - INFO - Built prompt with 5 contexts (909 tokens) in 0.002s
2025-09-05 03:56:11,109 - src.rag_pipeline - INFO - Generated response (139 tokens) in 8.783s
2025-09-05 03:56:11,261 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:56:11,310 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:56:11,317 - src.experiment_runner - INFO - Completed run 259/510: 10.04s
2025-09-05 03:56:11,317 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:56:11,317 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:56:11,317 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:56:11,317 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:56:11,317 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:56:11,317 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:56:11,317 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:56:11,322 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:56:11,322 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:56:11,322 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:56:11,322 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:56:11,323 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:56:11,323 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:56:11,825 - src.llm_wrapper - INFO - Model ready in 0.50s
2025-09-05 03:56:11,826 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:56:11,826 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:56:11,826 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:56:11,826 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:56:11,826 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:56:11,826 - src.rag_pipeline - INFO - Processing validated query: 'What are the challenges in AI development?'
2025-09-05 03:56:11,833 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are the challenges in AI development?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.75it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.73it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.68it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.68it/s, batch=1/1, memory=N/A]
2025-09-05 03:56:12,172 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:56:12,173 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:56:12,662 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.829s
2025-09-05 03:56:12,664 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:56:12,665 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1445 tokens) in 0.002s
2025-09-05 03:56:27,452 - src.rag_pipeline - INFO - Generated response (237 tokens) in 14.786s
2025-09-05 03:56:27,574 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:56:27,621 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:56:27,628 - src.experiment_runner - INFO - Completed run 260/510: 16.14s
2025-09-05 03:56:27,628 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:56:27,628 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:56:27,628 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:56:27,628 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:56:27,628 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:56:27,628 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:56:27,628 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:56:27,634 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:56:27,634 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:56:27,634 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:56:27,634 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:56:27,634 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:56:27,634 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:56:28,132 - src.llm_wrapper - INFO - Model ready in 0.50s
2025-09-05 03:56:28,132 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:56:28,132 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:56:28,132 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:56:28,132 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:56:28,132 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:56:28,132 - src.rag_pipeline - INFO - Processing validated query: 'What is machine learning?'
2025-09-05 03:56:28,140 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is machine learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  8.44it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  8.42it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  8.32it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  8.30it/s, batch=1/1, memory=N/A]
2025-09-05 03:56:28,374 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:56:28,375 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:56:28,705 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.565s
2025-09-05 03:56:28,707 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:56:28,707 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1162 tokens) in 0.002s
2025-09-05 03:56:34,332 - src.rag_pipeline - INFO - Generated response (59 tokens) in 5.615s
2025-09-05 03:56:34,514 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:56:34,562 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:56:34,568 - src.experiment_runner - INFO - Completed run 261/510: 6.71s
2025-09-05 03:56:34,569 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:56:34,569 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:56:34,569 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:56:34,569 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:56:34,569 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:56:34,569 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:56:34,569 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:56:34,575 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:56:34,576 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:56:34,576 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:56:34,576 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:56:34,576 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:56:34,576 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:56:35,104 - src.llm_wrapper - INFO - Model ready in 0.53s
2025-09-05 03:56:35,104 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:56:35,104 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:56:35,104 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:56:35,105 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:56:35,105 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:56:35,105 - src.rag_pipeline - INFO - Processing validated query: 'How does artificial intelligence work?'
2025-09-05 03:56:35,112 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does artificial intelligence work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.85it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.84it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.77it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.77it/s, batch=1/1, memory=N/A]
2025-09-05 03:56:35,380 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:56:35,381 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:56:35,819 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.706s
2025-09-05 03:56:35,821 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:56:35,822 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1211 tokens) in 0.002s
2025-09-05 03:56:53,308 - src.rag_pipeline - INFO - Generated response (287 tokens) in 17.481s
2025-09-05 03:56:53,608 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:56:53,676 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:56:53,686 - src.experiment_runner - INFO - Completed run 262/510: 18.74s
2025-09-05 03:56:53,686 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:56:53,686 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:56:53,687 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:56:53,687 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:56:53,687 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:56:53,687 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:56:53,687 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:56:53,692 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:56:53,692 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:56:53,692 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:56:53,692 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:56:53,692 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:56:53,692 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:56:54,290 - src.llm_wrapper - INFO - Model ready in 0.60s
2025-09-05 03:56:54,291 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:56:54,291 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:56:54,291 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:56:54,291 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:56:54,291 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:56:54,292 - src.rag_pipeline - INFO - Processing validated query: 'Explain deep learning algorithms.'
2025-09-05 03:56:54,302 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain deep learning algorithms....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.29it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.29it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.25it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.25it/s, batch=1/1, memory=N/A]
2025-09-05 03:56:54,909 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:56:54,910 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:56:55,358 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.055s
2025-09-05 03:56:55,366 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:56:55,367 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1198 tokens) in 0.006s
2025-09-05 03:57:00,762 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.391s
2025-09-05 03:57:00,946 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:57:01,006 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:57:01,013 - src.experiment_runner - INFO - Completed run 263/510: 7.08s
2025-09-05 03:57:01,013 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:57:01,013 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:57:01,013 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:57:01,013 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:57:01,013 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:57:01,013 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:57:01,013 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:57:01,019 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:57:01,019 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:57:01,019 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:57:01,019 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:57:01,019 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:57:01,019 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:57:01,559 - src.llm_wrapper - INFO - Model ready in 0.54s
2025-09-05 03:57:01,559 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:57:01,559 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:57:01,559 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:57:01,559 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:57:01,559 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:57:01,559 - src.rag_pipeline - INFO - Processing validated query: 'What are neural networks?'
2025-09-05 03:57:01,567 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are neural networks?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.15it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.14it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.07it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.07it/s, batch=1/1, memory=N/A]
2025-09-05 03:57:02,183 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:57:02,185 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:57:02,605 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.037s
2025-09-05 03:57:02,618 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:57:02,619 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1353 tokens) in 0.012s
2025-09-05 03:57:18,307 - src.rag_pipeline - INFO - Generated response (238 tokens) in 15.682s
2025-09-05 03:57:18,510 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:57:18,573 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:57:18,581 - src.experiment_runner - INFO - Completed run 264/510: 17.30s
2025-09-05 03:57:18,581 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:57:18,581 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:57:18,581 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:57:18,581 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:57:18,581 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:57:18,581 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:57:18,581 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:57:18,588 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:57:18,588 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:57:18,588 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:57:18,588 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:57:18,588 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:57:18,588 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:57:19,109 - src.llm_wrapper - INFO - Model ready in 0.52s
2025-09-05 03:57:19,109 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:57:19,109 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:57:19,109 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:57:19,109 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:57:19,109 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:57:19,109 - src.rag_pipeline - INFO - Processing validated query: 'How do large language models work?'
2025-09-05 03:57:19,117 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How do large language models work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.04it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.02it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.00it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.00it/s, batch=1/1, memory=N/A]
2025-09-05 03:57:19,746 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:57:19,748 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:57:20,160 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.043s
2025-09-05 03:57:20,171 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:57:20,172 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1337 tokens) in 0.009s
2025-09-05 03:57:26,003 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.826s
2025-09-05 03:57:26,191 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:57:26,248 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:57:26,257 - src.experiment_runner - INFO - Completed run 265/510: 7.42s
2025-09-05 03:57:26,257 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:57:26,257 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:57:26,257 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:57:26,257 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:57:26,257 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:57:26,257 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:57:26,257 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:57:26,263 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:57:26,263 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:57:26,263 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:57:26,263 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:57:26,263 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:57:26,263 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:57:26,801 - src.llm_wrapper - INFO - Model ready in 0.54s
2025-09-05 03:57:26,802 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:57:26,802 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:57:26,802 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:57:26,802 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:57:26,802 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:57:26,802 - src.rag_pipeline - INFO - Processing validated query: 'What is natural language processing?'
2025-09-05 03:57:26,810 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is natural language processing?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.92it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.92it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.89it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.89it/s, batch=1/1, memory=N/A]
2025-09-05 03:57:27,476 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:57:27,478 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:57:27,919 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.109s
2025-09-05 03:57:27,932 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:57:27,933 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1749 tokens) in 0.011s
2025-09-05 03:57:35,272 - src.rag_pipeline - INFO - Generated response (55 tokens) in 7.335s
2025-09-05 03:57:35,479 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:57:35,536 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:57:35,544 - src.experiment_runner - INFO - Completed run 266/510: 9.02s
2025-09-05 03:57:35,544 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:57:35,544 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:57:35,544 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:57:35,544 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:57:35,544 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:57:35,544 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:57:35,544 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:57:35,550 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:57:35,550 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:57:35,550 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:57:35,550 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:57:35,550 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:57:35,551 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:57:36,097 - src.llm_wrapper - INFO - Model ready in 0.55s
2025-09-05 03:57:36,097 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:57:36,097 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:57:36,097 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:57:36,097 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:57:36,097 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:57:36,097 - src.rag_pipeline - INFO - Processing validated query: 'Explain computer vision techniques.'
2025-09-05 03:57:36,105 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain computer vision techniques....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.12it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.11it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.08it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.08it/s, batch=1/1, memory=N/A]
2025-09-05 03:57:36,723 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:57:36,725 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:57:37,154 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.048s
2025-09-05 03:57:37,169 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:57:37,169 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1297 tokens) in 0.009s
2025-09-05 03:57:56,943 - src.rag_pipeline - INFO - Generated response (318 tokens) in 19.767s
2025-09-05 03:57:57,148 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:57:57,201 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:57:57,210 - src.experiment_runner - INFO - Completed run 267/510: 21.40s
2025-09-05 03:57:57,210 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:57:57,210 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:57:57,210 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:57:57,210 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:57:57,210 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:57:57,210 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:57:57,210 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:57:57,215 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:57:57,215 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:57:57,216 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:57:57,216 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:57:57,216 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:57:57,216 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:57:57,770 - src.llm_wrapper - INFO - Model ready in 0.55s
2025-09-05 03:57:57,770 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:57:57,770 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:57:57,770 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:57:57,770 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:57:57,770 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:57:57,770 - src.rag_pipeline - INFO - Processing validated query: 'What is reinforcement learning?'
2025-09-05 03:57:57,782 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is reinforcement learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.88it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.88it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.85it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.85it/s, batch=1/1, memory=N/A]
2025-09-05 03:57:58,460 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:57:58,463 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:57:58,877 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.095s
2025-09-05 03:57:58,885 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:57:58,886 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1236 tokens) in 0.008s
2025-09-05 03:58:11,292 - src.rag_pipeline - INFO - Generated response (180 tokens) in 12.395s
2025-09-05 03:58:11,496 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:58:11,549 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:58:11,559 - src.experiment_runner - INFO - Completed run 268/510: 14.08s
2025-09-05 03:58:11,559 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:58:11,559 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:58:11,559 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:58:11,559 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:58:11,559 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:58:11,559 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:58:11,559 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:58:11,565 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:58:11,566 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:58:11,566 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:58:11,566 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:58:11,566 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:58:11,566 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:58:12,112 - src.llm_wrapper - INFO - Model ready in 0.55s
2025-09-05 03:58:12,112 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:58:12,112 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:58:12,112 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:58:12,112 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:58:12,112 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:58:12,112 - src.rag_pipeline - INFO - Processing validated query: 'How does data preprocessing work?'
2025-09-05 03:58:12,119 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does data preprocessing work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.99it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.97it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.95it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.95it/s, batch=1/1, memory=N/A]
2025-09-05 03:58:12,751 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:58:12,754 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:58:13,202 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.083s
2025-09-05 03:58:13,211 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:58:13,211 - src.rag_pipeline - INFO - Built prompt with 5 contexts (909 tokens) in 0.008s
2025-09-05 03:58:22,489 - src.rag_pipeline - INFO - Generated response (139 tokens) in 9.273s
2025-09-05 03:58:22,696 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:58:22,745 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:58:22,755 - src.experiment_runner - INFO - Completed run 269/510: 10.93s
2025-09-05 03:58:22,755 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:58:22,755 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:58:22,755 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:58:22,755 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:58:22,755 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:58:22,755 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:58:22,755 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:58:22,761 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:58:22,761 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:58:22,761 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:58:22,761 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:58:22,762 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:58:22,762 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:58:23,312 - src.llm_wrapper - INFO - Model ready in 0.55s
2025-09-05 03:58:23,312 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:58:23,312 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:58:23,312 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:58:23,312 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:58:23,312 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:58:23,312 - src.rag_pipeline - INFO - Processing validated query: 'What are the challenges in AI development?'
2025-09-05 03:58:23,320 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are the challenges in AI development?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.81it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.80it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.78it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.78it/s, batch=1/1, memory=N/A]
2025-09-05 03:58:23,986 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:58:23,988 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:58:24,407 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.086s
2025-09-05 03:58:24,420 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:58:24,421 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1445 tokens) in 0.010s
2025-09-05 03:58:40,470 - src.rag_pipeline - INFO - Generated response (237 tokens) in 16.042s
2025-09-05 03:58:40,671 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:58:40,730 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:58:40,739 - src.experiment_runner - INFO - Completed run 270/510: 17.72s
2025-09-05 03:58:40,739 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:58:40,739 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:58:40,739 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:58:40,739 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:58:40,739 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:58:40,739 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:58:40,739 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:58:40,745 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:58:40,745 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:58:40,745 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:58:40,745 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:58:40,746 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:58:40,746 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:58:41,285 - src.llm_wrapper - INFO - Model ready in 0.54s
2025-09-05 03:58:41,285 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:58:41,285 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:58:41,285 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:58:41,285 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:58:41,285 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:58:41,285 - src.rag_pipeline - INFO - Processing validated query: 'What is machine learning?'
2025-09-05 03:58:41,303 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is machine learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.03it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.03it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.01it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.01it/s, batch=1/1, memory=N/A]
2025-09-05 03:58:41,913 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:58:41,915 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:58:42,357 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.054s
2025-09-05 03:58:42,366 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:58:42,366 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1162 tokens) in 0.007s
2025-09-05 03:58:48,296 - src.rag_pipeline - INFO - Generated response (59 tokens) in 5.925s
2025-09-05 03:58:48,500 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:58:48,552 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:58:48,559 - src.experiment_runner - INFO - Completed run 271/510: 7.56s
2025-09-05 03:58:48,559 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:58:48,559 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:58:48,559 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:58:48,559 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:58:48,559 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:58:48,559 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:58:48,559 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:58:48,565 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:58:48,565 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:58:48,565 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:58:48,565 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:58:48,565 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:58:48,565 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:58:49,102 - src.llm_wrapper - INFO - Model ready in 0.54s
2025-09-05 03:58:49,102 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:58:49,102 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:58:49,102 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:58:49,102 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:58:49,102 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:58:49,102 - src.rag_pipeline - INFO - Processing validated query: 'How does artificial intelligence work?'
2025-09-05 03:58:49,109 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does artificial intelligence work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.82it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.81it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.78it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.78it/s, batch=1/1, memory=N/A]
2025-09-05 03:58:49,778 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:58:49,780 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:58:50,195 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.085s
2025-09-05 03:58:50,208 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:58:50,209 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1211 tokens) in 0.012s
2025-09-05 03:59:08,062 - src.rag_pipeline - INFO - Generated response (287 tokens) in 17.845s
2025-09-05 03:59:08,244 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:59:08,295 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:59:08,301 - src.experiment_runner - INFO - Completed run 272/510: 19.50s
2025-09-05 03:59:08,302 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:59:08,302 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:59:08,302 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:59:08,302 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:59:08,302 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:59:08,302 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:59:08,302 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:59:08,307 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:59:08,307 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:59:08,308 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:59:08,308 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:59:08,308 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:59:08,308 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:59:08,845 - src.llm_wrapper - INFO - Model ready in 0.54s
2025-09-05 03:59:08,845 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:59:08,845 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:59:08,845 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:59:08,845 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:59:08,845 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:59:08,845 - src.rag_pipeline - INFO - Processing validated query: 'Explain deep learning algorithms.'
2025-09-05 03:59:08,859 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain deep learning algorithms....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.98it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.97it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.93it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.93it/s, batch=1/1, memory=N/A]
2025-09-05 03:59:09,436 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:59:09,437 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:59:09,851 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.992s
2025-09-05 03:59:09,858 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:59:09,858 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1198 tokens) in 0.005s
2025-09-05 03:59:15,360 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.498s
2025-09-05 03:59:15,542 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:59:15,595 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:59:15,600 - src.experiment_runner - INFO - Completed run 273/510: 7.06s
2025-09-05 03:59:15,600 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:59:15,600 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:59:15,600 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:59:15,601 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:59:15,601 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:59:15,601 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:59:15,601 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:59:15,607 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:59:15,608 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:59:15,608 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:59:15,608 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:59:15,608 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:59:15,608 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:59:16,141 - src.llm_wrapper - INFO - Model ready in 0.53s
2025-09-05 03:59:16,142 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:59:16,142 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:59:16,142 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:59:16,142 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:59:16,142 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:59:16,142 - src.rag_pipeline - INFO - Processing validated query: 'What are neural networks?'
2025-09-05 03:59:16,149 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are neural networks?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.96it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.94it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.92it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.92it/s, batch=1/1, memory=N/A]
2025-09-05 03:59:16,763 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:59:16,764 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:59:17,204 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.054s
2025-09-05 03:59:17,214 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:59:17,215 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1353 tokens) in 0.008s
2025-09-05 03:59:33,295 - src.rag_pipeline - INFO - Generated response (238 tokens) in 16.073s
2025-09-05 03:59:33,530 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:59:33,598 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:59:33,605 - src.experiment_runner - INFO - Completed run 274/510: 17.70s
2025-09-05 03:59:33,605 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:59:33,605 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:59:33,605 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:59:33,605 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:59:33,605 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:59:33,605 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:59:33,605 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:59:33,610 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:59:33,611 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:59:33,611 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:59:33,611 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:59:33,611 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:59:33,611 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:59:34,207 - src.llm_wrapper - INFO - Model ready in 0.60s
2025-09-05 03:59:34,207 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:59:34,207 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:59:34,207 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:59:34,207 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:59:34,207 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:59:34,207 - src.rag_pipeline - INFO - Processing validated query: 'How do large language models work?'
2025-09-05 03:59:34,215 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How do large language models work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.60it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.60it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.57it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.57it/s, batch=1/1, memory=N/A]
2025-09-05 03:59:35,040 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:59:35,043 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:59:35,506 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.290s
2025-09-05 03:59:35,516 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:59:35,517 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1337 tokens) in 0.010s
2025-09-05 03:59:41,324 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.802s
2025-09-05 03:59:41,528 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:59:41,589 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:59:41,596 - src.experiment_runner - INFO - Completed run 275/510: 7.72s
2025-09-05 03:59:41,596 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:59:41,596 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:59:41,596 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:59:41,596 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:59:41,596 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:59:41,596 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:59:41,596 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:59:41,601 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:59:41,602 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:59:41,602 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:59:41,602 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:59:41,602 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:59:41,602 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:59:42,174 - src.llm_wrapper - INFO - Model ready in 0.57s
2025-09-05 03:59:42,174 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:59:42,174 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:59:42,174 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:59:42,174 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:59:42,174 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:59:42,174 - src.rag_pipeline - INFO - Processing validated query: 'What is natural language processing?'
2025-09-05 03:59:42,181 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is natural language processing?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.87it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.87it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.85it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.85it/s, batch=1/1, memory=N/A]
2025-09-05 03:59:42,841 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:59:42,845 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:59:43,293 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.111s
2025-09-05 03:59:43,302 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:59:43,302 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1749 tokens) in 0.007s
2025-09-05 03:59:50,809 - src.rag_pipeline - INFO - Generated response (55 tokens) in 7.503s
2025-09-05 03:59:51,035 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 03:59:51,089 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 03:59:51,095 - src.experiment_runner - INFO - Completed run 276/510: 9.21s
2025-09-05 03:59:51,095 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 03:59:51,095 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 03:59:51,095 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 03:59:51,095 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 03:59:51,095 - src.embedding_service - INFO -   Device: mps
2025-09-05 03:59:51,095 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 03:59:51,095 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 03:59:51,101 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:59:51,101 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 03:59:51,102 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 03:59:51,102 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 03:59:51,102 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 03:59:51,102 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 03:59:51,662 - src.llm_wrapper - INFO - Model ready in 0.56s
2025-09-05 03:59:51,662 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 03:59:51,662 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 03:59:51,662 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 03:59:51,662 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 03:59:51,662 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 03:59:51,662 - src.rag_pipeline - INFO - Processing validated query: 'Explain computer vision techniques.'
2025-09-05 03:59:51,669 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain computer vision techniques....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.14it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.13it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.11it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.11it/s, batch=1/1, memory=N/A]
2025-09-05 03:59:52,248 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:59:52,251 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 03:59:52,722 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.053s
2025-09-05 03:59:52,729 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 03:59:52,729 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1297 tokens) in 0.006s
2025-09-05 04:00:12,836 - src.rag_pipeline - INFO - Generated response (318 tokens) in 20.085s
2025-09-05 04:00:13,203 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:00:13,266 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:00:13,274 - src.experiment_runner - INFO - Completed run 277/510: 21.74s
2025-09-05 04:00:13,275 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:00:13,275 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:00:13,275 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:00:13,275 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:00:13,275 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:00:13,275 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:00:13,275 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:00:13,281 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:00:13,282 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:00:13,282 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:00:13,282 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:00:13,282 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:00:13,283 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:00:13,879 - src.llm_wrapper - INFO - Model ready in 0.60s
2025-09-05 04:00:13,879 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:00:13,879 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:00:13,879 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:00:13,879 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:00:13,879 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:00:13,879 - src.rag_pipeline - INFO - Processing validated query: 'What is reinforcement learning?'
2025-09-05 04:00:13,888 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is reinforcement learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.01it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.95it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.81it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.81it/s, batch=1/1, memory=N/A]
2025-09-05 04:00:14,672 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:00:14,674 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:00:15,110 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.221s
2025-09-05 04:00:15,119 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:00:15,119 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1236 tokens) in 0.009s
2025-09-05 04:00:27,731 - src.rag_pipeline - INFO - Generated response (180 tokens) in 12.606s
2025-09-05 04:00:27,959 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:00:28,020 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:00:28,030 - src.experiment_runner - INFO - Completed run 278/510: 14.46s
2025-09-05 04:00:28,030 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:00:28,030 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:00:28,031 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:00:28,031 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:00:28,031 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:00:28,031 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:00:28,031 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:00:28,036 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:00:28,037 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:00:28,037 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:00:28,037 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:00:28,037 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:00:28,037 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:00:28,597 - src.llm_wrapper - INFO - Model ready in 0.56s
2025-09-05 04:00:28,598 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:00:28,598 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:00:28,598 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:00:28,598 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:00:28,598 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:00:28,598 - src.rag_pipeline - INFO - Processing validated query: 'How does data preprocessing work?'
2025-09-05 04:00:28,606 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does data preprocessing work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.01it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.00it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.95it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.95it/s, batch=1/1, memory=N/A]
2025-09-05 04:00:29,270 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:00:29,274 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:00:29,688 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.082s
2025-09-05 04:00:29,696 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:00:29,696 - src.rag_pipeline - INFO - Built prompt with 5 contexts (909 tokens) in 0.007s
2025-09-05 04:00:39,207 - src.rag_pipeline - INFO - Generated response (139 tokens) in 9.506s
2025-09-05 04:00:39,418 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:00:39,478 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:00:39,485 - src.experiment_runner - INFO - Completed run 279/510: 11.18s
2025-09-05 04:00:39,486 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:00:39,486 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:00:39,486 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:00:39,486 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:00:39,486 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:00:39,486 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:00:39,486 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:00:39,493 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:00:39,493 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:00:39,493 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:00:39,493 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:00:39,494 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:00:39,494 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:00:40,019 - src.llm_wrapper - INFO - Model ready in 0.53s
2025-09-05 04:00:40,020 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:00:40,020 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:00:40,020 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:00:40,020 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:00:40,020 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:00:40,020 - src.rag_pipeline - INFO - Processing validated query: 'What are the challenges in AI development?'
2025-09-05 04:00:40,027 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are the challenges in AI development?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.96it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.95it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.92it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.92it/s, batch=1/1, memory=N/A]
2025-09-05 04:00:40,811 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:00:40,814 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:00:41,254 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.226s
2025-09-05 04:00:41,265 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:00:41,266 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1445 tokens) in 0.010s
2025-09-05 04:00:57,373 - src.rag_pipeline - INFO - Generated response (237 tokens) in 16.097s
2025-09-05 04:00:57,605 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:00:57,663 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:00:57,670 - src.experiment_runner - INFO - Completed run 280/510: 17.89s
2025-09-05 04:00:57,670 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:00:57,670 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:00:57,671 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:00:57,671 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:00:57,671 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:00:57,671 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:00:57,671 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:00:57,676 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:00:57,677 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:00:57,677 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:00:57,677 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:00:57,677 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:00:57,677 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:00:58,280 - src.llm_wrapper - INFO - Model ready in 0.60s
2025-09-05 04:00:58,280 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:00:58,280 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:00:58,280 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:00:58,280 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:00:58,280 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:00:58,280 - src.rag_pipeline - INFO - Processing validated query: 'What is machine learning?'
2025-09-05 04:00:58,287 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is machine learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.06it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.06it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.01it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.01it/s, batch=1/1, memory=N/A]
2025-09-05 04:00:58,938 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:00:58,942 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:00:59,374 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.086s
2025-09-05 04:00:59,384 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:00:59,384 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1162 tokens) in 0.008s
2025-09-05 04:01:05,479 - src.rag_pipeline - INFO - Generated response (59 tokens) in 6.090s
2025-09-05 04:01:05,668 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:01:05,733 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:01:05,740 - src.experiment_runner - INFO - Completed run 281/510: 7.81s
2025-09-05 04:01:05,740 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:01:05,740 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:01:05,740 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:01:05,740 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:01:05,740 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:01:05,740 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:01:05,740 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:01:05,746 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:01:05,746 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:01:05,746 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:01:05,746 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:01:05,746 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:01:05,747 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:01:06,326 - src.llm_wrapper - INFO - Model ready in 0.58s
2025-09-05 04:01:06,327 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:01:06,327 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:01:06,327 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:01:06,327 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:01:06,327 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:01:06,327 - src.rag_pipeline - INFO - Processing validated query: 'How does artificial intelligence work?'
2025-09-05 04:01:06,337 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does artificial intelligence work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.03it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.02it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.00it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.99it/s, batch=1/1, memory=N/A]
2025-09-05 04:01:06,977 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:01:06,981 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:01:07,393 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.056s
2025-09-05 04:01:07,405 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:01:07,405 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1211 tokens) in 0.011s
2025-09-05 04:01:25,443 - src.rag_pipeline - INFO - Generated response (287 tokens) in 18.031s
2025-09-05 04:01:25,702 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:01:25,762 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:01:25,770 - src.experiment_runner - INFO - Completed run 282/510: 19.70s
2025-09-05 04:01:25,771 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:01:25,771 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:01:25,771 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:01:25,771 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:01:25,771 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:01:25,771 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:01:25,771 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:01:25,776 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:01:25,776 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:01:25,776 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:01:25,776 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:01:25,777 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:01:25,777 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:01:26,343 - src.llm_wrapper - INFO - Model ready in 0.57s
2025-09-05 04:01:26,344 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:01:26,344 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:01:26,344 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:01:26,344 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:01:26,344 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:01:26,344 - src.rag_pipeline - INFO - Processing validated query: 'Explain deep learning algorithms.'
2025-09-05 04:01:26,364 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain deep learning algorithms....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.70it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.69it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.67it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.67it/s, batch=1/1, memory=N/A]
2025-09-05 04:01:27,231 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:01:27,237 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:01:27,705 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.341s
2025-09-05 04:01:27,716 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:01:27,718 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1198 tokens) in 0.009s
2025-09-05 04:01:33,144 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.421s
2025-09-05 04:01:33,373 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:01:33,432 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:01:33,439 - src.experiment_runner - INFO - Completed run 283/510: 7.37s
2025-09-05 04:01:33,439 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:01:33,439 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:01:33,439 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:01:33,439 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:01:33,439 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:01:33,439 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:01:33,439 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:01:33,445 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:01:33,445 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:01:33,445 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:01:33,445 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:01:33,445 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:01:33,445 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:01:33,985 - src.llm_wrapper - INFO - Model ready in 0.54s
2025-09-05 04:01:33,985 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:01:33,985 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:01:33,985 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:01:33,985 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:01:33,985 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:01:33,985 - src.rag_pipeline - INFO - Processing validated query: 'What are neural networks?'
2025-09-05 04:01:33,993 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are neural networks?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.11it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.10it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.06it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.06it/s, batch=1/1, memory=N/A]
2025-09-05 04:01:34,626 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:01:34,629 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:01:35,102 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.109s
2025-09-05 04:01:35,110 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:01:35,111 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1353 tokens) in 0.007s
2025-09-05 04:01:50,841 - src.rag_pipeline - INFO - Generated response (238 tokens) in 15.725s
2025-09-05 04:01:51,065 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:01:51,123 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:01:51,131 - src.experiment_runner - INFO - Completed run 284/510: 17.40s
2025-09-05 04:01:51,131 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:01:51,131 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:01:51,131 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:01:51,131 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:01:51,131 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:01:51,131 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:01:51,131 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:01:51,140 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:01:51,141 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:01:51,141 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:01:51,141 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:01:51,141 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:01:51,141 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:01:51,704 - src.llm_wrapper - INFO - Model ready in 0.56s
2025-09-05 04:01:51,704 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:01:51,704 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:01:51,704 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:01:51,704 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:01:51,705 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:01:51,705 - src.rag_pipeline - INFO - Processing validated query: 'How do large language models work?'
2025-09-05 04:01:51,717 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How do large language models work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.00it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.99it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.96it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.96it/s, batch=1/1, memory=N/A]
2025-09-05 04:01:52,316 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:01:52,319 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:01:52,755 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.036s
2025-09-05 04:01:52,765 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:01:52,766 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1337 tokens) in 0.008s
2025-09-05 04:01:58,639 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.870s
2025-09-05 04:01:58,818 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:01:58,882 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:01:58,889 - src.experiment_runner - INFO - Completed run 285/510: 7.51s
2025-09-05 04:01:58,889 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:01:58,889 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:01:58,889 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:01:58,889 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:01:58,889 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:01:58,889 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:01:58,889 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:01:58,895 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:01:58,895 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:01:58,895 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:01:58,895 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:01:58,895 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:01:58,895 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:01:59,421 - src.llm_wrapper - INFO - Model ready in 0.53s
2025-09-05 04:01:59,422 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:01:59,422 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:01:59,422 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:01:59,422 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:01:59,422 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:01:59,422 - src.rag_pipeline - INFO - Processing validated query: 'What is natural language processing?'
2025-09-05 04:01:59,429 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is natural language processing?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.06it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.05it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.02it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.02it/s, batch=1/1, memory=N/A]
2025-09-05 04:02:00,017 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:02:00,019 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:02:00,456 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.026s
2025-09-05 04:02:00,460 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:02:00,460 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1749 tokens) in 0.004s
2025-09-05 04:02:07,845 - src.rag_pipeline - INFO - Generated response (55 tokens) in 7.380s
2025-09-05 04:02:08,040 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:02:08,098 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:02:08,104 - src.experiment_runner - INFO - Completed run 286/510: 8.96s
2025-09-05 04:02:08,105 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:02:08,105 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:02:08,105 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:02:08,105 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:02:08,105 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:02:08,105 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:02:08,105 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:02:08,110 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:02:08,110 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:02:08,111 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:02:08,111 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:02:08,111 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:02:08,111 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:02:08,687 - src.llm_wrapper - INFO - Model ready in 0.58s
2025-09-05 04:02:08,687 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:02:08,687 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:02:08,687 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:02:08,687 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:02:08,687 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:02:08,687 - src.rag_pipeline - INFO - Processing validated query: 'Explain computer vision techniques.'
2025-09-05 04:02:08,702 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain computer vision techniques....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.96it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.95it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.91it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.91it/s, batch=1/1, memory=N/A]
2025-09-05 04:02:09,407 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:02:09,409 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:02:09,847 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.144s
2025-09-05 04:02:09,858 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:02:09,859 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1297 tokens) in 0.009s
2025-09-05 04:02:29,765 - src.rag_pipeline - INFO - Generated response (318 tokens) in 19.898s
2025-09-05 04:02:29,981 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:02:30,041 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:02:30,049 - src.experiment_runner - INFO - Completed run 287/510: 21.66s
2025-09-05 04:02:30,049 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:02:30,049 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:02:30,049 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:02:30,049 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:02:30,049 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:02:30,049 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:02:30,049 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:02:30,055 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:02:30,055 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:02:30,055 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:02:30,055 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:02:30,056 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:02:30,056 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:02:30,650 - src.llm_wrapper - INFO - Model ready in 0.59s
2025-09-05 04:02:30,650 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:02:30,650 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:02:30,650 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:02:30,651 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:02:30,651 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:02:30,651 - src.rag_pipeline - INFO - Processing validated query: 'What is reinforcement learning?'
2025-09-05 04:02:30,660 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is reinforcement learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.09it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.07it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.05it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.05it/s, batch=1/1, memory=N/A]
2025-09-05 04:02:31,303 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:02:31,306 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:02:31,760 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.099s
2025-09-05 04:02:31,770 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:02:31,770 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1236 tokens) in 0.008s
2025-09-05 04:02:44,269 - src.rag_pipeline - INFO - Generated response (180 tokens) in 12.494s
2025-09-05 04:02:44,499 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:02:44,560 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:02:44,566 - src.experiment_runner - INFO - Completed run 288/510: 14.22s
2025-09-05 04:02:44,566 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:02:44,566 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:02:44,566 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:02:44,566 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:02:44,566 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:02:44,566 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:02:44,566 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:02:44,572 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:02:44,573 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:02:44,573 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:02:44,573 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:02:44,573 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:02:44,573 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:02:45,163 - src.llm_wrapper - INFO - Model ready in 0.59s
2025-09-05 04:02:45,164 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:02:45,164 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:02:45,164 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:02:45,164 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:02:45,164 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:02:45,164 - src.rag_pipeline - INFO - Processing validated query: 'How does data preprocessing work?'
2025-09-05 04:02:45,171 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does data preprocessing work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.95it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.94it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.91it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.91it/s, batch=1/1, memory=N/A]
2025-09-05 04:02:45,834 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:02:45,837 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:02:46,277 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.105s
2025-09-05 04:02:46,287 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:02:46,288 - src.rag_pipeline - INFO - Built prompt with 5 contexts (909 tokens) in 0.009s
2025-09-05 04:02:55,710 - src.rag_pipeline - INFO - Generated response (139 tokens) in 9.415s
2025-09-05 04:02:55,944 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:02:56,007 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:02:56,015 - src.experiment_runner - INFO - Completed run 289/510: 11.14s
2025-09-05 04:02:56,016 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:02:56,016 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:02:56,016 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:02:56,016 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:02:56,016 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:02:56,016 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:02:56,016 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:02:56,021 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:02:56,021 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:02:56,021 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:02:56,021 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:02:56,022 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:02:56,022 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:02:56,580 - src.llm_wrapper - INFO - Model ready in 0.56s
2025-09-05 04:02:56,580 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:02:56,580 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:02:56,580 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:02:56,581 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:02:56,581 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:02:56,581 - src.rag_pipeline - INFO - Processing validated query: 'What are the challenges in AI development?'
2025-09-05 04:02:56,589 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are the challenges in AI development?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.91it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.90it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.88it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.88it/s, batch=1/1, memory=N/A]
2025-09-05 04:02:57,271 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:02:57,273 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:02:57,723 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.134s
2025-09-05 04:02:57,732 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:02:57,733 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1445 tokens) in 0.008s
2025-09-05 04:03:13,826 - src.rag_pipeline - INFO - Generated response (237 tokens) in 16.087s
2025-09-05 04:03:14,037 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:03:14,101 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:03:14,109 - src.experiment_runner - INFO - Completed run 290/510: 17.81s
2025-09-05 04:03:14,109 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:03:14,109 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:03:14,109 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:03:14,109 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:03:14,109 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:03:14,109 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:03:14,109 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:03:14,116 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:03:14,116 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:03:14,116 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:03:14,116 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:03:14,116 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:03:14,116 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:03:14,645 - src.llm_wrapper - INFO - Model ready in 0.53s
2025-09-05 04:03:14,646 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:03:14,646 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:03:14,646 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:03:14,646 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:03:14,646 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:03:14,646 - src.rag_pipeline - INFO - Processing validated query: 'What is machine learning?'
2025-09-05 04:03:14,654 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is machine learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.92it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.90it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.87it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.86it/s, batch=1/1, memory=N/A]
2025-09-05 04:03:15,361 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:03:15,366 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:03:15,840 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.186s
2025-09-05 04:03:15,850 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:03:15,851 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1162 tokens) in 0.010s
2025-09-05 04:03:21,897 - src.rag_pipeline - INFO - Generated response (59 tokens) in 6.042s
2025-09-05 04:03:22,084 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:03:22,136 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:03:22,143 - src.experiment_runner - INFO - Completed run 291/510: 7.79s
2025-09-05 04:03:22,143 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:03:22,143 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:03:22,143 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:03:22,143 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:03:22,143 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:03:22,143 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:03:22,143 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:03:22,148 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:03:22,149 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:03:22,149 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:03:22,149 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:03:22,149 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:03:22,149 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:03:22,697 - src.llm_wrapper - INFO - Model ready in 0.55s
2025-09-05 04:03:22,697 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:03:22,697 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:03:22,697 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:03:22,697 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:03:22,697 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:03:22,697 - src.rag_pipeline - INFO - Processing validated query: 'How does artificial intelligence work?'
2025-09-05 04:03:22,704 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does artificial intelligence work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.11it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.10it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.07it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.07it/s, batch=1/1, memory=N/A]
2025-09-05 04:03:23,547 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:03:23,549 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:03:23,994 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.290s
2025-09-05 04:03:24,003 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:03:24,004 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1211 tokens) in 0.008s
2025-09-05 04:03:42,231 - src.rag_pipeline - INFO - Generated response (287 tokens) in 18.220s
2025-09-05 04:03:42,450 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:03:42,510 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:03:42,516 - src.experiment_runner - INFO - Completed run 292/510: 20.09s
2025-09-05 04:03:42,517 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:03:42,517 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:03:42,517 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:03:42,517 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:03:42,517 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:03:42,517 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:03:42,517 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:03:42,524 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:03:42,524 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:03:42,524 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:03:42,524 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:03:42,524 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:03:42,524 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:03:43,057 - src.llm_wrapper - INFO - Model ready in 0.53s
2025-09-05 04:03:43,057 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:03:43,057 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:03:43,057 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:03:43,057 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:03:43,058 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:03:43,058 - src.rag_pipeline - INFO - Processing validated query: 'Explain deep learning algorithms.'
2025-09-05 04:03:43,065 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain deep learning algorithms....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.83it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.82it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.78it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.78it/s, batch=1/1, memory=N/A]
2025-09-05 04:03:43,909 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:03:43,911 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:03:44,379 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.313s
2025-09-05 04:03:44,389 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:03:44,389 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1198 tokens) in 0.008s
2025-09-05 04:03:49,871 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.476s
2025-09-05 04:03:50,143 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:03:50,203 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:03:50,210 - src.experiment_runner - INFO - Completed run 293/510: 7.36s
2025-09-05 04:03:50,210 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:03:50,210 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:03:50,210 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:03:50,210 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:03:50,210 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:03:50,211 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:03:50,211 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:03:50,216 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:03:50,217 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:03:50,217 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:03:50,217 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:03:50,217 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:03:50,217 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:03:50,833 - src.llm_wrapper - INFO - Model ready in 0.62s
2025-09-05 04:03:50,833 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:03:50,833 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:03:50,834 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:03:50,834 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:03:50,834 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:03:50,834 - src.rag_pipeline - INFO - Processing validated query: 'What are neural networks?'
2025-09-05 04:03:50,842 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are neural networks?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.87it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.86it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.83it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.83it/s, batch=1/1, memory=N/A]
2025-09-05 04:03:51,591 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:03:51,595 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:03:52,061 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.219s
2025-09-05 04:03:52,071 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:03:52,072 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1353 tokens) in 0.009s
2025-09-05 04:04:07,777 - src.rag_pipeline - INFO - Generated response (238 tokens) in 15.697s
2025-09-05 04:04:08,024 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:04:08,086 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:04:08,096 - src.experiment_runner - INFO - Completed run 294/510: 17.57s
2025-09-05 04:04:08,096 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:04:08,096 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:04:08,096 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:04:08,096 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:04:08,096 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:04:08,096 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:04:08,096 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:04:08,102 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:04:08,102 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:04:08,102 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:04:08,102 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:04:08,102 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:04:08,103 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:04:08,658 - src.llm_wrapper - INFO - Model ready in 0.56s
2025-09-05 04:04:08,658 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:04:08,658 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:04:08,658 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:04:08,659 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:04:08,659 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:04:08,659 - src.rag_pipeline - INFO - Processing validated query: 'How do large language models work?'
2025-09-05 04:04:08,666 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How do large language models work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.84it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.82it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.80it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.80it/s, batch=1/1, memory=N/A]
2025-09-05 04:04:09,336 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:04:09,338 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:04:09,762 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.095s
2025-09-05 04:04:09,773 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:04:09,774 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1337 tokens) in 0.009s
2025-09-05 04:04:15,579 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.801s
2025-09-05 04:04:15,777 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:04:15,829 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:04:15,837 - src.experiment_runner - INFO - Completed run 295/510: 7.48s
2025-09-05 04:04:15,837 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:04:15,837 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:04:15,837 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:04:15,837 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:04:15,837 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:04:15,837 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:04:15,837 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:04:15,844 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:04:15,845 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:04:15,845 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:04:15,845 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:04:15,845 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:04:15,845 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:04:16,380 - src.llm_wrapper - INFO - Model ready in 0.53s
2025-09-05 04:04:16,380 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:04:16,380 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:04:16,380 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:04:16,380 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:04:16,380 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:04:16,380 - src.rag_pipeline - INFO - Processing validated query: 'What is natural language processing?'
2025-09-05 04:04:16,387 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is natural language processing?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.99it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.98it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.95it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.95it/s, batch=1/1, memory=N/A]
2025-09-05 04:04:17,076 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:04:17,078 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:04:17,502 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.114s
2025-09-05 04:04:17,510 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:04:17,511 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1749 tokens) in 0.007s
2025-09-05 04:04:24,913 - src.rag_pipeline - INFO - Generated response (55 tokens) in 7.399s
2025-09-05 04:04:25,116 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:04:25,172 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:04:25,177 - src.experiment_runner - INFO - Completed run 296/510: 9.08s
2025-09-05 04:04:25,178 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:04:25,178 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:04:25,178 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:04:25,178 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:04:25,178 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:04:25,178 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:04:25,178 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:04:25,184 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:04:25,185 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:04:25,185 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:04:25,185 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:04:25,185 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:04:25,185 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:04:25,727 - src.llm_wrapper - INFO - Model ready in 0.54s
2025-09-05 04:04:25,727 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:04:25,727 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:04:25,727 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:04:25,727 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:04:25,727 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:04:25,727 - src.rag_pipeline - INFO - Processing validated query: 'Explain computer vision techniques.'
2025-09-05 04:04:25,739 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain computer vision techniques....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.00it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.98it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.92it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.92it/s, batch=1/1, memory=N/A]
2025-09-05 04:04:26,358 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:04:26,359 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:04:26,774 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.035s
2025-09-05 04:04:26,787 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:04:26,790 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1297 tokens) in 0.012s
2025-09-05 04:04:46,739 - src.rag_pipeline - INFO - Generated response (318 tokens) in 19.942s
2025-09-05 04:04:46,981 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:04:47,043 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:04:47,051 - src.experiment_runner - INFO - Completed run 297/510: 21.56s
2025-09-05 04:04:47,052 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:04:47,052 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:04:47,052 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:04:47,052 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:04:47,052 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:04:47,052 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:04:47,052 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:04:47,058 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:04:47,058 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:04:47,058 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:04:47,058 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:04:47,058 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:04:47,058 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:04:47,638 - src.llm_wrapper - INFO - Model ready in 0.58s
2025-09-05 04:04:47,639 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:04:47,639 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:04:47,639 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:04:47,639 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:04:47,639 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:04:47,639 - src.rag_pipeline - INFO - Processing validated query: 'What is reinforcement learning?'
2025-09-05 04:04:47,651 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is reinforcement learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.32it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.31it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.30it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.30it/s, batch=1/1, memory=N/A]
2025-09-05 04:04:48,677 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:04:48,682 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:04:49,200 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.548s
2025-09-05 04:04:49,211 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:04:49,213 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1236 tokens) in 0.010s
2025-09-05 04:05:01,648 - src.rag_pipeline - INFO - Generated response (180 tokens) in 12.428s
2025-09-05 04:05:01,876 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:05:01,927 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:05:01,934 - src.experiment_runner - INFO - Completed run 298/510: 14.60s
2025-09-05 04:05:01,934 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:05:01,934 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:05:01,934 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:05:01,934 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:05:01,934 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:05:01,934 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:05:01,934 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:05:01,939 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:05:01,940 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:05:01,940 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:05:01,940 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:05:01,940 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:05:01,940 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:05:02,476 - src.llm_wrapper - INFO - Model ready in 0.54s
2025-09-05 04:05:02,476 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:05:02,476 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:05:02,476 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:05:02,476 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:05:02,476 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:05:02,476 - src.rag_pipeline - INFO - Processing validated query: 'How does data preprocessing work?'
2025-09-05 04:05:02,483 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does data preprocessing work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.10it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.10it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.07it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.07it/s, batch=1/1, memory=N/A]
2025-09-05 04:05:03,053 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:05:03,058 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:05:03,516 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.031s
2025-09-05 04:05:03,520 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:05:03,521 - src.rag_pipeline - INFO - Built prompt with 5 contexts (909 tokens) in 0.004s
2025-09-05 04:05:13,003 - src.rag_pipeline - INFO - Generated response (139 tokens) in 9.477s
2025-09-05 04:05:13,221 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:05:13,275 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:05:13,282 - src.experiment_runner - INFO - Completed run 299/510: 11.07s
2025-09-05 04:05:13,282 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:05:13,283 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:05:13,283 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:05:13,283 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:05:13,283 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:05:13,283 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:05:13,283 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:05:13,288 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:05:13,288 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:05:13,288 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:05:13,288 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:05:13,288 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:05:13,288 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:05:13,835 - src.llm_wrapper - INFO - Model ready in 0.55s
2025-09-05 04:05:13,835 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:05:13,835 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:05:13,835 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:05:13,835 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:05:13,835 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:05:13,835 - src.rag_pipeline - INFO - Processing validated query: 'What are the challenges in AI development?'
2025-09-05 04:05:13,844 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are the challenges in AI development?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.83it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.82it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.78it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.78it/s, batch=1/1, memory=N/A]
2025-09-05 04:05:14,488 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:05:14,490 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:05:14,886 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.042s
2025-09-05 04:05:14,895 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:05:14,897 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1445 tokens) in 0.008s
2025-09-05 04:05:30,983 - src.rag_pipeline - INFO - Generated response (237 tokens) in 16.080s
2025-09-05 04:05:31,186 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:05:31,240 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:05:31,248 - src.experiment_runner - INFO - Completed run 300/510: 17.70s
2025-09-05 04:05:31,248 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:05:31,248 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:05:31,249 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:05:31,249 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:05:31,249 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:05:31,249 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:05:31,249 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:05:31,254 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:05:31,254 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:05:31,254 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:05:31,254 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:05:31,254 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:05:31,255 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:05:31,804 - src.llm_wrapper - INFO - Model ready in 0.55s
2025-09-05 04:05:31,805 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:05:31,805 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:05:31,805 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:05:31,805 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:05:31,805 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:05:31,805 - src.rag_pipeline - INFO - Processing validated query: 'What is machine learning?'
2025-09-05 04:05:31,812 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is machine learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.03it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.02it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.99it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.99it/s, batch=1/1, memory=N/A]
2025-09-05 04:05:32,418 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:05:32,420 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:05:32,845 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.033s
2025-09-05 04:05:32,851 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:05:32,851 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1162 tokens) in 0.005s
2025-09-05 04:05:39,027 - src.rag_pipeline - INFO - Generated response (59 tokens) in 6.171s
2025-09-05 04:05:39,256 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:05:39,316 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:05:39,322 - src.experiment_runner - INFO - Completed run 301/510: 7.78s
2025-09-05 04:05:39,323 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:05:39,323 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:05:39,323 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:05:39,323 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:05:39,323 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:05:39,323 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:05:39,323 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:05:39,329 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:05:39,329 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:05:39,329 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:05:39,329 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:05:39,329 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:05:39,329 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:05:39,957 - src.llm_wrapper - INFO - Model ready in 0.63s
2025-09-05 04:05:39,957 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:05:39,957 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:05:39,957 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:05:39,957 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:05:39,957 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:05:39,957 - src.rag_pipeline - INFO - Processing validated query: 'How does artificial intelligence work?'
2025-09-05 04:05:39,965 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does artificial intelligence work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.37it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.34it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.28it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.28it/s, batch=1/1, memory=N/A]
2025-09-05 04:05:40,773 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:05:40,775 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:05:41,226 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.261s
2025-09-05 04:05:41,233 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:05:41,234 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1211 tokens) in 0.007s
2025-09-05 04:05:59,520 - src.rag_pipeline - INFO - Generated response (287 tokens) in 18.279s
2025-09-05 04:05:59,753 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:05:59,821 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:05:59,829 - src.experiment_runner - INFO - Completed run 302/510: 20.20s
2025-09-05 04:05:59,829 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:05:59,829 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:05:59,829 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:05:59,829 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:05:59,830 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:05:59,830 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:05:59,830 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:05:59,835 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:05:59,835 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:05:59,835 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:05:59,835 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:05:59,835 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:05:59,835 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:06:00,425 - src.llm_wrapper - INFO - Model ready in 0.59s
2025-09-05 04:06:00,425 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:06:00,425 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:06:00,425 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:06:00,425 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:06:00,425 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:06:00,425 - src.rag_pipeline - INFO - Processing validated query: 'Explain deep learning algorithms.'
2025-09-05 04:06:00,432 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain deep learning algorithms....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.62it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.60it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.58it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.58it/s, batch=1/1, memory=N/A]
2025-09-05 04:06:01,158 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:06:01,160 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:06:01,627 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.195s
2025-09-05 04:06:01,634 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:06:01,635 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1198 tokens) in 0.006s
2025-09-05 04:06:07,154 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.517s
2025-09-05 04:06:07,344 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:06:07,406 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:06:07,414 - src.experiment_runner - INFO - Completed run 303/510: 7.33s
2025-09-05 04:06:07,414 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:06:07,414 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:06:07,414 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:06:07,414 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:06:07,414 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:06:07,414 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:06:07,414 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:06:07,419 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:06:07,420 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:06:07,420 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:06:07,420 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:06:07,420 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:06:07,420 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:06:07,960 - src.llm_wrapper - INFO - Model ready in 0.54s
2025-09-05 04:06:07,960 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:06:07,960 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:06:07,960 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:06:07,960 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:06:07,960 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:06:07,960 - src.rag_pipeline - INFO - Processing validated query: 'What are neural networks?'
2025-09-05 04:06:07,968 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are neural networks?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.01it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.00it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.99it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.99it/s, batch=1/1, memory=N/A]
2025-09-05 04:06:08,634 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:06:08,636 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:06:09,050 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.082s
2025-09-05 04:06:09,055 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:06:09,058 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1353 tokens) in 0.005s
2025-09-05 04:06:24,928 - src.rag_pipeline - INFO - Generated response (238 tokens) in 15.865s
2025-09-05 04:06:25,140 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:06:25,196 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:06:25,206 - src.experiment_runner - INFO - Completed run 304/510: 17.51s
2025-09-05 04:06:25,206 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:06:25,206 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:06:25,206 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:06:25,206 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:06:25,206 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:06:25,206 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:06:25,206 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:06:25,211 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:06:25,211 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:06:25,212 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:06:25,212 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:06:25,212 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:06:25,212 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:06:25,747 - src.llm_wrapper - INFO - Model ready in 0.53s
2025-09-05 04:06:25,747 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:06:25,747 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:06:25,747 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:06:25,747 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:06:25,747 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:06:25,747 - src.rag_pipeline - INFO - Processing validated query: 'How do large language models work?'
2025-09-05 04:06:25,766 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How do large language models work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.21it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.17it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.10it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.09it/s, batch=1/1, memory=N/A]
2025-09-05 04:06:26,455 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:06:26,458 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:06:26,894 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.127s
2025-09-05 04:06:26,909 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:06:26,910 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1337 tokens) in 0.014s
2025-09-05 04:06:32,856 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.942s
2025-09-05 04:06:33,078 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:06:33,133 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:06:33,142 - src.experiment_runner - INFO - Completed run 305/510: 7.65s
2025-09-05 04:06:33,142 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:06:33,142 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:06:33,142 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:06:33,142 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:06:33,142 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:06:33,142 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:06:33,142 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:06:33,148 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:06:33,148 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:06:33,148 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:06:33,148 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:06:33,148 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:06:33,149 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:06:33,702 - src.llm_wrapper - INFO - Model ready in 0.55s
2025-09-05 04:06:33,702 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:06:33,702 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:06:33,702 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:06:33,702 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:06:33,702 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:06:33,702 - src.rag_pipeline - INFO - Processing validated query: 'What is natural language processing?'
2025-09-05 04:06:33,711 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is natural language processing?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.17it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.15it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.12it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.12it/s, batch=1/1, memory=N/A]
2025-09-05 04:06:34,337 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:06:34,339 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:06:34,757 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.046s
2025-09-05 04:06:34,765 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:06:34,766 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1749 tokens) in 0.007s
2025-09-05 04:06:42,394 - src.rag_pipeline - INFO - Generated response (55 tokens) in 7.623s
2025-09-05 04:06:42,608 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:06:42,667 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:06:42,676 - src.experiment_runner - INFO - Completed run 306/510: 9.25s
2025-09-05 04:06:42,676 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:06:42,676 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:06:42,676 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:06:42,676 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:06:42,676 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:06:42,677 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:06:42,677 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:06:42,683 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:06:42,683 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:06:42,683 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:06:42,683 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:06:42,683 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:06:42,683 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:06:43,304 - src.llm_wrapper - INFO - Model ready in 0.62s
2025-09-05 04:06:43,304 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:06:43,304 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:06:43,304 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:06:43,304 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:06:43,304 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:06:43,304 - src.rag_pipeline - INFO - Processing validated query: 'Explain computer vision techniques.'
2025-09-05 04:06:43,314 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain computer vision techniques....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.45it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.44it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.38it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.38it/s, batch=1/1, memory=N/A]
2025-09-05 04:06:43,892 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:06:43,895 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:06:44,302 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.987s
2025-09-05 04:06:44,310 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:06:44,310 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1297 tokens) in 0.007s
2025-09-05 04:07:04,188 - src.rag_pipeline - INFO - Generated response (318 tokens) in 19.872s
2025-09-05 04:07:04,371 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:07:04,427 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:07:04,435 - src.experiment_runner - INFO - Completed run 307/510: 21.51s
2025-09-05 04:07:04,435 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:07:04,435 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:07:04,435 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:07:04,435 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:07:04,435 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:07:04,435 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:07:04,435 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:07:04,441 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:07:04,441 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:07:04,441 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:07:04,441 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:07:04,441 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:07:04,441 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:07:04,974 - src.llm_wrapper - INFO - Model ready in 0.53s
2025-09-05 04:07:04,974 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:07:04,974 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:07:04,974 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:07:04,974 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:07:04,974 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:07:04,974 - src.rag_pipeline - INFO - Processing validated query: 'What is reinforcement learning?'
2025-09-05 04:07:04,981 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is reinforcement learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.96it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.95it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.92it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.92it/s, batch=1/1, memory=N/A]
2025-09-05 04:07:05,612 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:07:05,613 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:07:06,042 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.059s
2025-09-05 04:07:06,052 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:07:06,053 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1236 tokens) in 0.007s
2025-09-05 04:07:18,752 - src.rag_pipeline - INFO - Generated response (180 tokens) in 12.692s
2025-09-05 04:07:18,962 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:07:19,023 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:07:19,030 - src.experiment_runner - INFO - Completed run 308/510: 14.32s
2025-09-05 04:07:19,030 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:07:19,030 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:07:19,030 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:07:19,030 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:07:19,030 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:07:19,030 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:07:19,030 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:07:19,035 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:07:19,036 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:07:19,036 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:07:19,036 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:07:19,036 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:07:19,036 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:07:19,656 - src.llm_wrapper - INFO - Model ready in 0.62s
2025-09-05 04:07:19,656 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:07:19,656 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:07:19,656 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:07:19,656 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:07:19,656 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:07:19,656 - src.rag_pipeline - INFO - Processing validated query: 'How does data preprocessing work?'
2025-09-05 04:07:19,665 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does data preprocessing work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.58it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.54it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.48it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.48it/s, batch=1/1, memory=N/A]
2025-09-05 04:07:20,443 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:07:20,446 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:07:20,880 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.214s
2025-09-05 04:07:20,893 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:07:20,894 - src.rag_pipeline - INFO - Built prompt with 5 contexts (909 tokens) in 0.012s
2025-09-05 04:07:30,363 - src.rag_pipeline - INFO - Generated response (139 tokens) in 9.463s
2025-09-05 04:07:30,565 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:07:30,619 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:07:30,629 - src.experiment_runner - INFO - Completed run 309/510: 11.33s
2025-09-05 04:07:30,629 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:07:30,629 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:07:30,629 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:07:30,629 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:07:30,629 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:07:30,629 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:07:30,629 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:07:30,635 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:07:30,635 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:07:30,635 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:07:30,635 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:07:30,635 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:07:30,636 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:07:31,182 - src.llm_wrapper - INFO - Model ready in 0.55s
2025-09-05 04:07:31,182 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:07:31,182 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:07:31,182 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:07:31,182 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:07:31,182 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:07:31,182 - src.rag_pipeline - INFO - Processing validated query: 'What are the challenges in AI development?'
2025-09-05 04:07:31,191 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are the challenges in AI development?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.96it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.96it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.94it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.94it/s, batch=1/1, memory=N/A]
2025-09-05 04:07:31,822 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:07:31,827 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:07:32,251 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.059s
2025-09-05 04:07:32,260 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:07:32,261 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1445 tokens) in 0.008s
2025-09-05 04:07:48,801 - src.rag_pipeline - INFO - Generated response (237 tokens) in 16.534s
2025-09-05 04:07:49,054 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:07:49,119 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:07:49,127 - src.experiment_runner - INFO - Completed run 310/510: 18.17s
2025-09-05 04:07:49,127 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:07:49,127 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:07:49,127 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:07:49,127 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:07:49,128 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:07:49,128 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:07:49,128 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:07:49,133 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:07:49,134 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:07:49,134 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:07:49,134 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:07:49,134 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:07:49,134 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:07:49,727 - src.llm_wrapper - INFO - Model ready in 0.59s
2025-09-05 04:07:49,727 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:07:49,727 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:07:49,727 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:07:49,727 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:07:49,727 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:07:49,727 - src.rag_pipeline - INFO - Processing validated query: 'What is machine learning?'
2025-09-05 04:07:49,736 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is machine learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.33it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.33it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.30it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.30it/s, batch=1/1, memory=N/A]
2025-09-05 04:07:50,681 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:07:50,684 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:07:51,220 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.483s
2025-09-05 04:07:51,231 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:07:51,231 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1162 tokens) in 0.010s
2025-09-05 04:07:57,280 - src.rag_pipeline - INFO - Generated response (59 tokens) in 6.047s
2025-09-05 04:07:57,502 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:07:57,561 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:07:57,567 - src.experiment_runner - INFO - Completed run 311/510: 8.15s
2025-09-05 04:07:57,567 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:07:57,567 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:07:57,567 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:07:57,567 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:07:57,567 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:07:57,567 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:07:57,567 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:07:57,573 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:07:57,573 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:07:57,573 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:07:57,573 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:07:57,573 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:07:57,573 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:07:58,143 - src.llm_wrapper - INFO - Model ready in 0.57s
2025-09-05 04:07:58,143 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:07:58,143 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:07:58,143 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:07:58,143 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:07:58,143 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:07:58,143 - src.rag_pipeline - INFO - Processing validated query: 'How does artificial intelligence work?'
2025-09-05 04:07:58,151 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does artificial intelligence work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.07it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.05it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.98it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.98it/s, batch=1/1, memory=N/A]
2025-09-05 04:07:58,787 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:07:58,788 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:07:59,195 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.044s
2025-09-05 04:07:59,204 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:07:59,205 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1211 tokens) in 0.008s
2025-09-05 04:08:17,288 - src.rag_pipeline - INFO - Generated response (287 tokens) in 18.077s
2025-09-05 04:08:17,514 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:08:17,581 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:08:17,591 - src.experiment_runner - INFO - Completed run 312/510: 19.72s
2025-09-05 04:08:17,591 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:08:17,591 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:08:17,591 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:08:17,591 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:08:17,591 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:08:17,591 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:08:17,591 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:08:17,598 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:08:17,598 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:08:17,598 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:08:17,598 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:08:17,598 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:08:17,598 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:08:18,155 - src.llm_wrapper - INFO - Model ready in 0.56s
2025-09-05 04:08:18,155 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:08:18,155 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:08:18,155 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:08:18,155 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:08:18,155 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:08:18,155 - src.rag_pipeline - INFO - Processing validated query: 'Explain deep learning algorithms.'
2025-09-05 04:08:18,163 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain deep learning algorithms....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.11it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.10it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.07it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.07it/s, batch=1/1, memory=N/A]
2025-09-05 04:08:18,994 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:08:18,999 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:08:19,477 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.313s
2025-09-05 04:08:19,488 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:08:19,489 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1198 tokens) in 0.010s
2025-09-05 04:08:25,068 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.575s
2025-09-05 04:08:25,275 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:08:25,332 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:08:25,340 - src.experiment_runner - INFO - Completed run 313/510: 7.48s
2025-09-05 04:08:25,340 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:08:25,340 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:08:25,340 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:08:25,340 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:08:25,340 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:08:25,340 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:08:25,340 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:08:25,346 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:08:25,346 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:08:25,346 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:08:25,346 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:08:25,346 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:08:25,346 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:08:25,891 - src.llm_wrapper - INFO - Model ready in 0.54s
2025-09-05 04:08:25,891 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:08:25,891 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:08:25,891 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:08:25,891 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:08:25,891 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:08:25,891 - src.rag_pipeline - INFO - Processing validated query: 'What are neural networks?'
2025-09-05 04:08:25,899 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are neural networks?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.75it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.74it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.72it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.72it/s, batch=1/1, memory=N/A]
2025-09-05 04:08:26,805 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:08:26,808 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:08:27,248 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.348s
2025-09-05 04:08:27,258 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:08:27,259 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1353 tokens) in 0.009s
2025-09-05 04:08:43,002 - src.rag_pipeline - INFO - Generated response (238 tokens) in 15.738s
2025-09-05 04:08:43,202 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:08:43,271 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:08:43,280 - src.experiment_runner - INFO - Completed run 314/510: 17.66s
2025-09-05 04:08:43,280 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:08:43,280 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:08:43,280 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:08:43,280 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:08:43,281 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:08:43,281 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:08:43,281 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:08:43,287 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:08:43,287 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:08:43,287 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:08:43,287 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:08:43,287 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:08:43,287 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:08:43,823 - src.llm_wrapper - INFO - Model ready in 0.54s
2025-09-05 04:08:43,823 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:08:43,823 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:08:43,823 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:08:43,823 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:08:43,823 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:08:43,823 - src.rag_pipeline - INFO - Processing validated query: 'How do large language models work?'
2025-09-05 04:08:43,833 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How do large language models work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.03it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.01it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.99it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.99it/s, batch=1/1, memory=N/A]
2025-09-05 04:08:44,437 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:08:44,439 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:08:44,849 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.016s
2025-09-05 04:08:44,851 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:08:44,852 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1337 tokens) in 0.002s
2025-09-05 04:08:50,763 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.901s
2025-09-05 04:08:50,980 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:08:51,044 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:08:51,051 - src.experiment_runner - INFO - Completed run 315/510: 7.48s
2025-09-05 04:08:51,051 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:08:51,051 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:08:51,051 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:08:51,052 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:08:51,052 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:08:51,052 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:08:51,052 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:08:51,058 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:08:51,058 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:08:51,058 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:08:51,058 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:08:51,058 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:08:51,058 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:08:51,613 - src.llm_wrapper - INFO - Model ready in 0.56s
2025-09-05 04:08:51,613 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:08:51,613 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:08:51,613 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:08:51,614 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:08:51,614 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:08:51,614 - src.rag_pipeline - INFO - Processing validated query: 'What is natural language processing?'
2025-09-05 04:08:51,621 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is natural language processing?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.89it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.88it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.85it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.84it/s, batch=1/1, memory=N/A]
2025-09-05 04:08:52,412 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:08:52,417 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:08:52,911 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.290s
2025-09-05 04:08:52,918 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:08:52,918 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1749 tokens) in 0.006s
2025-09-05 04:09:00,414 - src.rag_pipeline - INFO - Generated response (55 tokens) in 7.491s
2025-09-05 04:09:00,610 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:09:00,666 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:09:00,674 - src.experiment_runner - INFO - Completed run 316/510: 9.36s
2025-09-05 04:09:00,675 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:09:00,675 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:09:00,675 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:09:00,675 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:09:00,675 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:09:00,675 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:09:00,675 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:09:00,681 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:09:00,682 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:09:00,682 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:09:00,682 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:09:00,682 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:09:00,682 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:09:01,223 - src.llm_wrapper - INFO - Model ready in 0.54s
2025-09-05 04:09:01,223 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:09:01,223 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:09:01,223 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:09:01,223 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:09:01,223 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:09:01,223 - src.rag_pipeline - INFO - Processing validated query: 'Explain computer vision techniques.'
2025-09-05 04:09:01,233 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain computer vision techniques....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.92it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.92it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.88it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.88it/s, batch=1/1, memory=N/A]
2025-09-05 04:09:01,867 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:09:01,869 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:09:02,315 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.082s
2025-09-05 04:09:02,325 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:09:02,326 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1297 tokens) in 0.009s
2025-09-05 04:09:22,224 - src.rag_pipeline - INFO - Generated response (318 tokens) in 19.891s
2025-09-05 04:09:22,447 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:09:22,506 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:09:22,513 - src.experiment_runner - INFO - Completed run 317/510: 21.55s
2025-09-05 04:09:22,513 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:09:22,513 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:09:22,513 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:09:22,513 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:09:22,513 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:09:22,513 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:09:22,513 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:09:22,520 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:09:22,520 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:09:22,520 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:09:22,520 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:09:22,520 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:09:22,520 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:09:23,103 - src.llm_wrapper - INFO - Model ready in 0.58s
2025-09-05 04:09:23,103 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:09:23,103 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:09:23,103 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:09:23,103 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:09:23,103 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:09:23,104 - src.rag_pipeline - INFO - Processing validated query: 'What is reinforcement learning?'
2025-09-05 04:09:23,111 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is reinforcement learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.15it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.13it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.10it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.10it/s, batch=1/1, memory=N/A]
2025-09-05 04:09:23,742 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:09:23,744 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:09:24,170 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.058s
2025-09-05 04:09:24,181 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:09:24,181 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1236 tokens) in 0.009s
2025-09-05 04:09:36,690 - src.rag_pipeline - INFO - Generated response (180 tokens) in 12.503s
2025-09-05 04:09:36,922 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:09:36,979 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:09:36,987 - src.experiment_runner - INFO - Completed run 318/510: 14.18s
2025-09-05 04:09:36,987 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:09:36,987 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:09:36,987 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:09:36,987 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:09:36,987 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:09:36,987 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:09:36,987 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:09:36,993 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:09:36,993 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:09:36,993 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:09:36,993 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:09:36,993 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:09:36,993 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:09:37,537 - src.llm_wrapper - INFO - Model ready in 0.54s
2025-09-05 04:09:37,537 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:09:37,538 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:09:37,538 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:09:37,538 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:09:37,538 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:09:37,538 - src.rag_pipeline - INFO - Processing validated query: 'How does data preprocessing work?'
2025-09-05 04:09:37,548 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does data preprocessing work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.63it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.63it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.60it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.59it/s, batch=1/1, memory=N/A]
2025-09-05 04:09:38,276 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:09:38,279 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:09:38,671 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.122s
2025-09-05 04:09:38,677 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:09:38,678 - src.rag_pipeline - INFO - Built prompt with 5 contexts (909 tokens) in 0.006s
2025-09-05 04:09:48,067 - src.rag_pipeline - INFO - Generated response (139 tokens) in 9.383s
2025-09-05 04:09:48,292 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:09:48,353 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:09:48,361 - src.experiment_runner - INFO - Completed run 319/510: 11.08s
2025-09-05 04:09:48,362 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:09:48,362 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:09:48,362 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:09:48,362 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:09:48,362 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:09:48,362 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:09:48,362 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:09:48,367 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:09:48,368 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:09:48,368 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:09:48,368 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:09:48,368 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:09:48,368 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:09:48,925 - src.llm_wrapper - INFO - Model ready in 0.56s
2025-09-05 04:09:48,925 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:09:48,925 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:09:48,925 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:09:48,925 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:09:48,925 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:09:48,925 - src.rag_pipeline - INFO - Processing validated query: 'What are the challenges in AI development?'
2025-09-05 04:09:48,936 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are the challenges in AI development?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.11it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.10it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.07it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.07it/s, batch=1/1, memory=N/A]
2025-09-05 04:09:49,596 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:09:49,601 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:09:50,067 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.129s
2025-09-05 04:09:50,079 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:09:50,080 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1445 tokens) in 0.011s
2025-09-05 04:10:06,236 - src.rag_pipeline - INFO - Generated response (237 tokens) in 16.140s
2025-09-05 04:10:06,491 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:10:06,549 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:10:06,555 - src.experiment_runner - INFO - Completed run 320/510: 17.88s
2025-09-05 04:10:06,555 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:10:06,555 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:10:06,555 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:10:06,555 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:10:06,555 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:10:06,555 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:10:06,555 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:10:06,561 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:10:06,561 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:10:06,562 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:10:06,562 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:10:06,562 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:10:06,562 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:10:07,094 - src.llm_wrapper - INFO - Model ready in 0.53s
2025-09-05 04:10:07,094 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:10:07,094 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:10:07,094 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:10:07,094 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:10:07,094 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:10:07,094 - src.rag_pipeline - INFO - Processing validated query: 'What is machine learning?'
2025-09-05 04:10:07,102 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is machine learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.76it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.75it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.72it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.72it/s, batch=1/1, memory=N/A]
2025-09-05 04:10:07,832 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:10:07,835 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:10:08,249 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.146s
2025-09-05 04:10:08,256 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:10:08,258 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1162 tokens) in 0.007s
2025-09-05 04:10:14,258 - src.rag_pipeline - INFO - Generated response (59 tokens) in 5.995s
2025-09-05 04:10:14,487 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:10:14,546 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:10:14,556 - src.experiment_runner - INFO - Completed run 321/510: 7.70s
2025-09-05 04:10:14,556 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:10:14,556 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:10:14,556 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:10:14,556 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:10:14,556 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:10:14,556 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:10:14,556 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:10:14,562 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:10:14,562 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:10:14,562 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:10:14,562 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:10:14,562 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:10:14,563 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:10:15,102 - src.llm_wrapper - INFO - Model ready in 0.54s
2025-09-05 04:10:15,102 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:10:15,102 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:10:15,102 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:10:15,102 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:10:15,102 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:10:15,102 - src.rag_pipeline - INFO - Processing validated query: 'How does artificial intelligence work?'
2025-09-05 04:10:15,114 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does artificial intelligence work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.18it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.18it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.16it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.16it/s, batch=1/1, memory=N/A]
2025-09-05 04:10:15,750 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:10:15,753 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:10:16,200 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.085s
2025-09-05 04:10:16,209 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:10:16,210 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1211 tokens) in 0.008s
2025-09-05 04:10:34,348 - src.rag_pipeline - INFO - Generated response (287 tokens) in 18.131s
2025-09-05 04:10:34,591 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:10:34,653 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:10:34,661 - src.experiment_runner - INFO - Completed run 322/510: 19.79s
2025-09-05 04:10:34,661 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:10:34,661 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:10:34,661 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:10:34,661 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:10:34,661 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:10:34,661 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:10:34,661 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:10:34,667 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:10:34,667 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:10:34,667 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:10:34,667 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:10:34,667 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:10:34,667 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:10:35,267 - src.llm_wrapper - INFO - Model ready in 0.60s
2025-09-05 04:10:35,267 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:10:35,268 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:10:35,268 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:10:35,268 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:10:35,268 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:10:35,268 - src.rag_pipeline - INFO - Processing validated query: 'Explain deep learning algorithms.'
2025-09-05 04:10:35,277 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain deep learning algorithms....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.66it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.65it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.62it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.62it/s, batch=1/1, memory=N/A]
2025-09-05 04:10:36,067 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:10:36,071 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:10:36,545 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.267s
2025-09-05 04:10:36,557 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:10:36,557 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1198 tokens) in 0.010s
2025-09-05 04:10:42,087 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.527s
2025-09-05 04:10:42,275 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:10:42,341 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:10:42,349 - src.experiment_runner - INFO - Completed run 323/510: 7.43s
2025-09-05 04:10:42,349 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:10:42,349 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:10:42,349 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:10:42,349 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:10:42,349 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:10:42,349 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:10:42,349 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:10:42,356 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:10:42,356 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:10:42,356 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:10:42,356 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:10:42,356 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:10:42,357 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:10:42,931 - src.llm_wrapper - INFO - Model ready in 0.57s
2025-09-05 04:10:42,931 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:10:42,931 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:10:42,932 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:10:42,932 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:10:42,932 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:10:42,932 - src.rag_pipeline - INFO - Processing validated query: 'What are neural networks?'
2025-09-05 04:10:42,939 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are neural networks?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.05it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.04it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.02it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.02it/s, batch=1/1, memory=N/A]
2025-09-05 04:10:43,587 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:10:43,590 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:10:44,033 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.094s
2025-09-05 04:10:44,043 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:10:44,044 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1353 tokens) in 0.010s
2025-09-05 04:11:00,038 - src.rag_pipeline - INFO - Generated response (238 tokens) in 15.988s
2025-09-05 04:11:00,254 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:11:00,308 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:11:00,317 - src.experiment_runner - INFO - Completed run 324/510: 17.69s
2025-09-05 04:11:00,317 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:11:00,317 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:11:00,317 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:11:00,317 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:11:00,317 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:11:00,317 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:11:00,317 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:11:00,323 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:11:00,323 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:11:00,323 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:11:00,323 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:11:00,323 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:11:00,323 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:11:00,894 - src.llm_wrapper - INFO - Model ready in 0.57s
2025-09-05 04:11:00,894 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:11:00,895 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:11:00,895 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:11:00,895 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:11:00,895 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:11:00,895 - src.rag_pipeline - INFO - Processing validated query: 'How do large language models work?'
2025-09-05 04:11:00,904 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How do large language models work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.96it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.95it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.91it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.91it/s, batch=1/1, memory=N/A]
2025-09-05 04:11:01,563 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:11:01,565 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:11:01,997 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.092s
2025-09-05 04:11:02,007 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:11:02,008 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1337 tokens) in 0.010s
2025-09-05 04:11:07,918 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.906s
2025-09-05 04:11:08,093 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:11:08,155 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:11:08,166 - src.experiment_runner - INFO - Completed run 325/510: 7.60s
2025-09-05 04:11:08,166 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:11:08,166 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:11:08,167 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:11:08,167 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:11:08,167 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:11:08,167 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:11:08,167 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:11:08,174 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:11:08,175 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:11:08,175 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:11:08,175 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:11:08,176 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:11:08,176 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:11:08,693 - src.llm_wrapper - INFO - Model ready in 0.52s
2025-09-05 04:11:08,693 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:11:08,693 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:11:08,693 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:11:08,693 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:11:08,694 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:11:08,694 - src.rag_pipeline - INFO - Processing validated query: 'What is natural language processing?'
2025-09-05 04:11:08,702 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is natural language processing?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.39it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.37it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.34it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.33it/s, batch=1/1, memory=N/A]
2025-09-05 04:11:09,264 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:11:09,265 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:11:09,674 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.972s
2025-09-05 04:11:09,682 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:11:09,683 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1749 tokens) in 0.007s
2025-09-05 04:11:17,134 - src.rag_pipeline - INFO - Generated response (55 tokens) in 7.447s
2025-09-05 04:11:17,354 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:11:17,408 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:11:17,415 - src.experiment_runner - INFO - Completed run 326/510: 8.97s
2025-09-05 04:11:17,415 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:11:17,415 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:11:17,415 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:11:17,415 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:11:17,415 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:11:17,415 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:11:17,415 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:11:17,421 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:11:17,422 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:11:17,422 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:11:17,422 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:11:17,422 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:11:17,422 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:11:17,950 - src.llm_wrapper - INFO - Model ready in 0.53s
2025-09-05 04:11:17,950 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:11:17,950 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:11:17,950 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:11:17,950 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:11:17,950 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:11:17,950 - src.rag_pipeline - INFO - Processing validated query: 'Explain computer vision techniques.'
2025-09-05 04:11:17,958 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain computer vision techniques....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.89it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.86it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.83it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.83it/s, batch=1/1, memory=N/A]
2025-09-05 04:11:18,676 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:11:18,679 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:11:19,106 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.147s
2025-09-05 04:11:19,116 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:11:19,118 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1297 tokens) in 0.009s
2025-09-05 04:11:39,408 - src.rag_pipeline - INFO - Generated response (318 tokens) in 20.283s
2025-09-05 04:11:39,653 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:11:39,713 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:11:39,722 - src.experiment_runner - INFO - Completed run 327/510: 21.99s
2025-09-05 04:11:39,722 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:11:39,722 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:11:39,722 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:11:39,722 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:11:39,722 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:11:39,722 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:11:39,722 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:11:39,727 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:11:39,728 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:11:39,728 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:11:39,728 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:11:39,728 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:11:39,728 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:11:40,317 - src.llm_wrapper - INFO - Model ready in 0.59s
2025-09-05 04:11:40,317 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:11:40,317 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:11:40,317 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:11:40,317 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:11:40,317 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:11:40,317 - src.rag_pipeline - INFO - Processing validated query: 'What is reinforcement learning?'
2025-09-05 04:11:40,326 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is reinforcement learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.78it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.77it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.75it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.75it/s, batch=1/1, memory=N/A]
2025-09-05 04:11:41,092 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:11:41,095 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:11:41,633 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.307s
2025-09-05 04:11:41,641 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:11:41,642 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1236 tokens) in 0.008s
2025-09-05 04:11:54,182 - src.rag_pipeline - INFO - Generated response (180 tokens) in 12.535s
2025-09-05 04:11:54,461 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:11:54,517 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:11:54,525 - src.experiment_runner - INFO - Completed run 328/510: 14.46s
2025-09-05 04:11:54,525 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:11:54,525 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:11:54,525 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:11:54,525 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:11:54,525 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:11:54,525 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:11:54,525 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:11:54,530 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:11:54,531 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:11:54,531 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:11:54,531 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:11:54,531 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:11:54,531 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:11:55,086 - src.llm_wrapper - INFO - Model ready in 0.56s
2025-09-05 04:11:55,086 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:11:55,087 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:11:55,087 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:11:55,087 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:11:55,087 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:11:55,087 - src.rag_pipeline - INFO - Processing validated query: 'How does data preprocessing work?'
2025-09-05 04:11:55,100 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does data preprocessing work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.25it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.23it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.21it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.21it/s, batch=1/1, memory=N/A]
2025-09-05 04:11:55,851 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:11:55,854 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:11:56,292 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.192s
2025-09-05 04:11:56,302 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:11:56,302 - src.rag_pipeline - INFO - Built prompt with 5 contexts (909 tokens) in 0.008s
2025-09-05 04:12:05,930 - src.rag_pipeline - INFO - Generated response (139 tokens) in 9.620s
2025-09-05 04:12:06,147 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:12:06,211 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:12:06,219 - src.experiment_runner - INFO - Completed run 329/510: 11.41s
2025-09-05 04:12:06,219 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:12:06,219 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:12:06,219 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:12:06,219 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:12:06,219 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:12:06,219 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:12:06,219 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:12:06,224 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:12:06,225 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:12:06,225 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:12:06,225 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:12:06,225 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:12:06,225 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:12:06,764 - src.llm_wrapper - INFO - Model ready in 0.54s
2025-09-05 04:12:06,764 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:12:06,764 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:12:06,765 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:12:06,765 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:12:06,765 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:12:06,765 - src.rag_pipeline - INFO - Processing validated query: 'What are the challenges in AI development?'
2025-09-05 04:12:06,772 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are the challenges in AI development?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.79it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.79it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.77it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.77it/s, batch=1/1, memory=N/A]
2025-09-05 04:12:07,457 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:12:07,460 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:12:07,937 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.165s
2025-09-05 04:12:07,948 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:12:07,949 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1445 tokens) in 0.010s
2025-09-05 04:12:24,012 - src.rag_pipeline - INFO - Generated response (237 tokens) in 16.056s
2025-09-05 04:12:24,233 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:12:24,293 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:12:24,302 - src.experiment_runner - INFO - Completed run 330/510: 17.79s
2025-09-05 04:12:24,302 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:12:24,302 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:12:24,302 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:12:24,302 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:12:24,302 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:12:24,302 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:12:24,302 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:12:24,308 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:12:24,308 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:12:24,308 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:12:24,308 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:12:24,308 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:12:24,308 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:12:24,892 - src.llm_wrapper - INFO - Model ready in 0.58s
2025-09-05 04:12:24,893 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:12:24,893 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:12:24,893 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:12:24,893 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:12:24,893 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:12:24,893 - src.rag_pipeline - INFO - Processing validated query: 'What is machine learning?'
2025-09-05 04:12:24,901 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is machine learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.68it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.68it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.65it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.65it/s, batch=1/1, memory=N/A]
2025-09-05 04:12:25,624 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:12:25,627 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:12:26,118 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.217s
2025-09-05 04:12:26,132 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:12:26,133 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1162 tokens) in 0.012s
2025-09-05 04:12:32,150 - src.rag_pipeline - INFO - Generated response (59 tokens) in 6.014s
2025-09-05 04:12:32,409 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:12:32,473 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:12:32,481 - src.experiment_runner - INFO - Completed run 331/510: 7.85s
2025-09-05 04:12:32,481 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:12:32,481 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:12:32,481 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:12:32,481 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:12:32,481 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:12:32,481 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:12:32,481 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:12:32,487 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:12:32,487 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:12:32,487 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:12:32,487 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:12:32,487 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:12:32,488 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:12:33,097 - src.llm_wrapper - INFO - Model ready in 0.61s
2025-09-05 04:12:33,097 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:12:33,097 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:12:33,097 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:12:33,097 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:12:33,097 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:12:33,097 - src.rag_pipeline - INFO - Processing validated query: 'How does artificial intelligence work?'
2025-09-05 04:12:33,104 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does artificial intelligence work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.92it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.92it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.88it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.88it/s, batch=1/1, memory=N/A]
2025-09-05 04:12:33,886 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:12:33,888 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:12:34,333 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.228s
2025-09-05 04:12:34,343 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:12:34,343 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1211 tokens) in 0.009s
2025-09-05 04:12:52,424 - src.rag_pipeline - INFO - Generated response (287 tokens) in 18.069s
2025-09-05 04:12:52,830 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:12:52,899 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:12:52,908 - src.experiment_runner - INFO - Completed run 332/510: 19.94s
2025-09-05 04:12:52,909 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:12:52,909 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:12:52,909 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:12:52,909 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:12:52,909 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:12:52,909 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:12:52,909 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:12:52,915 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:12:52,916 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:12:52,916 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:12:52,916 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:12:52,916 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:12:52,916 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:12:53,668 - src.llm_wrapper - INFO - Model ready in 0.75s
2025-09-05 04:12:53,669 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:12:53,669 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:12:53,669 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:12:53,669 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:12:53,670 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:12:53,670 - src.rag_pipeline - INFO - Processing validated query: 'Explain deep learning algorithms.'
2025-09-05 04:12:53,685 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain deep learning algorithms....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.37it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.37it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.34it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.34it/s, batch=1/1, memory=N/A]
2025-09-05 04:12:54,389 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:12:54,392 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:12:54,797 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.111s
2025-09-05 04:12:54,808 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:12:54,808 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1198 tokens) in 0.010s
2025-09-05 04:13:00,409 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.596s
2025-09-05 04:13:00,629 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:13:00,695 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:13:00,703 - src.experiment_runner - INFO - Completed run 333/510: 7.50s
2025-09-05 04:13:00,703 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:13:00,703 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:13:00,704 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:13:00,704 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:13:00,704 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:13:00,704 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:13:00,704 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:13:00,713 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:13:00,714 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:13:00,714 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:13:00,714 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:13:00,714 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:13:00,714 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:13:01,290 - src.llm_wrapper - INFO - Model ready in 0.58s
2025-09-05 04:13:01,290 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:13:01,290 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:13:01,290 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:13:01,290 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:13:01,290 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:13:01,290 - src.rag_pipeline - INFO - Processing validated query: 'What are neural networks?'
2025-09-05 04:13:01,299 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are neural networks?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.18it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.17it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.13it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.13it/s, batch=1/1, memory=N/A]
2025-09-05 04:13:01,936 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:13:01,940 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:13:02,377 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.077s
2025-09-05 04:13:02,386 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:13:02,388 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1353 tokens) in 0.009s
2025-09-05 04:13:18,215 - src.rag_pipeline - INFO - Generated response (238 tokens) in 15.822s
2025-09-05 04:13:18,432 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:13:18,491 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:13:18,499 - src.experiment_runner - INFO - Completed run 334/510: 17.51s
2025-09-05 04:13:18,500 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:13:18,500 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:13:18,500 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:13:18,500 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:13:18,500 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:13:18,500 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:13:18,500 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:13:18,505 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:13:18,505 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:13:18,505 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:13:18,505 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:13:18,506 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:13:18,506 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:13:19,050 - src.llm_wrapper - INFO - Model ready in 0.54s
2025-09-05 04:13:19,050 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:13:19,050 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:13:19,050 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:13:19,050 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:13:19,050 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:13:19,051 - src.rag_pipeline - INFO - Processing validated query: 'How do large language models work?'
2025-09-05 04:13:19,059 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How do large language models work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.15it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.14it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.10it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.10it/s, batch=1/1, memory=N/A]
2025-09-05 04:13:19,697 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:13:19,700 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:13:20,148 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.089s
2025-09-05 04:13:20,152 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:13:20,153 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1337 tokens) in 0.004s
2025-09-05 04:13:26,055 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.901s
2025-09-05 04:13:26,246 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:13:26,302 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:13:26,310 - src.experiment_runner - INFO - Completed run 335/510: 7.56s
2025-09-05 04:13:26,310 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:13:26,310 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:13:26,311 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:13:26,311 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:13:26,311 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:13:26,311 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:13:26,311 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:13:26,317 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:13:26,317 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:13:26,317 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:13:26,317 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:13:26,317 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:13:26,318 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:13:26,863 - src.llm_wrapper - INFO - Model ready in 0.55s
2025-09-05 04:13:26,863 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:13:26,863 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:13:26,863 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:13:26,863 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:13:26,863 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:13:26,863 - src.rag_pipeline - INFO - Processing validated query: 'What is natural language processing?'
2025-09-05 04:13:26,870 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is natural language processing?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.80it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.79it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.74it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.74it/s, batch=1/1, memory=N/A]
2025-09-05 04:13:27,186 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:13:27,187 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:13:27,554 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.684s
2025-09-05 04:13:27,557 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:13:27,557 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1749 tokens) in 0.003s
2025-09-05 04:13:35,081 - src.rag_pipeline - INFO - Generated response (55 tokens) in 7.521s
2025-09-05 04:13:35,250 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:13:35,304 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:13:35,313 - src.experiment_runner - INFO - Completed run 336/510: 8.77s
2025-09-05 04:13:35,313 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:13:35,313 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:13:35,313 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:13:35,313 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:13:35,314 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:13:35,314 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:13:35,314 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:13:35,320 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:13:35,320 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:13:35,320 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:13:35,320 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:13:35,320 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:13:35,320 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:13:35,868 - src.llm_wrapper - INFO - Model ready in 0.55s
2025-09-05 04:13:35,869 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:13:35,869 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:13:35,869 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:13:35,869 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:13:35,869 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:13:35,869 - src.rag_pipeline - INFO - Processing validated query: 'Explain computer vision techniques.'
2025-09-05 04:13:35,876 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain computer vision techniques....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.35it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.35it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.31it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.31it/s, batch=1/1, memory=N/A]
2025-09-05 04:13:36,225 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:13:36,226 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:13:36,613 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.737s
2025-09-05 04:13:36,615 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:13:36,615 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1297 tokens) in 0.002s
2025-09-05 04:13:56,392 - src.rag_pipeline - INFO - Generated response (318 tokens) in 19.769s
2025-09-05 04:13:56,634 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:13:56,689 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:13:56,698 - src.experiment_runner - INFO - Completed run 337/510: 21.08s
2025-09-05 04:13:56,698 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:13:56,698 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:13:56,698 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:13:56,698 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:13:56,698 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:13:56,698 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:13:56,698 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:13:56,705 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:13:56,705 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:13:56,705 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:13:56,705 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:13:56,705 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:13:56,706 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:13:57,255 - src.llm_wrapper - INFO - Model ready in 0.55s
2025-09-05 04:13:57,255 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:13:57,255 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:13:57,255 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:13:57,255 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:13:57,255 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:13:57,255 - src.rag_pipeline - INFO - Processing validated query: 'What is reinforcement learning?'
2025-09-05 04:13:57,262 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is reinforcement learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.28it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.27it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.25it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.25it/s, batch=1/1, memory=N/A]
2025-09-05 04:13:57,729 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:13:57,730 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:13:58,075 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.812s
2025-09-05 04:13:58,079 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:13:58,079 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1236 tokens) in 0.004s
2025-09-05 04:14:10,415 - src.rag_pipeline - INFO - Generated response (180 tokens) in 12.331s
2025-09-05 04:14:10,606 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:14:10,661 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:14:10,668 - src.experiment_runner - INFO - Completed run 338/510: 13.72s
2025-09-05 04:14:10,668 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:14:10,669 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:14:10,669 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:14:10,669 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:14:10,669 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:14:10,669 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:14:10,669 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:14:10,675 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:14:10,675 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:14:10,676 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:14:10,676 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:14:10,676 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:14:10,676 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:14:11,222 - src.llm_wrapper - INFO - Model ready in 0.55s
2025-09-05 04:14:11,222 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:14:11,222 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:14:11,222 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:14:11,222 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:14:11,222 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:14:11,222 - src.rag_pipeline - INFO - Processing validated query: 'How does data preprocessing work?'
2025-09-05 04:14:11,231 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does data preprocessing work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.99it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.96it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.94it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.94it/s, batch=1/1, memory=N/A]
2025-09-05 04:14:11,803 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:14:11,805 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:14:12,179 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.948s
2025-09-05 04:14:12,185 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:14:12,185 - src.rag_pipeline - INFO - Built prompt with 5 contexts (909 tokens) in 0.006s
2025-09-05 04:14:21,587 - src.rag_pipeline - INFO - Generated response (139 tokens) in 9.395s
2025-09-05 04:14:21,778 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:14:21,837 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:14:21,846 - src.experiment_runner - INFO - Completed run 339/510: 10.92s
2025-09-05 04:14:21,846 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:14:21,846 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:14:21,846 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:14:21,846 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:14:21,846 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:14:21,846 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:14:21,846 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:14:21,852 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:14:21,852 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:14:21,852 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:14:21,852 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:14:21,852 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:14:21,852 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:14:22,376 - src.llm_wrapper - INFO - Model ready in 0.52s
2025-09-05 04:14:22,376 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:14:22,376 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:14:22,376 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:14:22,376 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:14:22,376 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:14:22,376 - src.rag_pipeline - INFO - Processing validated query: 'What are the challenges in AI development?'
2025-09-05 04:14:22,383 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are the challenges in AI development?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.54it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.54it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.49it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.48it/s, batch=1/1, memory=N/A]
2025-09-05 04:14:22,797 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:14:22,799 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:14:23,204 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.820s
2025-09-05 04:14:23,208 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:14:23,209 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1445 tokens) in 0.004s
2025-09-05 04:14:39,352 - src.rag_pipeline - INFO - Generated response (237 tokens) in 16.138s
2025-09-05 04:14:39,639 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:14:39,694 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:14:39,703 - src.experiment_runner - INFO - Completed run 340/510: 17.51s
2025-09-05 04:14:39,703 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:14:39,703 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:14:39,703 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:14:39,703 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:14:39,703 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:14:39,703 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:14:39,703 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:14:39,709 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:14:39,710 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:14:39,710 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:14:39,710 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:14:39,710 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:14:39,710 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:14:40,317 - src.llm_wrapper - INFO - Model ready in 0.61s
2025-09-05 04:14:40,317 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:14:40,317 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:14:40,317 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:14:40,317 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:14:40,317 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:14:40,317 - src.rag_pipeline - INFO - Processing validated query: 'What is machine learning?'
2025-09-05 04:14:40,325 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is machine learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.49it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.49it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.48it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.48it/s, batch=1/1, memory=N/A]
2025-09-05 04:14:41,074 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:14:41,076 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:14:41,528 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.203s
2025-09-05 04:14:41,537 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:14:41,538 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1162 tokens) in 0.009s
2025-09-05 04:14:47,519 - src.rag_pipeline - INFO - Generated response (59 tokens) in 5.980s
2025-09-05 04:14:47,698 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:14:47,751 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:14:47,758 - src.experiment_runner - INFO - Completed run 341/510: 7.82s
2025-09-05 04:14:47,759 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:14:47,759 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:14:47,759 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:14:47,759 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:14:47,759 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:14:47,759 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:14:47,759 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:14:47,764 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:14:47,765 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:14:47,765 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:14:47,765 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:14:47,765 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:14:47,765 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:14:48,319 - src.llm_wrapper - INFO - Model ready in 0.55s
2025-09-05 04:14:48,319 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:14:48,319 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:14:48,319 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:14:48,319 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:14:48,319 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:14:48,319 - src.rag_pipeline - INFO - Processing validated query: 'How does artificial intelligence work?'
2025-09-05 04:14:48,326 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does artificial intelligence work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.85it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.84it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.82it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.82it/s, batch=1/1, memory=N/A]
2025-09-05 04:14:48,999 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:14:49,000 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:14:49,432 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.104s
2025-09-05 04:14:49,447 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:14:49,449 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1211 tokens) in 0.013s
2025-09-05 04:15:07,480 - src.rag_pipeline - INFO - Generated response (287 tokens) in 18.021s
2025-09-05 04:15:07,668 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:15:07,719 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:15:07,726 - src.experiment_runner - INFO - Completed run 342/510: 19.72s
2025-09-05 04:15:07,727 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:15:07,727 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:15:07,727 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:15:07,727 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:15:07,727 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:15:07,727 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:15:07,727 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:15:07,732 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:15:07,733 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:15:07,733 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:15:07,733 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:15:07,733 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:15:07,733 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:15:08,297 - src.llm_wrapper - INFO - Model ready in 0.56s
2025-09-05 04:15:08,298 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:15:08,298 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:15:08,298 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:15:08,298 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:15:08,298 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:15:08,298 - src.rag_pipeline - INFO - Processing validated query: 'Explain deep learning algorithms.'
2025-09-05 04:15:08,304 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain deep learning algorithms....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.06it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.05it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.03it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.03it/s, batch=1/1, memory=N/A]
2025-09-05 04:15:08,880 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:15:08,884 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:15:09,272 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.967s
2025-09-05 04:15:09,282 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:15:09,283 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1198 tokens) in 0.009s
2025-09-05 04:15:14,841 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.553s
2025-09-05 04:15:15,084 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:15:15,137 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:15:15,146 - src.experiment_runner - INFO - Completed run 343/510: 7.12s
2025-09-05 04:15:15,146 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:15:15,146 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:15:15,146 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:15:15,146 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:15:15,146 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:15:15,146 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:15:15,146 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:15:15,152 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:15:15,152 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:15:15,152 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:15:15,152 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:15:15,152 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:15:15,152 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:15:15,716 - src.llm_wrapper - INFO - Model ready in 0.56s
2025-09-05 04:15:15,716 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:15:15,716 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:15:15,716 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:15:15,716 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:15:15,716 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:15:15,716 - src.rag_pipeline - INFO - Processing validated query: 'What are neural networks?'
2025-09-05 04:15:15,723 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are neural networks?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.30it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.29it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.24it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.24it/s, batch=1/1, memory=N/A]
2025-09-05 04:15:16,344 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:15:16,346 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:15:16,751 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.027s
2025-09-05 04:15:16,757 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:15:16,758 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1353 tokens) in 0.003s
2025-09-05 04:15:32,535 - src.rag_pipeline - INFO - Generated response (238 tokens) in 15.771s
2025-09-05 04:15:32,715 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:15:32,783 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:15:32,791 - src.experiment_runner - INFO - Completed run 344/510: 17.39s
2025-09-05 04:15:32,791 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:15:32,791 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:15:32,791 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:15:32,792 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:15:32,792 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:15:32,792 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:15:32,792 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:15:32,797 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:15:32,798 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:15:32,798 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:15:32,798 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:15:32,798 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:15:32,798 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:15:33,335 - src.llm_wrapper - INFO - Model ready in 0.54s
2025-09-05 04:15:33,335 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:15:33,335 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:15:33,335 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:15:33,336 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:15:33,336 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:15:33,336 - src.rag_pipeline - INFO - Processing validated query: 'How do large language models work?'
2025-09-05 04:15:33,348 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How do large language models work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.80it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.79it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.77it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.76it/s, batch=1/1, memory=N/A]
2025-09-05 04:15:33,969 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:15:33,971 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:15:34,359 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.011s
2025-09-05 04:15:34,367 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:15:34,368 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1337 tokens) in 0.008s
2025-09-05 04:15:40,158 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.786s
2025-09-05 04:15:40,359 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:15:40,419 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:15:40,428 - src.experiment_runner - INFO - Completed run 345/510: 7.37s
2025-09-05 04:15:40,428 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:15:40,428 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:15:40,428 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:15:40,428 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:15:40,428 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:15:40,428 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:15:40,428 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:15:40,434 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:15:40,434 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:15:40,434 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:15:40,434 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:15:40,435 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:15:40,435 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:15:41,009 - src.llm_wrapper - INFO - Model ready in 0.57s
2025-09-05 04:15:41,009 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:15:41,009 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:15:41,009 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:15:41,009 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:15:41,009 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:15:41,009 - src.rag_pipeline - INFO - Processing validated query: 'What is natural language processing?'
2025-09-05 04:15:41,016 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is natural language processing?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.06it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.05it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.03it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.02it/s, batch=1/1, memory=N/A]
2025-09-05 04:15:41,618 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:15:41,621 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:15:42,039 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.022s
2025-09-05 04:15:42,047 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:15:42,049 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1749 tokens) in 0.007s
2025-09-05 04:15:49,574 - src.rag_pipeline - INFO - Generated response (55 tokens) in 7.520s
2025-09-05 04:15:49,808 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:15:49,867 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:15:49,874 - src.experiment_runner - INFO - Completed run 346/510: 9.15s
2025-09-05 04:15:49,874 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:15:49,874 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:15:49,874 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:15:49,874 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:15:49,874 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:15:49,874 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:15:49,874 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:15:49,880 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:15:49,880 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:15:49,880 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:15:49,880 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:15:49,881 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:15:49,881 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:15:50,438 - src.llm_wrapper - INFO - Model ready in 0.56s
2025-09-05 04:15:50,438 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:15:50,438 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:15:50,438 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:15:50,438 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:15:50,438 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:15:50,438 - src.rag_pipeline - INFO - Processing validated query: 'Explain computer vision techniques.'
2025-09-05 04:15:50,449 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain computer vision techniques....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.35it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.33it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.29it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.29it/s, batch=1/1, memory=N/A]
2025-09-05 04:15:51,025 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:15:51,028 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:15:51,432 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.983s
2025-09-05 04:15:51,445 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:15:51,446 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1297 tokens) in 0.013s
2025-09-05 04:16:11,250 - src.rag_pipeline - INFO - Generated response (318 tokens) in 19.797s
2025-09-05 04:16:11,472 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:16:11,527 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:16:11,536 - src.experiment_runner - INFO - Completed run 347/510: 21.38s
2025-09-05 04:16:11,536 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:16:11,536 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:16:11,536 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:16:11,536 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:16:11,536 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:16:11,536 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:16:11,536 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:16:11,542 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:16:11,542 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:16:11,542 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:16:11,542 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:16:11,542 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:16:11,542 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:16:12,093 - src.llm_wrapper - INFO - Model ready in 0.55s
2025-09-05 04:16:12,093 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:16:12,093 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:16:12,093 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:16:12,093 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:16:12,093 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:16:12,093 - src.rag_pipeline - INFO - Processing validated query: 'What is reinforcement learning?'
2025-09-05 04:16:12,100 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is reinforcement learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.02it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.01it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.99it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.99it/s, batch=1/1, memory=N/A]
2025-09-05 04:16:12,704 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:16:12,705 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:16:13,125 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.024s
2025-09-05 04:16:13,135 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:16:13,136 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1236 tokens) in 0.009s
2025-09-05 04:16:25,474 - src.rag_pipeline - INFO - Generated response (180 tokens) in 12.331s
2025-09-05 04:16:25,725 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:16:25,778 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:16:25,787 - src.experiment_runner - INFO - Completed run 348/510: 13.94s
2025-09-05 04:16:25,788 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:16:25,788 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:16:25,788 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:16:25,788 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:16:25,788 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:16:25,788 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:16:25,788 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:16:25,795 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:16:25,796 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:16:25,796 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:16:25,796 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:16:25,796 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:16:25,796 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:16:26,335 - src.llm_wrapper - INFO - Model ready in 0.54s
2025-09-05 04:16:26,335 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:16:26,335 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:16:26,335 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:16:26,335 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:16:26,335 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:16:26,335 - src.rag_pipeline - INFO - Processing validated query: 'How does data preprocessing work?'
2025-09-05 04:16:26,343 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does data preprocessing work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.08it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.07it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.04it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.04it/s, batch=1/1, memory=N/A]
2025-09-05 04:16:26,920 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:16:26,921 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:16:27,373 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.030s
2025-09-05 04:16:27,378 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:16:27,379 - src.rag_pipeline - INFO - Built prompt with 5 contexts (909 tokens) in 0.004s
2025-09-05 04:16:36,872 - src.rag_pipeline - INFO - Generated response (139 tokens) in 9.485s
2025-09-05 04:16:37,112 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:16:37,175 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:16:37,185 - src.experiment_runner - INFO - Completed run 349/510: 11.09s
2025-09-05 04:16:37,185 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:16:37,185 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:16:37,185 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:16:37,185 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:16:37,185 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:16:37,185 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:16:37,185 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:16:37,190 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:16:37,191 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:16:37,191 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:16:37,191 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:16:37,191 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:16:37,191 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:16:37,792 - src.llm_wrapper - INFO - Model ready in 0.60s
2025-09-05 04:16:37,792 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:16:37,792 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:16:37,792 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:16:37,792 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:16:37,792 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:16:37,792 - src.rag_pipeline - INFO - Processing validated query: 'What are the challenges in AI development?'
2025-09-05 04:16:37,799 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are the challenges in AI development?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.82it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.81it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.79it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.79it/s, batch=1/1, memory=N/A]
2025-09-05 04:16:38,429 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:16:38,430 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:16:38,827 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.028s
2025-09-05 04:16:38,839 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:16:38,840 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1445 tokens) in 0.009s
2025-09-05 04:16:55,190 - src.rag_pipeline - INFO - Generated response (237 tokens) in 16.343s
2025-09-05 04:16:55,388 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:16:55,447 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:16:55,456 - src.experiment_runner - INFO - Completed run 350/510: 18.01s
2025-09-05 04:16:55,456 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:16:55,456 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:16:55,456 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:16:55,456 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:16:55,456 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:16:55,456 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:16:55,456 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:16:55,462 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:16:55,462 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:16:55,462 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:16:55,462 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:16:55,462 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:16:55,462 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:16:56,005 - src.llm_wrapper - INFO - Model ready in 0.54s
2025-09-05 04:16:56,005 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:16:56,005 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:16:56,005 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:16:56,005 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:16:56,005 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:16:56,005 - src.rag_pipeline - INFO - Processing validated query: 'What is machine learning?'
2025-09-05 04:16:56,013 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is machine learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.94it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.93it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.91it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.91it/s, batch=1/1, memory=N/A]
2025-09-05 04:16:56,716 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:16:56,718 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:16:57,130 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.116s
2025-09-05 04:16:57,143 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:16:57,144 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1162 tokens) in 0.013s
2025-09-05 04:17:03,201 - src.rag_pipeline - INFO - Generated response (59 tokens) in 6.052s
2025-09-05 04:17:03,384 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:17:03,444 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:17:03,452 - src.experiment_runner - INFO - Completed run 351/510: 7.75s
2025-09-05 04:17:03,452 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:17:03,452 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:17:03,452 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:17:03,452 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:17:03,452 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:17:03,452 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:17:03,452 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:17:03,458 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:17:03,458 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:17:03,458 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:17:03,458 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:17:03,459 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:17:03,459 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:17:03,986 - src.llm_wrapper - INFO - Model ready in 0.53s
2025-09-05 04:17:03,986 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:17:03,986 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:17:03,986 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:17:03,986 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:17:03,986 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:17:03,986 - src.rag_pipeline - INFO - Processing validated query: 'How does artificial intelligence work?'
2025-09-05 04:17:03,993 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does artificial intelligence work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.09it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.08it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.05it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.05it/s, batch=1/1, memory=N/A]
2025-09-05 04:17:04,595 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:17:04,598 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:17:05,041 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.047s
2025-09-05 04:17:05,050 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:17:05,052 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1211 tokens) in 0.007s
2025-09-05 04:17:23,098 - src.rag_pipeline - INFO - Generated response (287 tokens) in 18.038s
2025-09-05 04:17:23,296 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:17:23,358 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:17:23,367 - src.experiment_runner - INFO - Completed run 352/510: 19.65s
2025-09-05 04:17:23,367 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:17:23,367 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:17:23,367 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:17:23,367 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:17:23,367 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:17:23,367 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:17:23,367 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:17:23,373 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:17:23,373 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:17:23,373 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:17:23,373 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:17:23,373 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:17:23,373 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:17:23,897 - src.llm_wrapper - INFO - Model ready in 0.52s
2025-09-05 04:17:23,897 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:17:23,897 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:17:23,897 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:17:23,897 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:17:23,897 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:17:23,897 - src.rag_pipeline - INFO - Processing validated query: 'Explain deep learning algorithms.'
2025-09-05 04:17:23,907 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain deep learning algorithms....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.94it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.93it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.88it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.88it/s, batch=1/1, memory=N/A]
2025-09-05 04:17:24,534 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:17:24,538 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:17:24,951 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.043s
2025-09-05 04:17:24,956 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:17:24,956 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1198 tokens) in 0.005s
2025-09-05 04:17:30,441 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.477s
2025-09-05 04:17:30,653 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:17:30,707 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:17:30,714 - src.experiment_runner - INFO - Completed run 353/510: 7.08s
2025-09-05 04:17:30,714 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:17:30,715 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:17:30,715 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:17:30,715 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:17:30,715 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:17:30,715 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:17:30,715 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:17:30,722 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:17:30,722 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:17:30,722 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:17:30,722 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:17:30,722 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:17:30,722 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:17:31,291 - src.llm_wrapper - INFO - Model ready in 0.57s
2025-09-05 04:17:31,291 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:17:31,291 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:17:31,291 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:17:31,291 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:17:31,291 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:17:31,291 - src.rag_pipeline - INFO - Processing validated query: 'What are neural networks?'
2025-09-05 04:17:31,299 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are neural networks?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.60it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.59it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.57it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.57it/s, batch=1/1, memory=N/A]
2025-09-05 04:17:32,014 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:17:32,016 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:17:32,429 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.129s
2025-09-05 04:17:32,438 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:17:32,439 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1353 tokens) in 0.008s
2025-09-05 04:17:48,230 - src.rag_pipeline - INFO - Generated response (238 tokens) in 15.784s
2025-09-05 04:17:48,446 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:17:48,502 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:17:48,511 - src.experiment_runner - INFO - Completed run 354/510: 17.52s
2025-09-05 04:17:48,511 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:17:48,511 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:17:48,511 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:17:48,511 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:17:48,511 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:17:48,511 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:17:48,511 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:17:48,517 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:17:48,518 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:17:48,518 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:17:48,518 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:17:48,518 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:17:48,518 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:17:49,094 - src.llm_wrapper - INFO - Model ready in 0.58s
2025-09-05 04:17:49,094 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:17:49,094 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:17:49,094 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:17:49,094 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:17:49,094 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:17:49,094 - src.rag_pipeline - INFO - Processing validated query: 'How do large language models work?'
2025-09-05 04:17:49,101 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How do large language models work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.08it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.07it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.05it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.05it/s, batch=1/1, memory=N/A]
2025-09-05 04:17:49,690 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:17:49,695 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:17:50,127 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.025s
2025-09-05 04:17:50,133 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:17:50,134 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1337 tokens) in 0.005s
2025-09-05 04:17:55,906 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.768s
2025-09-05 04:17:56,097 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:17:56,156 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:17:56,163 - src.experiment_runner - INFO - Completed run 355/510: 7.40s
2025-09-05 04:17:56,163 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:17:56,163 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:17:56,163 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:17:56,163 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:17:56,163 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:17:56,163 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:17:56,163 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:17:56,169 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:17:56,170 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:17:56,170 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:17:56,170 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:17:56,170 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:17:56,170 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:17:56,729 - src.llm_wrapper - INFO - Model ready in 0.56s
2025-09-05 04:17:56,729 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:17:56,729 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:17:56,729 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:17:56,729 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:17:56,729 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:17:56,729 - src.rag_pipeline - INFO - Processing validated query: 'What is natural language processing?'
2025-09-05 04:17:56,736 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is natural language processing?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.20it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.19it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.15it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.15it/s, batch=1/1, memory=N/A]
2025-09-05 04:17:57,362 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:17:57,363 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:17:57,782 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.045s
2025-09-05 04:17:57,790 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:17:57,791 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1749 tokens) in 0.008s
2025-09-05 04:18:05,183 - src.rag_pipeline - INFO - Generated response (55 tokens) in 7.388s
2025-09-05 04:18:05,401 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:18:05,458 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:18:05,466 - src.experiment_runner - INFO - Completed run 356/510: 9.02s
2025-09-05 04:18:05,466 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:18:05,466 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:18:05,466 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:18:05,467 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:18:05,467 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:18:05,467 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:18:05,467 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:18:05,472 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:18:05,473 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:18:05,473 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:18:05,473 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:18:05,473 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:18:05,473 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:18:06,007 - src.llm_wrapper - INFO - Model ready in 0.53s
2025-09-05 04:18:06,007 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:18:06,007 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:18:06,007 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:18:06,007 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:18:06,007 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:18:06,007 - src.rag_pipeline - INFO - Processing validated query: 'Explain computer vision techniques.'
2025-09-05 04:18:06,015 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain computer vision techniques....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.86it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.83it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.80it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.80it/s, batch=1/1, memory=N/A]
2025-09-05 04:18:06,709 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:18:06,710 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:18:07,133 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.117s
2025-09-05 04:18:07,143 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:18:07,145 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1297 tokens) in 0.010s
2025-09-05 04:18:26,885 - src.rag_pipeline - INFO - Generated response (318 tokens) in 19.732s
2025-09-05 04:18:27,076 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:18:27,135 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:18:27,144 - src.experiment_runner - INFO - Completed run 357/510: 21.42s
2025-09-05 04:18:27,144 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:18:27,144 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:18:27,144 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:18:27,144 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:18:27,144 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:18:27,144 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:18:27,144 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:18:27,152 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:18:27,153 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:18:27,153 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:18:27,153 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:18:27,153 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:18:27,153 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:18:27,677 - src.llm_wrapper - INFO - Model ready in 0.52s
2025-09-05 04:18:27,677 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:18:27,677 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:18:27,677 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:18:27,677 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:18:27,677 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:18:27,677 - src.rag_pipeline - INFO - Processing validated query: 'What is reinforcement learning?'
2025-09-05 04:18:27,684 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is reinforcement learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.90it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.90it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.88it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.87it/s, batch=1/1, memory=N/A]
2025-09-05 04:18:28,319 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:18:28,322 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:18:28,762 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.078s
2025-09-05 04:18:28,770 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:18:28,771 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1236 tokens) in 0.006s
2025-09-05 04:18:41,255 - src.rag_pipeline - INFO - Generated response (180 tokens) in 12.476s
2025-09-05 04:18:41,484 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:18:41,553 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:18:41,559 - src.experiment_runner - INFO - Completed run 358/510: 14.11s
2025-09-05 04:18:41,559 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:18:41,559 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:18:41,559 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:18:41,559 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:18:41,559 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:18:41,559 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:18:41,559 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:18:41,565 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:18:41,566 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:18:41,566 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:18:41,566 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:18:41,566 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:18:41,566 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:18:42,184 - src.llm_wrapper - INFO - Model ready in 0.62s
2025-09-05 04:18:42,184 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:18:42,184 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:18:42,184 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:18:42,184 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:18:42,184 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:18:42,184 - src.rag_pipeline - INFO - Processing validated query: 'How does data preprocessing work?'
2025-09-05 04:18:42,192 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does data preprocessing work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.74it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.73it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.71it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.71it/s, batch=1/1, memory=N/A]
2025-09-05 04:18:42,978 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:18:42,981 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:18:43,469 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.275s
2025-09-05 04:18:43,480 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:18:43,481 - src.rag_pipeline - INFO - Built prompt with 5 contexts (909 tokens) in 0.009s
2025-09-05 04:18:52,891 - src.rag_pipeline - INFO - Generated response (139 tokens) in 9.387s
2025-09-05 04:18:53,212 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:18:53,286 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:18:53,296 - src.experiment_runner - INFO - Completed run 359/510: 11.33s
2025-09-05 04:18:53,296 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:18:53,296 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:18:53,296 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:18:53,296 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:18:53,296 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:18:53,296 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:18:53,296 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:18:53,304 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:18:53,305 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:18:53,305 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:18:53,305 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:18:53,305 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:18:53,305 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:18:53,907 - src.llm_wrapper - INFO - Model ready in 0.60s
2025-09-05 04:18:53,907 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:18:53,907 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:18:53,907 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:18:53,907 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:18:53,908 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:18:53,908 - src.rag_pipeline - INFO - Processing validated query: 'What are the challenges in AI development?'
2025-09-05 04:18:53,916 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are the challenges in AI development?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.79it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.77it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.75it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.75it/s, batch=1/1, memory=N/A]
2025-09-05 04:18:54,417 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:18:54,419 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:18:54,890 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.974s
2025-09-05 04:18:54,900 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:18:54,901 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1445 tokens) in 0.010s
2025-09-05 04:19:11,255 - src.rag_pipeline - INFO - Generated response (237 tokens) in 16.343s
2025-09-05 04:19:11,524 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:19:11,584 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:19:11,592 - src.experiment_runner - INFO - Completed run 360/510: 17.96s
2025-09-05 04:19:11,592 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:19:11,592 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:19:11,592 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:19:11,592 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:19:11,592 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:19:11,592 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:19:11,592 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:19:11,599 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:19:11,600 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:19:11,600 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:19:11,600 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:19:11,600 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:19:11,600 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:19:12,201 - src.llm_wrapper - INFO - Model ready in 0.60s
2025-09-05 04:19:12,201 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:19:12,201 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:19:12,201 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:19:12,201 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:19:12,201 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:19:12,201 - src.rag_pipeline - INFO - Processing validated query: 'What is machine learning?'
2025-09-05 04:19:12,212 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is machine learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.04it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.02it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.98it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.98it/s, batch=1/1, memory=N/A]
2025-09-05 04:19:12,864 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:19:12,869 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:19:13,365 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.152s
2025-09-05 04:19:13,380 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:19:13,381 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1162 tokens) in 0.013s
2025-09-05 04:19:19,679 - src.rag_pipeline - INFO - Generated response (59 tokens) in 6.285s
2025-09-05 04:19:20,008 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:19:20,081 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:19:20,090 - src.experiment_runner - INFO - Completed run 361/510: 8.09s
2025-09-05 04:19:20,090 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:19:20,090 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:19:20,090 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:19:20,090 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:19:20,090 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:19:20,091 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:19:20,091 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:19:20,097 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:19:20,097 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:19:20,097 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:19:20,097 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:19:20,097 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:19:20,097 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:19:20,743 - src.llm_wrapper - INFO - Model ready in 0.65s
2025-09-05 04:19:20,747 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:19:20,747 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:19:20,747 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:19:20,747 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:19:20,747 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:19:20,747 - src.rag_pipeline - INFO - Processing validated query: 'How does artificial intelligence work?'
2025-09-05 04:19:20,756 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does artificial intelligence work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.20it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.18it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.14it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.14it/s, batch=1/1, memory=N/A]
2025-09-05 04:19:21,693 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:19:21,696 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:19:22,194 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.438s
2025-09-05 04:19:22,207 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:19:22,208 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1211 tokens) in 0.012s
2025-09-05 04:19:40,714 - src.rag_pipeline - INFO - Generated response (287 tokens) in 18.488s
2025-09-05 04:19:41,000 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:19:41,057 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:19:41,065 - src.experiment_runner - INFO - Completed run 362/510: 20.63s
2025-09-05 04:19:41,065 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:19:41,065 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:19:41,065 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:19:41,065 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:19:41,065 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:19:41,065 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:19:41,065 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:19:41,071 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:19:41,072 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:19:41,072 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:19:41,072 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:19:41,072 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:19:41,072 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:19:41,745 - src.llm_wrapper - INFO - Model ready in 0.67s
2025-09-05 04:19:41,747 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:19:41,747 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:19:41,747 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:19:41,747 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:19:41,747 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:19:41,748 - src.rag_pipeline - INFO - Processing validated query: 'Explain deep learning algorithms.'
2025-09-05 04:19:41,760 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain deep learning algorithms....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.96it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.95it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.92it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.92it/s, batch=1/1, memory=N/A]
2025-09-05 04:19:42,677 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:19:42,680 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:19:43,195 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.435s
2025-09-05 04:19:43,210 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:19:43,211 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1198 tokens) in 0.012s
2025-09-05 04:19:48,717 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.501s
2025-09-05 04:19:49,015 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:19:49,081 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:19:49,087 - src.experiment_runner - INFO - Completed run 363/510: 7.65s
2025-09-05 04:19:49,087 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:19:49,087 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:19:49,088 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:19:49,088 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:19:49,088 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:19:49,088 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:19:49,088 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:19:49,093 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:19:49,094 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:19:49,094 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:19:49,094 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:19:49,094 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:19:49,094 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:19:49,727 - src.llm_wrapper - INFO - Model ready in 0.63s
2025-09-05 04:19:49,727 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:19:49,727 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:19:49,727 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:19:49,727 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:19:49,727 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:19:49,727 - src.rag_pipeline - INFO - Processing validated query: 'What are neural networks?'
2025-09-05 04:19:49,736 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are neural networks?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.17it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.17it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.15it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.14it/s, batch=1/1, memory=N/A]
2025-09-05 04:19:50,585 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:19:50,588 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:19:51,132 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.396s
2025-09-05 04:19:51,146 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:19:51,147 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1353 tokens) in 0.012s
2025-09-05 04:20:07,200 - src.rag_pipeline - INFO - Generated response (238 tokens) in 16.046s
2025-09-05 04:20:07,537 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:20:07,602 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:20:07,609 - src.experiment_runner - INFO - Completed run 364/510: 18.11s
2025-09-05 04:20:07,609 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:20:07,609 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:20:07,609 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:20:07,610 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:20:07,610 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:20:07,610 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:20:07,610 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:20:07,617 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:20:07,618 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:20:07,618 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:20:07,618 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:20:07,618 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:20:07,619 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:20:08,254 - src.llm_wrapper - INFO - Model ready in 0.64s
2025-09-05 04:20:08,255 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:20:08,255 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:20:08,255 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:20:08,255 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:20:08,255 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:20:08,255 - src.rag_pipeline - INFO - Processing validated query: 'How do large language models work?'
2025-09-05 04:20:08,263 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How do large language models work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.96it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.95it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.92it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.92it/s, batch=1/1, memory=N/A]
2025-09-05 04:20:08,917 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:20:08,919 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:20:09,404 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.140s
2025-09-05 04:20:09,415 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:20:09,416 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1337 tokens) in 0.010s
2025-09-05 04:20:15,345 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.925s
2025-09-05 04:20:15,594 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:20:15,663 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:20:15,674 - src.experiment_runner - INFO - Completed run 365/510: 7.74s
2025-09-05 04:20:15,674 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:20:15,674 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:20:15,675 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:20:15,675 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:20:15,675 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:20:15,675 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:20:15,675 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:20:15,681 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:20:15,681 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:20:15,681 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:20:15,681 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:20:15,681 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:20:15,682 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:20:16,267 - src.llm_wrapper - INFO - Model ready in 0.59s
2025-09-05 04:20:16,268 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:20:16,268 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:20:16,268 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:20:16,268 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:20:16,268 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:20:16,268 - src.rag_pipeline - INFO - Processing validated query: 'What is natural language processing?'
2025-09-05 04:20:16,277 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is natural language processing?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.99it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.98it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.96it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.96it/s, batch=1/1, memory=N/A]
2025-09-05 04:20:17,032 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:20:17,035 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:20:17,568 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.291s
2025-09-05 04:20:17,581 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:20:17,582 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1749 tokens) in 0.012s
2025-09-05 04:20:25,025 - src.rag_pipeline - INFO - Generated response (55 tokens) in 7.438s
2025-09-05 04:20:25,231 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:20:25,295 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:20:25,303 - src.experiment_runner - INFO - Completed run 366/510: 9.35s
2025-09-05 04:20:25,304 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:20:25,304 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:20:25,304 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:20:25,304 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:20:25,304 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:20:25,304 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:20:25,304 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:20:25,309 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:20:25,310 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:20:25,310 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:20:25,310 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:20:25,310 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:20:25,310 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:20:25,923 - src.llm_wrapper - INFO - Model ready in 0.61s
2025-09-05 04:20:25,923 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:20:25,923 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:20:25,923 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:20:25,923 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:20:25,923 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:20:25,923 - src.rag_pipeline - INFO - Processing validated query: 'Explain computer vision techniques.'
2025-09-05 04:20:25,937 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain computer vision techniques....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.24it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.23it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.19it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.19it/s, batch=1/1, memory=N/A]
2025-09-05 04:20:26,577 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:20:26,579 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:20:27,114 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.176s
2025-09-05 04:20:27,119 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:20:27,120 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1297 tokens) in 0.004s
2025-09-05 04:20:47,190 - src.rag_pipeline - INFO - Generated response (318 tokens) in 20.063s
2025-09-05 04:20:47,520 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:20:47,595 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:20:47,603 - src.experiment_runner - INFO - Completed run 367/510: 21.89s
2025-09-05 04:20:47,603 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:20:47,603 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:20:47,603 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:20:47,604 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:20:47,604 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:20:47,604 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:20:47,604 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:20:47,611 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:20:47,612 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:20:47,612 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:20:47,612 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:20:47,612 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:20:47,612 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:20:48,223 - src.llm_wrapper - INFO - Model ready in 0.61s
2025-09-05 04:20:48,224 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:20:48,224 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:20:48,224 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:20:48,224 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:20:48,224 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:20:48,224 - src.rag_pipeline - INFO - Processing validated query: 'What is reinforcement learning?'
2025-09-05 04:20:48,234 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is reinforcement learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.65it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.64it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.61it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.61it/s, batch=1/1, memory=N/A]
2025-09-05 04:20:48,998 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:20:49,001 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:20:49,573 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.338s
2025-09-05 04:20:49,581 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:20:49,583 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1236 tokens) in 0.007s
2025-09-05 04:21:02,291 - src.rag_pipeline - INFO - Generated response (180 tokens) in 12.696s
2025-09-05 04:21:02,588 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:21:02,654 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:21:02,664 - src.experiment_runner - INFO - Completed run 368/510: 14.69s
2025-09-05 04:21:02,664 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:21:02,664 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:21:02,664 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:21:02,664 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:21:02,664 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:21:02,664 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:21:02,664 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:21:02,671 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:21:02,671 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:21:02,671 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:21:02,671 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:21:02,672 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:21:02,672 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:21:03,269 - src.llm_wrapper - INFO - Model ready in 0.60s
2025-09-05 04:21:03,270 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:21:03,270 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:21:03,270 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:21:03,270 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:21:03,270 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:21:03,270 - src.rag_pipeline - INFO - Processing validated query: 'How does data preprocessing work?'
2025-09-05 04:21:03,277 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does data preprocessing work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.18it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.18it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.16it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.15it/s, batch=1/1, memory=N/A]
2025-09-05 04:21:03,930 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:21:03,933 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:21:04,415 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.138s
2025-09-05 04:21:04,420 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:21:04,420 - src.rag_pipeline - INFO - Built prompt with 5 contexts (909 tokens) in 0.004s
2025-09-05 04:21:13,935 - src.rag_pipeline - INFO - Generated response (139 tokens) in 9.509s
2025-09-05 04:21:14,199 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:21:14,260 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:21:14,269 - src.experiment_runner - INFO - Completed run 369/510: 11.27s
2025-09-05 04:21:14,269 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:21:14,269 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:21:14,269 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:21:14,269 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:21:14,269 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:21:14,269 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:21:14,270 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:21:14,277 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:21:14,277 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:21:14,277 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:21:14,277 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:21:14,277 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:21:14,277 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:21:14,889 - src.llm_wrapper - INFO - Model ready in 0.61s
2025-09-05 04:21:14,889 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:21:14,889 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:21:14,889 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:21:14,889 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:21:14,889 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:21:14,889 - src.rag_pipeline - INFO - Processing validated query: 'What are the challenges in AI development?'
2025-09-05 04:21:14,901 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are the challenges in AI development?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.67it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.66it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.61it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.61it/s, batch=1/1, memory=N/A]
2025-09-05 04:21:15,681 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:21:15,684 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:21:16,218 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.318s
2025-09-05 04:21:16,238 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:21:16,239 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1445 tokens) in 0.018s
2025-09-05 04:21:32,637 - src.rag_pipeline - INFO - Generated response (237 tokens) in 16.379s
2025-09-05 04:21:33,062 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:21:33,144 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:21:33,153 - src.experiment_runner - INFO - Completed run 370/510: 18.37s
2025-09-05 04:21:33,153 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:21:33,153 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:21:33,153 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:21:33,153 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:21:33,153 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:21:33,153 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:21:33,153 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:21:33,161 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:21:33,162 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:21:33,162 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:21:33,162 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:21:33,162 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:21:33,162 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:21:33,911 - src.llm_wrapper - INFO - Model ready in 0.75s
2025-09-05 04:21:33,911 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:21:33,911 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:21:33,911 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:21:33,911 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:21:33,911 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:21:33,911 - src.rag_pipeline - INFO - Processing validated query: 'What is machine learning?'
2025-09-05 04:21:33,924 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is machine learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.62it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.62it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.55it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.55it/s, batch=1/1, memory=N/A]
2025-09-05 04:21:34,607 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:21:34,610 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:21:35,111 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.187s
2025-09-05 04:21:35,122 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:21:35,123 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1162 tokens) in 0.010s
2025-09-05 04:21:41,185 - src.rag_pipeline - INFO - Generated response (59 tokens) in 6.054s
2025-09-05 04:21:41,404 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:21:41,480 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:21:41,489 - src.experiment_runner - INFO - Completed run 371/510: 8.03s
2025-09-05 04:21:41,489 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:21:41,489 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:21:41,490 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:21:41,490 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:21:41,490 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:21:41,490 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:21:41,490 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:21:41,496 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:21:41,497 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:21:41,497 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:21:41,497 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:21:41,497 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:21:41,497 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:21:42,102 - src.llm_wrapper - INFO - Model ready in 0.61s
2025-09-05 04:21:42,102 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:21:42,102 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:21:42,102 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:21:42,102 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:21:42,102 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:21:42,102 - src.rag_pipeline - INFO - Processing validated query: 'How does artificial intelligence work?'
2025-09-05 04:21:42,110 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does artificial intelligence work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.97it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.96it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.94it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.93it/s, batch=1/1, memory=N/A]
2025-09-05 04:21:42,873 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:21:42,877 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:21:43,369 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.259s
2025-09-05 04:21:43,381 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:21:43,382 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1211 tokens) in 0.010s
2025-09-05 04:22:01,700 - src.rag_pipeline - INFO - Generated response (287 tokens) in 18.311s
2025-09-05 04:22:01,967 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:22:02,026 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:22:02,034 - src.experiment_runner - INFO - Completed run 372/510: 20.21s
2025-09-05 04:22:02,034 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:22:02,034 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:22:02,034 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:22:02,035 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:22:02,035 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:22:02,035 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:22:02,035 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:22:02,040 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:22:02,041 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:22:02,041 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:22:02,041 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:22:02,041 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:22:02,041 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:22:02,654 - src.llm_wrapper - INFO - Model ready in 0.61s
2025-09-05 04:22:02,654 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:22:02,654 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:22:02,654 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:22:02,654 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:22:02,654 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:22:02,654 - src.rag_pipeline - INFO - Processing validated query: 'Explain deep learning algorithms.'
2025-09-05 04:22:02,666 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain deep learning algorithms....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.87it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.87it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.85it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.85it/s, batch=1/1, memory=N/A]
2025-09-05 04:22:03,406 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:22:03,409 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:22:03,855 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.189s
2025-09-05 04:22:03,863 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:22:03,864 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1198 tokens) in 0.007s
2025-09-05 04:22:09,421 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.555s
2025-09-05 04:22:09,610 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:22:09,670 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:22:09,680 - src.experiment_runner - INFO - Completed run 373/510: 7.39s
2025-09-05 04:22:09,681 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:22:09,681 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:22:09,681 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:22:09,681 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:22:09,681 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:22:09,681 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:22:09,681 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:22:09,686 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:22:09,687 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:22:09,687 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:22:09,687 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:22:09,687 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:22:09,687 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:22:10,240 - src.llm_wrapper - INFO - Model ready in 0.55s
2025-09-05 04:22:10,240 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:22:10,240 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:22:10,240 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:22:10,240 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:22:10,240 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:22:10,240 - src.rag_pipeline - INFO - Processing validated query: 'What are neural networks?'
2025-09-05 04:22:10,247 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are neural networks?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.87it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.87it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.83it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.82it/s, batch=1/1, memory=N/A]
2025-09-05 04:22:10,638 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:22:10,640 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:22:10,989 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.742s
2025-09-05 04:22:10,995 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:22:10,996 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1353 tokens) in 0.006s
2025-09-05 04:22:27,014 - src.rag_pipeline - INFO - Generated response (238 tokens) in 16.014s
2025-09-05 04:22:27,250 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:22:27,306 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:22:27,317 - src.experiment_runner - INFO - Completed run 374/510: 17.33s
2025-09-05 04:22:27,317 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:22:27,317 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:22:27,317 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:22:27,317 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:22:27,317 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:22:27,317 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:22:27,317 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:22:27,323 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:22:27,324 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:22:27,324 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:22:27,324 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:22:27,324 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:22:27,324 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:22:27,879 - src.llm_wrapper - INFO - Model ready in 0.56s
2025-09-05 04:22:27,879 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:22:27,879 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:22:27,879 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:22:27,879 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:22:27,879 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:22:27,880 - src.rag_pipeline - INFO - Processing validated query: 'How do large language models work?'
2025-09-05 04:22:27,886 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How do large language models work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.21it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.20it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.17it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.17it/s, batch=1/1, memory=N/A]
2025-09-05 04:22:28,433 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:22:28,435 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:22:28,803 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.916s
2025-09-05 04:22:28,805 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:22:28,806 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1337 tokens) in 0.002s
2025-09-05 04:22:34,712 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.903s
2025-09-05 04:22:34,882 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:22:34,946 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:22:34,954 - src.experiment_runner - INFO - Completed run 375/510: 7.40s
2025-09-05 04:22:34,954 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:22:34,954 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:22:34,954 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:22:34,954 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:22:34,954 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:22:34,954 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:22:34,954 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:22:34,960 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:22:34,960 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:22:34,960 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:22:34,960 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:22:34,960 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:22:34,960 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:22:35,468 - src.llm_wrapper - INFO - Model ready in 0.51s
2025-09-05 04:22:35,468 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:22:35,468 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:22:35,468 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:22:35,468 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:22:35,468 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:22:35,468 - src.rag_pipeline - INFO - Processing validated query: 'What is natural language processing?'
2025-09-05 04:22:35,476 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is natural language processing?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.26it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.26it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.23it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.23it/s, batch=1/1, memory=N/A]
2025-09-05 04:22:35,850 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:22:35,852 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:22:36,205 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.729s
2025-09-05 04:22:36,213 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:22:36,213 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1749 tokens) in 0.007s
2025-09-05 04:22:43,592 - src.rag_pipeline - INFO - Generated response (55 tokens) in 7.374s
2025-09-05 04:22:43,801 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:22:43,867 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:22:43,875 - src.experiment_runner - INFO - Completed run 376/510: 8.64s
2025-09-05 04:22:43,875 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:22:43,875 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:22:43,875 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:22:43,875 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:22:43,875 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:22:43,875 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:22:43,875 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:22:43,882 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:22:43,883 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:22:43,883 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:22:43,883 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:22:43,883 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:22:43,883 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:22:44,526 - src.llm_wrapper - INFO - Model ready in 0.64s
2025-09-05 04:22:44,526 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:22:44,526 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:22:44,526 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:22:44,526 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:22:44,526 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:22:44,526 - src.rag_pipeline - INFO - Processing validated query: 'Explain computer vision techniques.'
2025-09-05 04:22:44,533 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain computer vision techniques....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.94it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.93it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.91it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.91it/s, batch=1/1, memory=N/A]
2025-09-05 04:22:45,140 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:22:45,141 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:22:45,556 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.023s
2025-09-05 04:22:45,561 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:22:45,562 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1297 tokens) in 0.005s
2025-09-05 04:23:05,410 - src.rag_pipeline - INFO - Generated response (318 tokens) in 19.841s
2025-09-05 04:23:05,619 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:23:05,670 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:23:05,678 - src.experiment_runner - INFO - Completed run 377/510: 21.54s
2025-09-05 04:23:05,678 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:23:05,678 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:23:05,678 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:23:05,678 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:23:05,678 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:23:05,678 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:23:05,678 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:23:05,684 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:23:05,684 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:23:05,684 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:23:05,684 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:23:05,684 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:23:05,684 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:23:06,259 - src.llm_wrapper - INFO - Model ready in 0.57s
2025-09-05 04:23:06,259 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:23:06,259 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:23:06,259 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:23:06,259 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:23:06,259 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:23:06,259 - src.rag_pipeline - INFO - Processing validated query: 'What is reinforcement learning?'
2025-09-05 04:23:06,266 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is reinforcement learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.91it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.90it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.88it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.88it/s, batch=1/1, memory=N/A]
2025-09-05 04:23:06,930 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:23:06,932 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:23:07,368 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.102s
2025-09-05 04:23:07,379 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:23:07,380 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1236 tokens) in 0.010s
2025-09-05 04:23:20,388 - src.rag_pipeline - INFO - Generated response (180 tokens) in 13.001s
2025-09-05 04:23:20,590 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:23:20,651 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:23:20,659 - src.experiment_runner - INFO - Completed run 378/510: 14.71s
2025-09-05 04:23:20,659 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:23:20,659 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:23:20,659 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:23:20,659 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:23:20,659 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:23:20,659 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:23:20,659 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:23:20,665 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:23:20,665 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:23:20,665 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:23:20,665 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:23:20,665 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:23:20,665 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:23:21,196 - src.llm_wrapper - INFO - Model ready in 0.53s
2025-09-05 04:23:21,196 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:23:21,196 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:23:21,196 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:23:21,196 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:23:21,196 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:23:21,196 - src.rag_pipeline - INFO - Processing validated query: 'How does data preprocessing work?'
2025-09-05 04:23:21,203 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does data preprocessing work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.98it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.97it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.96it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.96it/s, batch=1/1, memory=N/A]
2025-09-05 04:23:21,829 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:23:21,831 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:23:22,254 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.051s
2025-09-05 04:23:22,263 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:23:22,263 - src.rag_pipeline - INFO - Built prompt with 5 contexts (909 tokens) in 0.007s
2025-09-05 04:23:31,581 - src.rag_pipeline - INFO - Generated response (139 tokens) in 9.309s
2025-09-05 04:23:31,821 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:23:31,880 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:23:31,890 - src.experiment_runner - INFO - Completed run 379/510: 10.92s
2025-09-05 04:23:31,891 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:23:31,891 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:23:31,891 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:23:31,891 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:23:31,891 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:23:31,891 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:23:31,891 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:23:31,898 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:23:31,898 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:23:31,898 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:23:31,898 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:23:31,898 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:23:31,898 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:23:32,416 - src.llm_wrapper - INFO - Model ready in 0.52s
2025-09-05 04:23:32,416 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:23:32,416 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:23:32,416 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:23:32,416 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:23:32,416 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:23:32,416 - src.rag_pipeline - INFO - Processing validated query: 'What are the challenges in AI development?'
2025-09-05 04:23:32,423 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are the challenges in AI development?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.18it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.18it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.16it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.16it/s, batch=1/1, memory=N/A]
2025-09-05 04:23:32,988 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:23:32,991 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:23:33,386 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.963s
2025-09-05 04:23:33,399 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:23:33,400 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1445 tokens) in 0.012s
2025-09-05 04:23:49,625 - src.rag_pipeline - INFO - Generated response (237 tokens) in 16.218s
2025-09-05 04:23:49,824 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:23:49,880 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:23:49,887 - src.experiment_runner - INFO - Completed run 380/510: 17.74s
2025-09-05 04:23:49,887 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:23:49,887 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:23:49,887 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:23:49,887 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:23:49,887 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:23:49,887 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:23:49,887 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:23:49,893 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:23:49,893 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:23:49,893 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:23:49,893 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:23:49,893 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:23:49,893 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:23:50,455 - src.llm_wrapper - INFO - Model ready in 0.56s
2025-09-05 04:23:50,455 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:23:50,455 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:23:50,455 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:23:50,456 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:23:50,456 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:23:50,456 - src.rag_pipeline - INFO - Processing validated query: 'What is machine learning?'
2025-09-05 04:23:50,465 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is machine learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.92it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.91it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.88it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.88it/s, batch=1/1, memory=N/A]
2025-09-05 04:23:51,110 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:23:51,112 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:23:51,520 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.055s
2025-09-05 04:23:51,531 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:23:51,532 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1162 tokens) in 0.009s
2025-09-05 04:23:57,574 - src.rag_pipeline - INFO - Generated response (59 tokens) in 6.037s
2025-09-05 04:23:57,782 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:23:57,846 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:23:57,853 - src.experiment_runner - INFO - Completed run 381/510: 7.69s
2025-09-05 04:23:57,853 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:23:57,853 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:23:57,853 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:23:57,853 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:23:57,853 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:23:57,853 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:23:57,853 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:23:57,859 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:23:57,859 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:23:57,859 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:23:57,859 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:23:57,859 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:23:57,859 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:23:58,390 - src.llm_wrapper - INFO - Model ready in 0.53s
2025-09-05 04:23:58,390 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:23:58,390 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:23:58,390 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:23:58,390 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:23:58,390 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:23:58,390 - src.rag_pipeline - INFO - Processing validated query: 'How does artificial intelligence work?'
2025-09-05 04:23:58,397 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does artificial intelligence work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.10it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.10it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.08it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.08it/s, batch=1/1, memory=N/A]
2025-09-05 04:23:59,068 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:23:59,073 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:23:59,486 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.089s
2025-09-05 04:23:59,493 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:23:59,494 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1211 tokens) in 0.007s
2025-09-05 04:24:17,398 - src.rag_pipeline - INFO - Generated response (287 tokens) in 17.896s
2025-09-05 04:24:17,586 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:24:17,641 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:24:17,648 - src.experiment_runner - INFO - Completed run 382/510: 19.55s
2025-09-05 04:24:17,648 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:24:17,648 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:24:17,648 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:24:17,648 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:24:17,648 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:24:17,648 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:24:17,648 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:24:17,655 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:24:17,655 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:24:17,656 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:24:17,656 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:24:17,656 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:24:17,656 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:24:18,204 - src.llm_wrapper - INFO - Model ready in 0.55s
2025-09-05 04:24:18,204 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:24:18,204 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:24:18,204 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:24:18,204 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:24:18,204 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:24:18,204 - src.rag_pipeline - INFO - Processing validated query: 'Explain deep learning algorithms.'
2025-09-05 04:24:18,210 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain deep learning algorithms....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.98it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.97it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.95it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.95it/s, batch=1/1, memory=N/A]
2025-09-05 04:24:18,825 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:24:18,828 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:24:19,237 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.026s
2025-09-05 04:24:19,245 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:24:19,246 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1198 tokens) in 0.005s
2025-09-05 04:24:24,675 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.426s
2025-09-05 04:24:24,848 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:24:24,899 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:24:24,906 - src.experiment_runner - INFO - Completed run 383/510: 7.03s
2025-09-05 04:24:24,906 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:24:24,906 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:24:24,906 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:24:24,906 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:24:24,906 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:24:24,906 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:24:24,906 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:24:24,913 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:24:24,913 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:24:24,913 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:24:24,913 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:24:24,913 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:24:24,913 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:24:25,434 - src.llm_wrapper - INFO - Model ready in 0.52s
2025-09-05 04:24:25,434 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:24:25,434 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:24:25,434 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:24:25,434 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:24:25,434 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:24:25,434 - src.rag_pipeline - INFO - Processing validated query: 'What are neural networks?'
2025-09-05 04:24:25,441 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are neural networks?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.33it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.33it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.31it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.31it/s, batch=1/1, memory=N/A]
2025-09-05 04:24:25,904 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:24:25,906 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:24:26,309 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.867s
2025-09-05 04:24:26,319 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:24:26,320 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1353 tokens) in 0.010s
2025-09-05 04:24:42,044 - src.rag_pipeline - INFO - Generated response (238 tokens) in 15.717s
2025-09-05 04:24:42,265 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:24:42,320 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:24:42,328 - src.experiment_runner - INFO - Completed run 384/510: 17.14s
2025-09-05 04:24:42,329 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:24:42,329 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:24:42,329 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:24:42,329 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:24:42,329 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:24:42,329 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:24:42,329 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:24:42,337 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:24:42,337 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:24:42,337 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:24:42,337 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:24:42,337 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:24:42,337 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:24:42,978 - src.llm_wrapper - INFO - Model ready in 0.64s
2025-09-05 04:24:42,978 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:24:42,978 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:24:42,978 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:24:42,978 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:24:42,978 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:24:42,978 - src.rag_pipeline - INFO - Processing validated query: 'How do large language models work?'
2025-09-05 04:24:42,994 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How do large language models work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.90it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.89it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.84it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.84it/s, batch=1/1, memory=N/A]
2025-09-05 04:24:43,735 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:24:43,738 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:24:44,170 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.175s
2025-09-05 04:24:44,182 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:24:44,184 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1337 tokens) in 0.009s
2025-09-05 04:24:50,175 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.984s
2025-09-05 04:24:50,421 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:24:50,487 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:24:50,496 - src.experiment_runner - INFO - Completed run 385/510: 7.85s
2025-09-05 04:24:50,496 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:24:50,496 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:24:50,496 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:24:50,496 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:24:50,496 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:24:50,496 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:24:50,496 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:24:50,502 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:24:50,502 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:24:50,502 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:24:50,502 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:24:50,503 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:24:50,503 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:24:51,140 - src.llm_wrapper - INFO - Model ready in 0.64s
2025-09-05 04:24:51,140 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:24:51,140 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:24:51,140 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:24:51,140 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:24:51,140 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:24:51,140 - src.rag_pipeline - INFO - Processing validated query: 'What is natural language processing?'
2025-09-05 04:24:51,150 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is natural language processing?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.71it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.70it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.68it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.68it/s, batch=1/1, memory=N/A]
2025-09-05 04:24:52,126 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:24:52,130 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:24:52,584 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.432s
2025-09-05 04:24:52,595 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:24:52,596 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1749 tokens) in 0.010s
2025-09-05 04:25:00,167 - src.rag_pipeline - INFO - Generated response (55 tokens) in 7.566s
2025-09-05 04:25:00,405 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:25:00,468 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:25:00,477 - src.experiment_runner - INFO - Completed run 386/510: 9.67s
2025-09-05 04:25:00,477 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:25:00,477 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:25:00,477 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:25:00,477 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:25:00,477 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:25:00,477 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:25:00,477 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:25:00,483 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:25:00,484 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:25:00,484 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:25:00,484 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:25:00,484 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:25:00,484 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:25:01,081 - src.llm_wrapper - INFO - Model ready in 0.60s
2025-09-05 04:25:01,081 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:25:01,081 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:25:01,081 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:25:01,081 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:25:01,081 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:25:01,081 - src.rag_pipeline - INFO - Processing validated query: 'Explain computer vision techniques.'
2025-09-05 04:25:01,091 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain computer vision techniques....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.91it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.90it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.88it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.88it/s, batch=1/1, memory=N/A]
2025-09-05 04:25:01,807 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:25:01,810 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:25:02,266 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.174s
2025-09-05 04:25:02,277 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:25:02,277 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1297 tokens) in 0.009s
2025-09-05 04:25:22,211 - src.rag_pipeline - INFO - Generated response (318 tokens) in 19.928s
2025-09-05 04:25:22,431 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:25:22,485 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:25:22,549 - src.experiment_runner - INFO - Completed run 387/510: 21.74s
2025-09-05 04:25:22,549 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:25:22,549 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:25:22,549 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:25:22,549 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:25:22,549 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:25:22,549 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:25:22,549 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:25:22,555 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:25:22,555 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:25:22,555 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:25:22,555 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:25:22,555 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:25:22,555 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:25:23,153 - src.llm_wrapper - INFO - Model ready in 0.60s
2025-09-05 04:25:23,153 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:25:23,153 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:25:23,153 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:25:23,153 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:25:23,154 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:25:23,154 - src.rag_pipeline - INFO - Processing validated query: 'What is reinforcement learning?'
2025-09-05 04:25:23,162 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is reinforcement learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.08it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.07it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.05it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.05it/s, batch=1/1, memory=N/A]
2025-09-05 04:25:23,771 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:25:23,773 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:25:24,177 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.014s
2025-09-05 04:25:24,188 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:25:24,189 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1236 tokens) in 0.010s
2025-09-05 04:25:36,528 - src.rag_pipeline - INFO - Generated response (180 tokens) in 12.334s
2025-09-05 04:25:36,746 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:25:36,806 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:25:36,815 - src.experiment_runner - INFO - Completed run 388/510: 13.98s
2025-09-05 04:25:36,815 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:25:36,815 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:25:36,815 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:25:36,815 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:25:36,815 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:25:36,815 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:25:36,815 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:25:36,822 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:25:36,823 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:25:36,823 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:25:36,823 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:25:36,823 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:25:36,823 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:25:37,378 - src.llm_wrapper - INFO - Model ready in 0.56s
2025-09-05 04:25:37,378 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:25:37,378 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:25:37,378 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:25:37,378 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:25:37,378 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:25:37,378 - src.rag_pipeline - INFO - Processing validated query: 'How does data preprocessing work?'
2025-09-05 04:25:37,389 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does data preprocessing work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.88it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.87it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.86it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.86it/s, batch=1/1, memory=N/A]
2025-09-05 04:25:38,184 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:25:38,187 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:25:38,608 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.219s
2025-09-05 04:25:38,621 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:25:38,622 - src.rag_pipeline - INFO - Built prompt with 5 contexts (909 tokens) in 0.012s
2025-09-05 04:25:48,070 - src.rag_pipeline - INFO - Generated response (139 tokens) in 9.444s
2025-09-05 04:25:48,283 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:25:48,342 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:25:48,350 - src.experiment_runner - INFO - Completed run 389/510: 11.26s
2025-09-05 04:25:48,351 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:25:48,351 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:25:48,351 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:25:48,351 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:25:48,351 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:25:48,351 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:25:48,351 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:25:48,358 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:25:48,358 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:25:48,358 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:25:48,358 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:25:48,359 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:25:48,359 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:25:48,933 - src.llm_wrapper - INFO - Model ready in 0.57s
2025-09-05 04:25:48,933 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:25:48,933 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:25:48,933 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:25:48,933 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:25:48,933 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:25:48,933 - src.rag_pipeline - INFO - Processing validated query: 'What are the challenges in AI development?'
2025-09-05 04:25:48,941 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are the challenges in AI development?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.94it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.93it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.89it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.89it/s, batch=1/1, memory=N/A]
2025-09-05 04:25:49,607 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:25:49,610 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:25:50,055 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.113s
2025-09-05 04:25:50,069 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:25:50,070 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1445 tokens) in 0.013s
2025-09-05 04:26:05,991 - src.rag_pipeline - INFO - Generated response (237 tokens) in 15.914s
2025-09-05 04:26:06,183 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:26:06,237 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:26:06,248 - src.experiment_runner - INFO - Completed run 390/510: 17.64s
2025-09-05 04:26:06,248 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:26:06,248 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:26:06,248 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:26:06,248 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:26:06,248 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:26:06,248 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:26:06,248 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:26:06,254 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:26:06,254 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:26:06,254 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:26:06,254 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:26:06,254 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:26:06,254 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:26:06,773 - src.llm_wrapper - INFO - Model ready in 0.52s
2025-09-05 04:26:06,773 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:26:06,773 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:26:06,773 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:26:06,773 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:26:06,773 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:26:06,774 - src.rag_pipeline - INFO - Processing validated query: 'What is machine learning?'
2025-09-05 04:26:06,781 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is machine learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.20it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.18it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.15it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.15it/s, batch=1/1, memory=N/A]
2025-09-05 04:26:07,384 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:26:07,389 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:26:07,811 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.027s
2025-09-05 04:26:07,818 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:26:07,819 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1162 tokens) in 0.006s
2025-09-05 04:26:13,825 - src.rag_pipeline - INFO - Generated response (59 tokens) in 6.003s
2025-09-05 04:26:14,026 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:26:14,093 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:26:14,100 - src.experiment_runner - INFO - Completed run 391/510: 7.58s
2025-09-05 04:26:14,100 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:26:14,100 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:26:14,100 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:26:14,100 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:26:14,100 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:26:14,101 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:26:14,101 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:26:14,106 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:26:14,106 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:26:14,106 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:26:14,106 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:26:14,106 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:26:14,106 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:26:14,715 - src.llm_wrapper - INFO - Model ready in 0.61s
2025-09-05 04:26:14,715 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:26:14,715 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:26:14,715 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:26:14,715 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:26:14,715 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:26:14,715 - src.rag_pipeline - INFO - Processing validated query: 'How does artificial intelligence work?'
2025-09-05 04:26:14,722 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does artificial intelligence work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.90it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.89it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.86it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.86it/s, batch=1/1, memory=N/A]
2025-09-05 04:26:15,453 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:26:15,456 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:26:15,869 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.147s
2025-09-05 04:26:15,883 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:26:15,883 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1211 tokens) in 0.012s
2025-09-05 04:26:33,663 - src.rag_pipeline - INFO - Generated response (287 tokens) in 17.771s
2025-09-05 04:26:33,880 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:26:33,937 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:26:33,944 - src.experiment_runner - INFO - Completed run 392/510: 19.56s
2025-09-05 04:26:33,944 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:26:33,944 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:26:33,944 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:26:33,944 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:26:33,944 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:26:33,944 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:26:33,944 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:26:33,950 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:26:33,950 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:26:33,950 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:26:33,950 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:26:33,950 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:26:33,950 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:26:34,511 - src.llm_wrapper - INFO - Model ready in 0.56s
2025-09-05 04:26:34,511 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:26:34,511 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:26:34,511 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:26:34,511 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:26:34,511 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:26:34,511 - src.rag_pipeline - INFO - Processing validated query: 'Explain deep learning algorithms.'
2025-09-05 04:26:34,520 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain deep learning algorithms....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.24it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.21it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.19it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.19it/s, batch=1/1, memory=N/A]
2025-09-05 04:26:35,083 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:26:35,085 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:26:35,488 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.967s
2025-09-05 04:26:35,498 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:26:35,499 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1198 tokens) in 0.009s
2025-09-05 04:26:41,017 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.514s
2025-09-05 04:26:41,197 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:26:41,252 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:26:41,260 - src.experiment_runner - INFO - Completed run 393/510: 7.07s
2025-09-05 04:26:41,260 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:26:41,260 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:26:41,260 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:26:41,260 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:26:41,260 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:26:41,260 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:26:41,260 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:26:41,265 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:26:41,266 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:26:41,266 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:26:41,266 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:26:41,266 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:26:41,266 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:26:41,796 - src.llm_wrapper - INFO - Model ready in 0.53s
2025-09-05 04:26:41,796 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:26:41,796 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:26:41,796 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:26:41,796 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:26:41,796 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:26:41,796 - src.rag_pipeline - INFO - Processing validated query: 'What are neural networks?'
2025-09-05 04:26:41,803 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are neural networks?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.11it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.11it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.09it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.09it/s, batch=1/1, memory=N/A]
2025-09-05 04:26:42,536 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:26:42,539 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:26:42,947 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.144s
2025-09-05 04:26:42,956 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:26:42,957 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1353 tokens) in 0.008s
2025-09-05 04:26:58,672 - src.rag_pipeline - INFO - Generated response (238 tokens) in 15.704s
2025-09-05 04:26:58,892 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:26:58,947 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:26:58,956 - src.experiment_runner - INFO - Completed run 394/510: 17.41s
2025-09-05 04:26:58,956 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:26:58,956 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:26:58,956 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:26:58,957 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:26:58,957 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:26:58,957 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:26:58,957 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:26:58,963 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:26:58,963 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:26:58,963 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:26:58,963 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:26:58,964 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:26:58,964 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:26:59,487 - src.llm_wrapper - INFO - Model ready in 0.52s
2025-09-05 04:26:59,487 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:26:59,487 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:26:59,487 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:26:59,487 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:26:59,487 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:26:59,487 - src.rag_pipeline - INFO - Processing validated query: 'How do large language models work?'
2025-09-05 04:26:59,496 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How do large language models work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.18it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.12it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.08it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.08it/s, batch=1/1, memory=N/A]
2025-09-05 04:27:00,095 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:27:00,096 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:27:00,510 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.013s
2025-09-05 04:27:00,519 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:27:00,520 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1337 tokens) in 0.007s
2025-09-05 04:27:06,292 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.767s
2025-09-05 04:27:06,469 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:27:06,522 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:27:06,529 - src.experiment_runner - INFO - Completed run 395/510: 7.34s
2025-09-05 04:27:06,529 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:27:06,529 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:27:06,529 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:27:06,529 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:27:06,529 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:27:06,529 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:27:06,529 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:27:06,534 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:27:06,535 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:27:06,535 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:27:06,535 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:27:06,535 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:27:06,535 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:27:07,083 - src.llm_wrapper - INFO - Model ready in 0.55s
2025-09-05 04:27:07,083 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:27:07,083 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:27:07,083 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:27:07,083 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:27:07,083 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:27:07,083 - src.rag_pipeline - INFO - Processing validated query: 'What is natural language processing?'
2025-09-05 04:27:07,092 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is natural language processing?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.30it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.28it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.24it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.24it/s, batch=1/1, memory=N/A]
2025-09-05 04:27:07,632 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:27:07,633 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:27:08,017 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.925s
2025-09-05 04:27:08,026 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:27:08,027 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1749 tokens) in 0.008s
2025-09-05 04:27:15,502 - src.rag_pipeline - INFO - Generated response (55 tokens) in 7.467s
2025-09-05 04:27:15,690 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:27:15,745 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:27:15,752 - src.experiment_runner - INFO - Completed run 396/510: 8.97s
2025-09-05 04:27:15,753 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:27:15,753 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:27:15,753 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:27:15,753 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:27:15,753 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:27:15,753 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:27:15,753 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:27:15,758 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:27:15,758 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:27:15,758 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:27:15,758 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:27:15,758 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:27:15,758 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:27:16,298 - src.llm_wrapper - INFO - Model ready in 0.54s
2025-09-05 04:27:16,298 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:27:16,298 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:27:16,298 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:27:16,298 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:27:16,298 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:27:16,298 - src.rag_pipeline - INFO - Processing validated query: 'Explain computer vision techniques.'
2025-09-05 04:27:16,306 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain computer vision techniques....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.12it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.10it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.07it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.07it/s, batch=1/1, memory=N/A]
2025-09-05 04:27:16,899 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:27:16,901 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:27:17,273 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.966s
2025-09-05 04:27:17,283 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:27:17,284 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1297 tokens) in 0.008s
2025-09-05 04:27:36,819 - src.rag_pipeline - INFO - Generated response (318 tokens) in 19.528s
2025-09-05 04:27:37,041 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:27:37,097 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:27:37,110 - src.experiment_runner - INFO - Completed run 397/510: 21.07s
2025-09-05 04:27:37,110 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:27:37,110 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:27:37,110 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:27:37,110 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:27:37,110 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:27:37,110 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:27:37,110 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:27:37,118 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:27:37,118 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:27:37,118 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:27:37,118 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:27:37,118 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:27:37,118 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:27:37,629 - src.llm_wrapper - INFO - Model ready in 0.51s
2025-09-05 04:27:37,629 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:27:37,630 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:27:37,630 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:27:37,630 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:27:37,630 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:27:37,630 - src.rag_pipeline - INFO - Processing validated query: 'What is reinforcement learning?'
2025-09-05 04:27:37,639 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is reinforcement learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.12it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.12it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.08it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.08it/s, batch=1/1, memory=N/A]
2025-09-05 04:27:38,213 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:27:38,216 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:27:38,628 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.989s
2025-09-05 04:27:38,633 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:27:38,633 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1236 tokens) in 0.004s
2025-09-05 04:27:50,937 - src.rag_pipeline - INFO - Generated response (180 tokens) in 12.297s
2025-09-05 04:27:51,178 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:27:51,230 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:27:51,238 - src.experiment_runner - INFO - Completed run 398/510: 13.83s
2025-09-05 04:27:51,238 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:27:51,238 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:27:51,239 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:27:51,239 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:27:51,239 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:27:51,239 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:27:51,239 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:27:51,244 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:27:51,245 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:27:51,245 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:27:51,245 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:27:51,245 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:27:51,245 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:27:51,830 - src.llm_wrapper - INFO - Model ready in 0.58s
2025-09-05 04:27:51,830 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:27:51,830 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:27:51,830 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:27:51,830 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:27:51,830 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:27:51,830 - src.rag_pipeline - INFO - Processing validated query: 'How does data preprocessing work?'
2025-09-05 04:27:51,850 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does data preprocessing work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.03it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.02it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.99it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.99it/s, batch=1/1, memory=N/A]
2025-09-05 04:27:52,620 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:27:52,622 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:27:53,014 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.163s
2025-09-05 04:27:53,022 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:27:53,022 - src.rag_pipeline - INFO - Built prompt with 5 contexts (909 tokens) in 0.006s
2025-09-05 04:28:02,606 - src.rag_pipeline - INFO - Generated response (139 tokens) in 9.574s
2025-09-05 04:28:02,865 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:28:02,926 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:28:02,933 - src.experiment_runner - INFO - Completed run 399/510: 11.37s
2025-09-05 04:28:02,933 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:28:02,933 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:28:02,933 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:28:02,933 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:28:02,933 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:28:02,933 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:28:02,933 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:28:02,938 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:28:02,939 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:28:02,939 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:28:02,939 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:28:02,939 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:28:02,939 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:28:03,475 - src.llm_wrapper - INFO - Model ready in 0.54s
2025-09-05 04:28:03,475 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:28:03,475 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:28:03,476 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:28:03,476 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:28:03,476 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:28:03,476 - src.rag_pipeline - INFO - Processing validated query: 'What are the challenges in AI development?'
2025-09-05 04:28:03,483 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are the challenges in AI development?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.02it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.00it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.97it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.97it/s, batch=1/1, memory=N/A]
2025-09-05 04:28:04,228 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:28:04,230 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:28:04,639 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.156s
2025-09-05 04:28:04,651 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:28:04,653 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1445 tokens) in 0.010s
2025-09-05 04:28:20,595 - src.rag_pipeline - INFO - Generated response (237 tokens) in 15.935s
2025-09-05 04:28:20,791 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:28:20,854 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:28:20,864 - src.experiment_runner - INFO - Completed run 400/510: 17.66s
2025-09-05 04:28:20,864 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:28:20,864 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:28:20,864 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:28:20,864 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:28:20,864 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:28:20,864 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:28:20,864 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:28:20,870 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:28:20,870 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:28:20,870 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:28:20,870 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:28:20,870 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:28:20,870 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:28:21,423 - src.llm_wrapper - INFO - Model ready in 0.55s
2025-09-05 04:28:21,423 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:28:21,423 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:28:21,423 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:28:21,423 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:28:21,423 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:28:21,423 - src.rag_pipeline - INFO - Processing validated query: 'What is machine learning?'
2025-09-05 04:28:21,430 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is machine learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.12it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.11it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.10it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.10it/s, batch=1/1, memory=N/A]
2025-09-05 04:28:22,004 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:28:22,006 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:28:22,391 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.960s
2025-09-05 04:28:22,397 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:28:22,399 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1162 tokens) in 0.005s
2025-09-05 04:28:28,396 - src.rag_pipeline - INFO - Generated response (59 tokens) in 5.993s
2025-09-05 04:28:28,616 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:28:28,670 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:28:28,676 - src.experiment_runner - INFO - Completed run 401/510: 7.53s
2025-09-05 04:28:28,676 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:28:28,676 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:28:28,676 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:28:28,677 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:28:28,677 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:28:28,677 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:28:28,677 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:28:28,682 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:28:28,682 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:28:28,682 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:28:28,682 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:28:28,683 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:28:28,683 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:28:29,247 - src.llm_wrapper - INFO - Model ready in 0.56s
2025-09-05 04:28:29,248 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:28:29,248 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:28:29,248 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:28:29,248 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:28:29,248 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:28:29,248 - src.rag_pipeline - INFO - Processing validated query: 'How does artificial intelligence work?'
2025-09-05 04:28:29,265 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does artificial intelligence work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.39it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.38it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.34it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.34it/s, batch=1/1, memory=N/A]
2025-09-05 04:28:29,824 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:28:29,827 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:28:30,234 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.969s
2025-09-05 04:28:30,243 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:28:30,243 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1211 tokens) in 0.008s
2025-09-05 04:28:48,113 - src.rag_pipeline - INFO - Generated response (287 tokens) in 17.863s
2025-09-05 04:28:48,339 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:28:48,396 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:28:48,406 - src.experiment_runner - INFO - Completed run 402/510: 19.44s
2025-09-05 04:28:48,406 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:28:48,406 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:28:48,406 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:28:48,406 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:28:48,406 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:28:48,406 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:28:48,406 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:28:48,412 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:28:48,412 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:28:48,412 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:28:48,412 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:28:48,412 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:28:48,412 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:28:48,943 - src.llm_wrapper - INFO - Model ready in 0.53s
2025-09-05 04:28:48,943 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:28:48,943 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:28:48,943 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:28:48,943 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:28:48,943 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:28:48,943 - src.rag_pipeline - INFO - Processing validated query: 'Explain deep learning algorithms.'
2025-09-05 04:28:48,952 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain deep learning algorithms....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.13it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.13it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.08it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.08it/s, batch=1/1, memory=N/A]
2025-09-05 04:28:49,570 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:28:49,572 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:28:49,980 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.027s
2025-09-05 04:28:49,988 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:28:49,989 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1198 tokens) in 0.007s
2025-09-05 04:28:55,409 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.416s
2025-09-05 04:28:55,595 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:28:55,659 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:28:55,667 - src.experiment_runner - INFO - Completed run 403/510: 7.00s
2025-09-05 04:28:55,667 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:28:55,667 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:28:55,667 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:28:55,667 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:28:55,667 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:28:55,667 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:28:55,667 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:28:55,672 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:28:55,672 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:28:55,672 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:28:55,672 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:28:55,672 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:28:55,672 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:28:56,198 - src.llm_wrapper - INFO - Model ready in 0.53s
2025-09-05 04:28:56,199 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:28:56,199 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:28:56,199 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:28:56,199 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:28:56,199 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:28:56,199 - src.rag_pipeline - INFO - Processing validated query: 'What are neural networks?'
2025-09-05 04:28:56,207 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are neural networks?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.16it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.16it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.12it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.12it/s, batch=1/1, memory=N/A]
2025-09-05 04:28:56,749 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:28:56,751 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:28:57,140 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.933s
2025-09-05 04:28:57,145 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:28:57,146 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1353 tokens) in 0.005s
2025-09-05 04:29:12,569 - src.rag_pipeline - INFO - Generated response (238 tokens) in 15.416s
2025-09-05 04:29:12,794 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:29:12,851 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:29:12,860 - src.experiment_runner - INFO - Completed run 404/510: 16.90s
2025-09-05 04:29:12,860 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:29:12,860 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:29:12,860 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:29:12,860 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:29:12,860 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:29:12,860 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:29:12,860 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:29:12,866 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:29:12,866 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:29:12,866 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:29:12,866 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:29:12,866 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:29:12,866 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:29:13,393 - src.llm_wrapper - INFO - Model ready in 0.53s
2025-09-05 04:29:13,393 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:29:13,393 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:29:13,393 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:29:13,393 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:29:13,393 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:29:13,393 - src.rag_pipeline - INFO - Processing validated query: 'How do large language models work?'
2025-09-05 04:29:13,402 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How do large language models work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.07it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.04it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.02it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.02it/s, batch=1/1, memory=N/A]
2025-09-05 04:29:14,025 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:29:14,027 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:29:14,479 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.077s
2025-09-05 04:29:14,489 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:29:14,490 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1337 tokens) in 0.010s
2025-09-05 04:29:20,299 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.805s
2025-09-05 04:29:20,478 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:29:20,531 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:29:20,537 - src.experiment_runner - INFO - Completed run 405/510: 7.44s
2025-09-05 04:29:20,537 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:29:20,537 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:29:20,537 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:29:20,537 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:29:20,538 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:29:20,538 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:29:20,538 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:29:20,544 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:29:20,545 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:29:20,545 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:29:20,545 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:29:20,545 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:29:20,545 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:29:21,097 - src.llm_wrapper - INFO - Model ready in 0.55s
2025-09-05 04:29:21,097 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:29:21,097 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:29:21,097 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:29:21,097 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:29:21,097 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:29:21,097 - src.rag_pipeline - INFO - Processing validated query: 'What is natural language processing?'
2025-09-05 04:29:21,104 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is natural language processing?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.58it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.58it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.56it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.56it/s, batch=1/1, memory=N/A]
2025-09-05 04:29:21,501 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:29:21,502 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:29:21,894 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.789s
2025-09-05 04:29:21,901 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:29:21,902 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1749 tokens) in 0.007s
2025-09-05 04:29:29,341 - src.rag_pipeline - INFO - Generated response (55 tokens) in 7.435s
2025-09-05 04:29:29,522 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:29:29,581 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:29:29,588 - src.experiment_runner - INFO - Completed run 406/510: 8.80s
2025-09-05 04:29:29,588 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:29:29,588 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:29:29,589 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:29:29,589 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:29:29,589 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:29:29,589 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:29:29,589 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:29:29,595 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:29:29,595 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:29:29,595 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:29:29,595 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:29:29,596 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:29:29,596 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:29:30,131 - src.llm_wrapper - INFO - Model ready in 0.54s
2025-09-05 04:29:30,131 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:29:30,131 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:29:30,131 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:29:30,131 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:29:30,131 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:29:30,131 - src.rag_pipeline - INFO - Processing validated query: 'Explain computer vision techniques.'
2025-09-05 04:29:30,141 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain computer vision techniques....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.08it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.07it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.04it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.04it/s, batch=1/1, memory=N/A]
2025-09-05 04:29:30,824 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:29:30,826 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:29:31,231 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.090s
2025-09-05 04:29:31,242 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:29:31,243 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1297 tokens) in 0.008s
2025-09-05 04:29:51,003 - src.rag_pipeline - INFO - Generated response (318 tokens) in 19.754s
2025-09-05 04:29:51,210 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:29:51,270 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:29:51,279 - src.experiment_runner - INFO - Completed run 407/510: 21.42s
2025-09-05 04:29:51,279 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:29:51,279 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:29:51,279 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:29:51,279 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:29:51,279 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:29:51,279 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:29:51,279 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:29:51,284 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:29:51,284 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:29:51,284 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:29:51,284 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:29:51,284 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:29:51,285 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:29:51,856 - src.llm_wrapper - INFO - Model ready in 0.57s
2025-09-05 04:29:51,856 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:29:51,856 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:29:51,856 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:29:51,856 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:29:51,856 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:29:51,856 - src.rag_pipeline - INFO - Processing validated query: 'What is reinforcement learning?'
2025-09-05 04:29:51,864 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is reinforcement learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.00it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.00it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.97it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.97it/s, batch=1/1, memory=N/A]
2025-09-05 04:29:52,577 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:29:52,580 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:29:52,996 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.131s
2025-09-05 04:29:53,004 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:29:53,005 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1236 tokens) in 0.007s
2025-09-05 04:30:05,439 - src.rag_pipeline - INFO - Generated response (180 tokens) in 12.430s
2025-09-05 04:30:05,646 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:30:05,711 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:30:05,721 - src.experiment_runner - INFO - Completed run 408/510: 14.16s
2025-09-05 04:30:05,721 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:30:05,721 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:30:05,721 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:30:05,721 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:30:05,721 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:30:05,721 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:30:05,721 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:30:05,730 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:30:05,731 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:30:05,731 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:30:05,731 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:30:05,731 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:30:05,731 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:30:06,264 - src.llm_wrapper - INFO - Model ready in 0.53s
2025-09-05 04:30:06,264 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:30:06,264 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:30:06,264 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:30:06,264 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:30:06,264 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:30:06,264 - src.rag_pipeline - INFO - Processing validated query: 'How does data preprocessing work?'
2025-09-05 04:30:06,271 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does data preprocessing work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.07it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.06it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.04it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.04it/s, batch=1/1, memory=N/A]
2025-09-05 04:30:06,911 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:30:06,915 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:30:07,343 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.072s
2025-09-05 04:30:07,353 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:30:07,353 - src.rag_pipeline - INFO - Built prompt with 5 contexts (909 tokens) in 0.008s
2025-09-05 04:30:16,695 - src.rag_pipeline - INFO - Generated response (139 tokens) in 9.331s
2025-09-05 04:30:16,898 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:30:16,957 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:30:16,964 - src.experiment_runner - INFO - Completed run 409/510: 10.97s
2025-09-05 04:30:16,964 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:30:16,964 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:30:16,965 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:30:16,965 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:30:16,965 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:30:16,965 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:30:16,965 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:30:16,970 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:30:16,970 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:30:16,970 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:30:16,970 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:30:16,970 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:30:16,970 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:30:17,484 - src.llm_wrapper - INFO - Model ready in 0.51s
2025-09-05 04:30:17,484 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:30:17,484 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:30:17,484 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:30:17,484 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:30:17,484 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:30:17,484 - src.rag_pipeline - INFO - Processing validated query: 'What are the challenges in AI development?'
2025-09-05 04:30:17,492 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are the challenges in AI development?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.93it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.92it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.90it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.90it/s, batch=1/1, memory=N/A]
2025-09-05 04:30:18,140 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:30:18,142 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:30:18,544 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.052s
2025-09-05 04:30:18,548 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:30:18,549 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1445 tokens) in 0.004s
2025-09-05 04:30:34,321 - src.rag_pipeline - INFO - Generated response (237 tokens) in 15.765s
2025-09-05 04:30:34,503 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:30:34,556 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:30:34,563 - src.experiment_runner - INFO - Completed run 410/510: 17.36s
2025-09-05 04:30:34,563 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:30:34,563 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:30:34,563 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:30:34,563 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:30:34,563 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:30:34,563 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:30:34,563 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:30:34,570 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:30:34,570 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:30:34,570 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:30:34,570 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:30:34,571 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:30:34,571 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:30:35,102 - src.llm_wrapper - INFO - Model ready in 0.53s
2025-09-05 04:30:35,102 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:30:35,102 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:30:35,102 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:30:35,102 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:30:35,102 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:30:35,102 - src.rag_pipeline - INFO - Processing validated query: 'What is machine learning?'
2025-09-05 04:30:35,109 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is machine learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.35it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.34it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.32it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.32it/s, batch=1/1, memory=N/A]
2025-09-05 04:30:35,525 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:30:35,526 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:30:35,902 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.792s
2025-09-05 04:30:35,911 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:30:35,911 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1162 tokens) in 0.007s
2025-09-05 04:30:41,833 - src.rag_pipeline - INFO - Generated response (59 tokens) in 5.917s
2025-09-05 04:30:42,019 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:30:42,074 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:30:42,081 - src.experiment_runner - INFO - Completed run 411/510: 7.27s
2025-09-05 04:30:42,081 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:30:42,081 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:30:42,081 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:30:42,081 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:30:42,081 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:30:42,081 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:30:42,081 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:30:42,087 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:30:42,087 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:30:42,087 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:30:42,087 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:30:42,087 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:30:42,087 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:30:42,613 - src.llm_wrapper - INFO - Model ready in 0.53s
2025-09-05 04:30:42,614 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:30:42,614 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:30:42,614 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:30:42,614 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:30:42,614 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:30:42,614 - src.rag_pipeline - INFO - Processing validated query: 'How does artificial intelligence work?'
2025-09-05 04:30:42,621 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does artificial intelligence work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.62it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.62it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.60it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.60it/s, batch=1/1, memory=N/A]
2025-09-05 04:30:43,038 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:30:43,041 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:30:43,435 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.814s
2025-09-05 04:30:43,442 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:30:43,444 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1211 tokens) in 0.006s
2025-09-05 04:31:01,029 - src.rag_pipeline - INFO - Generated response (287 tokens) in 17.576s
2025-09-05 04:31:01,220 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:31:01,273 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:31:01,280 - src.experiment_runner - INFO - Completed run 412/510: 18.95s
2025-09-05 04:31:01,280 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:31:01,280 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:31:01,280 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:31:01,280 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:31:01,280 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:31:01,280 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:31:01,280 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:31:01,286 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:31:01,286 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:31:01,286 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:31:01,286 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:31:01,286 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:31:01,286 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:31:01,836 - src.llm_wrapper - INFO - Model ready in 0.55s
2025-09-05 04:31:01,836 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:31:01,836 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:31:01,836 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:31:01,836 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:31:01,837 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:31:01,837 - src.rag_pipeline - INFO - Processing validated query: 'Explain deep learning algorithms.'
2025-09-05 04:31:01,844 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain deep learning algorithms....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.28it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.28it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.26it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.26it/s, batch=1/1, memory=N/A]
2025-09-05 04:31:02,307 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:31:02,309 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:31:02,735 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.890s
2025-09-05 04:31:02,745 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:31:02,746 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1198 tokens) in 0.009s
2025-09-05 04:31:08,321 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.570s
2025-09-05 04:31:08,546 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:31:08,606 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:31:08,615 - src.experiment_runner - INFO - Completed run 413/510: 7.04s
2025-09-05 04:31:08,615 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:31:08,615 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:31:08,615 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:31:08,615 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:31:08,615 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:31:08,615 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:31:08,615 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:31:08,621 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:31:08,621 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:31:08,622 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:31:08,622 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:31:08,622 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:31:08,622 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:31:09,172 - src.llm_wrapper - INFO - Model ready in 0.55s
2025-09-05 04:31:09,172 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:31:09,172 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:31:09,172 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:31:09,172 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:31:09,172 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:31:09,172 - src.rag_pipeline - INFO - Processing validated query: 'What are neural networks?'
2025-09-05 04:31:09,184 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are neural networks?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.05it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.01it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.95it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.95it/s, batch=1/1, memory=N/A]
2025-09-05 04:31:09,956 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:31:09,961 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:31:10,643 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.458s
2025-09-05 04:31:10,653 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:31:10,653 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1353 tokens) in 0.010s
2025-09-05 04:31:26,416 - src.rag_pipeline - INFO - Generated response (238 tokens) in 15.757s
2025-09-05 04:31:26,637 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:31:26,700 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:31:26,755 - src.experiment_runner - INFO - Completed run 414/510: 17.80s
2025-09-05 04:31:26,756 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:31:26,756 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:31:26,756 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:31:26,756 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:31:26,756 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:31:26,756 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:31:26,756 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:31:26,761 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:31:26,762 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:31:26,762 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:31:26,762 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:31:26,762 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:31:26,762 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:31:27,339 - src.llm_wrapper - INFO - Model ready in 0.58s
2025-09-05 04:31:27,339 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:31:27,339 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:31:27,339 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:31:27,339 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:31:27,339 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:31:27,339 - src.rag_pipeline - INFO - Processing validated query: 'How do large language models work?'
2025-09-05 04:31:27,349 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How do large language models work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.84it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.82it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.78it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.77it/s, batch=1/1, memory=N/A]
2025-09-05 04:31:28,085 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:31:28,088 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:31:28,583 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.233s
2025-09-05 04:31:28,598 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:31:28,598 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1337 tokens) in 0.013s
2025-09-05 04:31:34,395 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.793s
2025-09-05 04:31:34,570 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:31:34,625 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:31:34,632 - src.experiment_runner - INFO - Completed run 415/510: 7.64s
2025-09-05 04:31:34,632 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:31:34,632 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:31:34,632 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:31:34,632 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:31:34,632 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:31:34,632 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:31:34,632 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:31:34,638 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:31:34,639 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:31:34,639 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:31:34,639 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:31:34,639 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:31:34,639 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:31:35,170 - src.llm_wrapper - INFO - Model ready in 0.53s
2025-09-05 04:31:35,170 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:31:35,170 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:31:35,170 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:31:35,170 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:31:35,170 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:31:35,170 - src.rag_pipeline - INFO - Processing validated query: 'What is natural language processing?'
2025-09-05 04:31:35,177 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is natural language processing?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.26it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.25it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.22it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.22it/s, batch=1/1, memory=N/A]
2025-09-05 04:31:35,730 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:31:35,731 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:31:36,147 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.969s
2025-09-05 04:31:36,156 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:31:36,157 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1749 tokens) in 0.007s
2025-09-05 04:31:43,512 - src.rag_pipeline - INFO - Generated response (55 tokens) in 7.351s
2025-09-05 04:31:43,685 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:31:43,743 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:31:43,751 - src.experiment_runner - INFO - Completed run 416/510: 8.88s
2025-09-05 04:31:43,751 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:31:43,751 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:31:43,751 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:31:43,751 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:31:43,751 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:31:43,751 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:31:43,751 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:31:43,757 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:31:43,758 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:31:43,758 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:31:43,758 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:31:43,758 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:31:43,758 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:31:44,300 - src.llm_wrapper - INFO - Model ready in 0.54s
2025-09-05 04:31:44,300 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:31:44,300 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:31:44,300 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:31:44,300 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:31:44,300 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:31:44,300 - src.rag_pipeline - INFO - Processing validated query: 'Explain computer vision techniques.'
2025-09-05 04:31:44,307 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain computer vision techniques....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.24it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.24it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.21it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.21it/s, batch=1/1, memory=N/A]
2025-09-05 04:31:44,849 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:31:44,851 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:31:45,255 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.948s
2025-09-05 04:31:45,266 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:31:45,266 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1297 tokens) in 0.010s
2025-09-05 04:32:04,948 - src.rag_pipeline - INFO - Generated response (318 tokens) in 19.675s
2025-09-05 04:32:05,143 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:32:05,225 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:32:05,234 - src.experiment_runner - INFO - Completed run 417/510: 21.20s
2025-09-05 04:32:05,234 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:32:05,234 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:32:05,234 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:32:05,234 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:32:05,234 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:32:05,234 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:32:05,234 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:32:05,239 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:32:05,240 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:32:05,240 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:32:05,240 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:32:05,240 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:32:05,240 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:32:05,786 - src.llm_wrapper - INFO - Model ready in 0.55s
2025-09-05 04:32:05,786 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:32:05,786 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:32:05,786 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:32:05,786 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:32:05,786 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:32:05,786 - src.rag_pipeline - INFO - Processing validated query: 'What is reinforcement learning?'
2025-09-05 04:32:05,793 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is reinforcement learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.97it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.95it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.93it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.93it/s, batch=1/1, memory=N/A]
2025-09-05 04:32:06,398 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:32:06,400 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:32:06,852 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.058s
2025-09-05 04:32:06,859 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:32:06,860 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1236 tokens) in 0.006s
2025-09-05 04:32:19,515 - src.rag_pipeline - INFO - Generated response (180 tokens) in 12.640s
2025-09-05 04:32:19,763 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:32:19,827 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:32:19,836 - src.experiment_runner - INFO - Completed run 418/510: 14.28s
2025-09-05 04:32:19,836 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:32:19,836 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:32:19,836 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:32:19,836 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:32:19,836 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:32:19,836 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:32:19,836 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:32:19,842 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:32:19,843 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:32:19,843 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:32:19,843 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:32:19,843 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:32:19,843 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:32:20,458 - src.llm_wrapper - INFO - Model ready in 0.61s
2025-09-05 04:32:20,459 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:32:20,459 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:32:20,459 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:32:20,459 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:32:20,459 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:32:20,459 - src.rag_pipeline - INFO - Processing validated query: 'How does data preprocessing work?'
2025-09-05 04:32:20,469 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does data preprocessing work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.96it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.95it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.90it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.90it/s, batch=1/1, memory=N/A]
2025-09-05 04:32:21,334 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:32:21,337 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:32:21,838 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.369s
2025-09-05 04:32:21,847 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:32:21,848 - src.rag_pipeline - INFO - Built prompt with 5 contexts (909 tokens) in 0.008s
2025-09-05 04:32:31,202 - src.rag_pipeline - INFO - Generated response (139 tokens) in 9.345s
2025-09-05 04:32:31,475 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:32:31,533 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:32:31,541 - src.experiment_runner - INFO - Completed run 419/510: 11.37s
2025-09-05 04:32:31,542 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:32:31,542 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:32:31,542 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:32:31,542 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:32:31,542 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:32:31,542 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:32:31,542 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:32:31,548 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:32:31,548 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:32:31,548 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:32:31,548 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:32:31,549 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:32:31,549 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:32:32,138 - src.llm_wrapper - INFO - Model ready in 0.59s
2025-09-05 04:32:32,138 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:32:32,138 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:32:32,138 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:32:32,138 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:32:32,139 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:32:32,139 - src.rag_pipeline - INFO - Processing validated query: 'What are the challenges in AI development?'
2025-09-05 04:32:32,148 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are the challenges in AI development?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.64it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.61it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.55it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.55it/s, batch=1/1, memory=N/A]
2025-09-05 04:32:32,868 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:32:32,871 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:32:33,285 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.137s
2025-09-05 04:32:33,293 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:32:33,294 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1445 tokens) in 0.008s
2025-09-05 04:32:49,349 - src.rag_pipeline - INFO - Generated response (237 tokens) in 16.045s
2025-09-05 04:32:49,586 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:32:49,646 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:32:49,654 - src.experiment_runner - INFO - Completed run 420/510: 17.81s
2025-09-05 04:32:49,654 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:32:49,654 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:32:49,654 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:32:49,654 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:32:49,654 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:32:49,654 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:32:49,654 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:32:49,661 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:32:49,661 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:32:49,661 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:32:49,661 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:32:49,662 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:32:49,662 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:32:50,276 - src.llm_wrapper - INFO - Model ready in 0.61s
2025-09-05 04:32:50,276 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:32:50,276 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:32:50,276 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:32:50,276 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:32:50,276 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:32:50,276 - src.rag_pipeline - INFO - Processing validated query: 'What is machine learning?'
2025-09-05 04:32:50,284 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is machine learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.21it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.21it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.18it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.18it/s, batch=1/1, memory=N/A]
2025-09-05 04:32:51,112 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:32:51,116 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:32:51,580 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.295s
2025-09-05 04:32:51,592 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:32:51,593 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1162 tokens) in 0.011s
2025-09-05 04:32:57,644 - src.rag_pipeline - INFO - Generated response (59 tokens) in 6.048s
2025-09-05 04:32:57,836 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:32:57,896 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:32:57,903 - src.experiment_runner - INFO - Completed run 421/510: 7.99s
2025-09-05 04:32:57,903 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:32:57,903 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:32:57,904 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:32:57,904 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:32:57,904 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:32:57,904 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:32:57,904 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:32:57,910 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:32:57,911 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:32:57,911 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:32:57,911 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:32:57,911 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:32:57,911 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:32:58,466 - src.llm_wrapper - INFO - Model ready in 0.56s
2025-09-05 04:32:58,466 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:32:58,466 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:32:58,466 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:32:58,466 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:32:58,466 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:32:58,466 - src.rag_pipeline - INFO - Processing validated query: 'How does artificial intelligence work?'
2025-09-05 04:32:58,473 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does artificial intelligence work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.12it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.11it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.08it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.08it/s, batch=1/1, memory=N/A]
2025-09-05 04:32:59,058 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:32:59,059 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:32:59,466 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.992s
2025-09-05 04:32:59,475 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:32:59,476 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1211 tokens) in 0.008s
2025-09-05 04:33:17,188 - src.rag_pipeline - INFO - Generated response (287 tokens) in 17.706s
2025-09-05 04:33:17,399 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:33:17,460 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:33:17,468 - src.experiment_runner - INFO - Completed run 422/510: 19.29s
2025-09-05 04:33:17,469 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:33:17,469 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:33:17,469 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:33:17,469 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:33:17,469 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:33:17,469 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:33:17,469 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:33:17,476 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:33:17,476 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:33:17,476 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:33:17,476 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:33:17,476 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:33:17,477 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:33:18,068 - src.llm_wrapper - INFO - Model ready in 0.59s
2025-09-05 04:33:18,069 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:33:18,069 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:33:18,069 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:33:18,069 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:33:18,069 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:33:18,069 - src.rag_pipeline - INFO - Processing validated query: 'Explain deep learning algorithms.'
2025-09-05 04:33:18,079 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain deep learning algorithms....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.66it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.66it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.62it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.62it/s, batch=1/1, memory=N/A]
2025-09-05 04:33:18,485 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:33:18,486 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:33:18,825 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.746s
2025-09-05 04:33:18,835 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:33:18,837 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1198 tokens) in 0.011s
2025-09-05 04:33:24,442 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.600s
2025-09-05 04:33:24,664 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:33:24,724 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:33:24,731 - src.experiment_runner - INFO - Completed run 423/510: 6.97s
2025-09-05 04:33:24,731 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:33:24,731 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:33:24,731 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:33:24,731 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:33:24,731 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:33:24,731 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:33:24,731 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:33:24,737 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:33:24,738 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:33:24,738 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:33:24,738 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:33:24,738 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:33:24,738 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:33:25,301 - src.llm_wrapper - INFO - Model ready in 0.56s
2025-09-05 04:33:25,301 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:33:25,301 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:33:25,301 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:33:25,301 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:33:25,301 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:33:25,302 - src.rag_pipeline - INFO - Processing validated query: 'What are neural networks?'
2025-09-05 04:33:25,319 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are neural networks?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.62it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.60it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.55it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.55it/s, batch=1/1, memory=N/A]
2025-09-05 04:33:25,978 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:33:25,980 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:33:26,467 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.149s
2025-09-05 04:33:26,477 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:33:26,478 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1353 tokens) in 0.009s
2025-09-05 04:33:42,888 - src.rag_pipeline - INFO - Generated response (238 tokens) in 16.395s
2025-09-05 04:33:43,132 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:33:43,195 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:33:43,204 - src.experiment_runner - INFO - Completed run 424/510: 18.16s
2025-09-05 04:33:43,205 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:33:43,205 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:33:43,205 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:33:43,205 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:33:43,205 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:33:43,205 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:33:43,205 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:33:43,211 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:33:43,212 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:33:43,212 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:33:43,212 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:33:43,212 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:33:43,212 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:33:43,786 - src.llm_wrapper - INFO - Model ready in 0.57s
2025-09-05 04:33:43,786 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:33:43,786 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:33:43,786 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:33:43,786 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:33:43,786 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:33:43,786 - src.rag_pipeline - INFO - Processing validated query: 'How do large language models work?'
2025-09-05 04:33:43,797 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How do large language models work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.77it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.75it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.69it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.69it/s, batch=1/1, memory=N/A]
2025-09-05 04:33:44,607 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:33:44,612 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:33:45,049 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.251s
2025-09-05 04:33:45,056 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:33:45,057 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1337 tokens) in 0.006s
2025-09-05 04:33:51,103 - src.rag_pipeline - INFO - Generated response (47 tokens) in 6.043s
2025-09-05 04:33:51,323 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:33:51,389 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:33:51,396 - src.experiment_runner - INFO - Completed run 425/510: 7.90s
2025-09-05 04:33:51,397 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:33:51,397 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:33:51,397 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:33:51,397 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:33:51,397 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:33:51,397 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:33:51,397 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:33:51,405 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:33:51,406 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:33:51,406 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:33:51,406 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:33:51,406 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:33:51,406 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:33:51,994 - src.llm_wrapper - INFO - Model ready in 0.59s
2025-09-05 04:33:51,994 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:33:51,994 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:33:51,994 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:33:51,994 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:33:51,994 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:33:51,994 - src.rag_pipeline - INFO - Processing validated query: 'What is natural language processing?'
2025-09-05 04:33:52,004 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is natural language processing?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.16it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.15it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.12it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.12it/s, batch=1/1, memory=N/A]
2025-09-05 04:33:52,645 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:33:52,648 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:33:53,112 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.107s
2025-09-05 04:33:53,115 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:33:53,115 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1749 tokens) in 0.003s
2025-09-05 04:34:00,738 - src.rag_pipeline - INFO - Generated response (55 tokens) in 7.617s
2025-09-05 04:34:00,949 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:34:01,009 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:34:01,018 - src.experiment_runner - INFO - Completed run 426/510: 9.34s
2025-09-05 04:34:01,018 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:34:01,018 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:34:01,018 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:34:01,018 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:34:01,018 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:34:01,018 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:34:01,018 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:34:01,024 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:34:01,024 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:34:01,024 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:34:01,024 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:34:01,024 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:34:01,024 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:34:01,635 - src.llm_wrapper - INFO - Model ready in 0.61s
2025-09-05 04:34:01,635 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:34:01,635 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:34:01,635 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:34:01,635 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:34:01,635 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:34:01,635 - src.rag_pipeline - INFO - Processing validated query: 'Explain computer vision techniques.'
2025-09-05 04:34:01,642 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain computer vision techniques....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.46it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.45it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.39it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.38it/s, batch=1/1, memory=N/A]
2025-09-05 04:34:02,279 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:34:02,282 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:34:02,714 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.072s
2025-09-05 04:34:02,723 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:34:02,723 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1297 tokens) in 0.004s
2025-09-05 04:34:22,401 - src.rag_pipeline - INFO - Generated response (318 tokens) in 19.671s
2025-09-05 04:34:22,598 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:34:22,663 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:34:22,670 - src.experiment_runner - INFO - Completed run 427/510: 21.38s
2025-09-05 04:34:22,670 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:34:22,670 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:34:22,670 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:34:22,670 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:34:22,670 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:34:22,670 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:34:22,670 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:34:22,675 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:34:22,676 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:34:22,676 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:34:22,676 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:34:22,676 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:34:22,676 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:34:23,261 - src.llm_wrapper - INFO - Model ready in 0.58s
2025-09-05 04:34:23,261 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:34:23,261 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:34:23,261 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:34:23,261 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:34:23,261 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:34:23,261 - src.rag_pipeline - INFO - Processing validated query: 'What is reinforcement learning?'
2025-09-05 04:34:23,277 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is reinforcement learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.10it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.08it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.05it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.05it/s, batch=1/1, memory=N/A]
2025-09-05 04:34:23,940 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:34:23,941 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:34:24,327 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.050s
2025-09-05 04:34:24,333 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:34:24,334 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1236 tokens) in 0.005s
2025-09-05 04:34:36,655 - src.rag_pipeline - INFO - Generated response (180 tokens) in 12.313s
2025-09-05 04:34:36,884 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:34:36,947 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:34:36,956 - src.experiment_runner - INFO - Completed run 428/510: 13.99s
2025-09-05 04:34:36,956 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:34:36,956 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:34:36,956 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:34:36,956 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:34:36,956 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:34:36,956 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:34:36,956 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:34:36,964 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:34:36,965 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:34:36,965 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:34:36,965 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:34:36,965 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:34:36,965 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:34:37,527 - src.llm_wrapper - INFO - Model ready in 0.56s
2025-09-05 04:34:37,528 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:34:37,528 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:34:37,528 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:34:37,528 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:34:37,528 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:34:37,528 - src.rag_pipeline - INFO - Processing validated query: 'How does data preprocessing work?'
2025-09-05 04:34:37,537 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does data preprocessing work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.30it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.29it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.24it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.24it/s, batch=1/1, memory=N/A]
2025-09-05 04:34:38,109 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:34:38,111 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:34:38,519 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.981s
2025-09-05 04:34:38,527 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:34:38,528 - src.rag_pipeline - INFO - Built prompt with 5 contexts (909 tokens) in 0.005s
2025-09-05 04:34:47,641 - src.rag_pipeline - INFO - Generated response (139 tokens) in 9.108s
2025-09-05 04:34:47,816 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:34:47,870 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:34:47,878 - src.experiment_runner - INFO - Completed run 429/510: 10.69s
2025-09-05 04:34:47,878 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:34:47,878 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:34:47,878 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:34:47,878 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:34:47,878 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:34:47,878 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:34:47,878 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:34:47,884 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:34:47,885 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:34:47,885 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:34:47,885 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:34:47,885 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:34:47,885 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:34:48,400 - src.llm_wrapper - INFO - Model ready in 0.52s
2025-09-05 04:34:48,400 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:34:48,400 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:34:48,400 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:34:48,400 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:34:48,400 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:34:48,400 - src.rag_pipeline - INFO - Processing validated query: 'What are the challenges in AI development?'
2025-09-05 04:34:48,407 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are the challenges in AI development?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.26it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.24it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.21it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.21it/s, batch=1/1, memory=N/A]
2025-09-05 04:34:48,975 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:34:48,976 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:34:49,378 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.970s
2025-09-05 04:34:49,384 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:34:49,384 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1445 tokens) in 0.006s
2025-09-05 04:35:05,244 - src.rag_pipeline - INFO - Generated response (237 tokens) in 15.853s
2025-09-05 04:35:05,440 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:35:05,494 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:35:05,501 - src.experiment_runner - INFO - Completed run 430/510: 17.37s
2025-09-05 04:35:05,501 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:35:05,501 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:35:05,502 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:35:05,502 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:35:05,502 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:35:05,502 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:35:05,502 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:35:05,509 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:35:05,509 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:35:05,509 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:35:05,509 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:35:05,509 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:35:05,509 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:35:06,047 - src.llm_wrapper - INFO - Model ready in 0.54s
2025-09-05 04:35:06,047 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:35:06,047 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:35:06,047 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:35:06,047 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:35:06,047 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:35:06,047 - src.rag_pipeline - INFO - Processing validated query: 'What is machine learning?'
2025-09-05 04:35:06,056 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is machine learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.31it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.30it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.24it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.24it/s, batch=1/1, memory=N/A]
2025-09-05 04:35:06,626 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:35:06,628 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:35:07,046 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.990s
2025-09-05 04:35:07,053 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:35:07,054 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1162 tokens) in 0.006s
2025-09-05 04:35:13,078 - src.rag_pipeline - INFO - Generated response (59 tokens) in 6.019s
2025-09-05 04:35:13,332 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:35:13,394 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:35:13,402 - src.experiment_runner - INFO - Completed run 431/510: 7.58s
2025-09-05 04:35:13,402 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:35:13,402 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:35:13,402 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:35:13,402 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:35:13,402 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:35:13,402 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:35:13,402 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:35:13,408 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:35:13,408 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:35:13,408 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:35:13,408 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:35:13,408 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:35:13,408 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:35:13,969 - src.llm_wrapper - INFO - Model ready in 0.56s
2025-09-05 04:35:13,969 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:35:13,969 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:35:13,969 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:35:13,969 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:35:13,969 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:35:13,969 - src.rag_pipeline - INFO - Processing validated query: 'How does artificial intelligence work?'
2025-09-05 04:35:13,979 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does artificial intelligence work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.65it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.62it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.56it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.56it/s, batch=1/1, memory=N/A]
2025-09-05 04:35:14,722 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:35:14,724 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:35:15,198 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.219s
2025-09-05 04:35:15,213 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:35:15,213 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1211 tokens) in 0.013s
2025-09-05 04:35:33,104 - src.rag_pipeline - INFO - Generated response (287 tokens) in 17.879s
2025-09-05 04:35:33,303 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:35:33,361 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:35:33,369 - src.experiment_runner - INFO - Completed run 432/510: 19.70s
2025-09-05 04:35:33,369 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:35:33,369 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:35:33,369 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:35:33,369 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:35:33,369 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:35:33,369 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:35:33,369 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:35:33,377 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:35:33,377 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:35:33,377 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:35:33,377 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:35:33,378 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:35:33,378 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:35:33,915 - src.llm_wrapper - INFO - Model ready in 0.54s
2025-09-05 04:35:33,915 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:35:33,915 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:35:33,915 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:35:33,915 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:35:33,915 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:35:33,915 - src.rag_pipeline - INFO - Processing validated query: 'Explain deep learning algorithms.'
2025-09-05 04:35:33,922 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain deep learning algorithms....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.03it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.01it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.98it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.97it/s, batch=1/1, memory=N/A]
2025-09-05 04:35:34,536 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:35:34,538 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:35:34,957 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.034s
2025-09-05 04:35:34,962 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:35:34,963 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1198 tokens) in 0.005s
2025-09-05 04:35:40,488 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.517s
2025-09-05 04:35:40,770 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:35:40,830 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:35:40,840 - src.experiment_runner - INFO - Completed run 433/510: 7.12s
2025-09-05 04:35:40,840 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:35:40,840 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:35:40,840 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:35:40,840 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:35:40,840 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:35:40,840 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:35:40,840 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:35:40,846 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:35:40,847 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:35:40,847 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:35:40,847 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:35:40,847 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:35:40,847 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:35:41,420 - src.llm_wrapper - INFO - Model ready in 0.57s
2025-09-05 04:35:41,420 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:35:41,420 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:35:41,420 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:35:41,420 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:35:41,420 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:35:41,420 - src.rag_pipeline - INFO - Processing validated query: 'What are neural networks?'
2025-09-05 04:35:41,428 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are neural networks?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.78it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.77it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.73it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.73it/s, batch=1/1, memory=N/A]
2025-09-05 04:35:42,219 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:35:42,221 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:35:42,665 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.237s
2025-09-05 04:35:42,679 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:35:42,680 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1353 tokens) in 0.012s
2025-09-05 04:35:58,666 - src.rag_pipeline - INFO - Generated response (238 tokens) in 15.974s
2025-09-05 04:35:58,843 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:35:58,904 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:35:58,914 - src.experiment_runner - INFO - Completed run 434/510: 17.83s
2025-09-05 04:35:58,914 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:35:58,914 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:35:58,914 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:35:58,914 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:35:58,914 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:35:58,914 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:35:58,914 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:35:58,922 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:35:58,922 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:35:58,922 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:35:58,922 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:35:58,922 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:35:58,923 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:35:59,505 - src.llm_wrapper - INFO - Model ready in 0.58s
2025-09-05 04:35:59,505 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:35:59,505 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:35:59,505 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:35:59,505 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:35:59,505 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:35:59,505 - src.rag_pipeline - INFO - Processing validated query: 'How do large language models work?'
2025-09-05 04:35:59,513 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How do large language models work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.97it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.96it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.93it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.93it/s, batch=1/1, memory=N/A]
2025-09-05 04:36:00,150 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:36:00,154 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:36:00,581 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.068s
2025-09-05 04:36:00,589 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:36:00,591 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1337 tokens) in 0.008s
2025-09-05 04:36:06,457 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.862s
2025-09-05 04:36:06,648 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:36:06,714 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:36:06,723 - src.experiment_runner - INFO - Completed run 435/510: 7.54s
2025-09-05 04:36:06,723 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:36:06,723 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:36:06,723 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:36:06,723 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:36:06,723 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:36:06,723 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:36:06,723 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:36:06,729 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:36:06,730 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:36:06,730 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:36:06,730 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:36:06,730 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:36:06,730 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:36:07,307 - src.llm_wrapper - INFO - Model ready in 0.58s
2025-09-05 04:36:07,307 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:36:07,307 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:36:07,307 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:36:07,308 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:36:07,308 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:36:07,308 - src.rag_pipeline - INFO - Processing validated query: 'What is natural language processing?'
2025-09-05 04:36:07,315 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is natural language processing?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.41it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.41it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.39it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.39it/s, batch=1/1, memory=N/A]
2025-09-05 04:36:07,792 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:36:07,798 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:36:08,185 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.870s
2025-09-05 04:36:08,191 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:36:08,192 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1749 tokens) in 0.004s
2025-09-05 04:36:15,618 - src.rag_pipeline - INFO - Generated response (55 tokens) in 7.417s
2025-09-05 04:36:15,909 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:36:15,969 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:36:15,978 - src.experiment_runner - INFO - Completed run 436/510: 8.90s
2025-09-05 04:36:15,978 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:36:15,978 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:36:15,978 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:36:15,978 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:36:15,978 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:36:15,978 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:36:15,978 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:36:15,983 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:36:15,983 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:36:15,983 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:36:15,983 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:36:15,983 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:36:15,983 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:36:16,556 - src.llm_wrapper - INFO - Model ready in 0.57s
2025-09-05 04:36:16,556 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:36:16,556 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:36:16,557 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:36:16,557 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:36:16,557 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:36:16,557 - src.rag_pipeline - INFO - Processing validated query: 'Explain computer vision techniques.'
2025-09-05 04:36:16,564 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain computer vision techniques....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.04it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.03it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.01it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.01it/s, batch=1/1, memory=N/A]
2025-09-05 04:36:17,153 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:36:17,155 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:36:17,597 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.033s
2025-09-05 04:36:17,609 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:36:17,609 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1297 tokens) in 0.011s
2025-09-05 04:36:37,548 - src.rag_pipeline - INFO - Generated response (318 tokens) in 19.928s
2025-09-05 04:36:37,759 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:36:37,823 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:36:37,833 - src.experiment_runner - INFO - Completed run 437/510: 21.57s
2025-09-05 04:36:37,833 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:36:37,833 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:36:37,833 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:36:37,833 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:36:37,833 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:36:37,833 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:36:37,833 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:36:37,839 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:36:37,839 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:36:37,839 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:36:37,839 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:36:37,839 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:36:37,839 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:36:38,417 - src.llm_wrapper - INFO - Model ready in 0.58s
2025-09-05 04:36:38,417 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:36:38,417 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:36:38,417 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:36:38,417 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:36:38,417 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:36:38,417 - src.rag_pipeline - INFO - Processing validated query: 'What is reinforcement learning?'
2025-09-05 04:36:38,433 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is reinforcement learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.22it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.21it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.16it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.15it/s, batch=1/1, memory=N/A]
2025-09-05 04:36:39,082 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:36:39,089 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:36:39,536 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.103s
2025-09-05 04:36:39,546 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:36:39,547 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1236 tokens) in 0.008s
2025-09-05 04:36:52,216 - src.rag_pipeline - INFO - Generated response (180 tokens) in 12.664s
2025-09-05 04:36:52,438 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:36:52,496 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:36:52,505 - src.experiment_runner - INFO - Completed run 438/510: 14.39s
2025-09-05 04:36:52,505 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:36:52,505 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:36:52,505 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:36:52,505 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:36:52,505 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:36:52,505 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:36:52,505 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:36:52,511 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:36:52,511 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:36:52,511 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:36:52,511 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:36:52,511 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:36:52,511 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:36:53,073 - src.llm_wrapper - INFO - Model ready in 0.56s
2025-09-05 04:36:53,073 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:36:53,073 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:36:53,073 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:36:53,073 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:36:53,073 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:36:53,073 - src.rag_pipeline - INFO - Processing validated query: 'How does data preprocessing work?'
2025-09-05 04:36:53,090 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does data preprocessing work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.08it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.06it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.03it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.03it/s, batch=1/1, memory=N/A]
2025-09-05 04:36:53,762 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:36:53,766 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:36:54,174 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.082s
2025-09-05 04:36:54,184 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:36:54,184 - src.rag_pipeline - INFO - Built prompt with 5 contexts (909 tokens) in 0.008s
2025-09-05 04:37:03,723 - src.rag_pipeline - INFO - Generated response (139 tokens) in 9.534s
2025-09-05 04:37:03,981 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:37:04,042 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:37:04,052 - src.experiment_runner - INFO - Completed run 439/510: 11.22s
2025-09-05 04:37:04,052 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:37:04,052 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:37:04,052 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:37:04,052 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:37:04,052 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:37:04,052 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:37:04,052 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:37:04,059 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:37:04,060 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:37:04,060 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:37:04,060 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:37:04,060 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:37:04,060 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:37:04,643 - src.llm_wrapper - INFO - Model ready in 0.58s
2025-09-05 04:37:04,643 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:37:04,643 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:37:04,643 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:37:04,644 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:37:04,644 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:37:04,644 - src.rag_pipeline - INFO - Processing validated query: 'What are the challenges in AI development?'
2025-09-05 04:37:04,651 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are the challenges in AI development?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.19it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.18it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.16it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.16it/s, batch=1/1, memory=N/A]
2025-09-05 04:37:05,257 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:37:05,260 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:37:05,683 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.032s
2025-09-05 04:37:05,691 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:37:05,694 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1445 tokens) in 0.007s
2025-09-05 04:37:21,973 - src.rag_pipeline - INFO - Generated response (237 tokens) in 16.272s
2025-09-05 04:37:22,189 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:37:22,243 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:37:22,251 - src.experiment_runner - INFO - Completed run 440/510: 17.92s
2025-09-05 04:37:22,251 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:37:22,251 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:37:22,251 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:37:22,251 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:37:22,251 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:37:22,251 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:37:22,251 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:37:22,257 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:37:22,257 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:37:22,257 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:37:22,257 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:37:22,257 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:37:22,257 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:37:22,861 - src.llm_wrapper - INFO - Model ready in 0.60s
2025-09-05 04:37:22,861 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:37:22,861 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:37:22,861 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:37:22,861 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:37:22,861 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:37:22,861 - src.rag_pipeline - INFO - Processing validated query: 'What is machine learning?'
2025-09-05 04:37:22,869 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is machine learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.48it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.48it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.45it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.45it/s, batch=1/1, memory=N/A]
2025-09-05 04:37:23,685 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:37:23,688 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:37:24,174 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.303s
2025-09-05 04:37:24,183 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:37:24,184 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1162 tokens) in 0.008s
2025-09-05 04:37:30,282 - src.rag_pipeline - INFO - Generated response (59 tokens) in 6.094s
2025-09-05 04:37:30,487 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:37:30,540 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:37:30,549 - src.experiment_runner - INFO - Completed run 441/510: 8.03s
2025-09-05 04:37:30,550 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:37:30,550 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:37:30,550 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:37:30,550 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:37:30,550 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:37:30,550 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:37:30,550 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:37:30,555 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:37:30,556 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:37:30,556 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:37:30,556 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:37:30,556 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:37:30,556 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:37:31,100 - src.llm_wrapper - INFO - Model ready in 0.54s
2025-09-05 04:37:31,100 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:37:31,100 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:37:31,100 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:37:31,100 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:37:31,100 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:37:31,100 - src.rag_pipeline - INFO - Processing validated query: 'How does artificial intelligence work?'
2025-09-05 04:37:31,108 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does artificial intelligence work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.03it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.03it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.99it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.99it/s, batch=1/1, memory=N/A]
2025-09-05 04:37:31,708 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:37:31,709 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:37:32,128 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.019s
2025-09-05 04:37:32,137 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:37:32,139 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1211 tokens) in 0.008s
2025-09-05 04:37:50,227 - src.rag_pipeline - INFO - Generated response (287 tokens) in 18.080s
2025-09-05 04:37:50,456 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:37:50,511 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:37:50,518 - src.experiment_runner - INFO - Completed run 442/510: 19.68s
2025-09-05 04:37:50,518 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:37:50,518 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:37:50,518 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:37:50,518 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:37:50,518 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:37:50,518 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:37:50,518 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:37:50,525 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:37:50,525 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:37:50,525 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:37:50,526 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:37:50,526 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:37:50,526 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:37:51,152 - src.llm_wrapper - INFO - Model ready in 0.63s
2025-09-05 04:37:51,152 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:37:51,152 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:37:51,152 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:37:51,152 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:37:51,152 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:37:51,152 - src.rag_pipeline - INFO - Processing validated query: 'Explain deep learning algorithms.'
2025-09-05 04:37:51,160 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain deep learning algorithms....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.13it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.12it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.08it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.08it/s, batch=1/1, memory=N/A]
2025-09-05 04:37:51,804 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:37:51,807 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:37:52,258 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.096s
2025-09-05 04:37:52,269 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:37:52,270 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1198 tokens) in 0.009s
2025-09-05 04:37:57,785 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.511s
2025-09-05 04:37:57,971 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:37:58,031 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:37:58,039 - src.experiment_runner - INFO - Completed run 443/510: 7.27s
2025-09-05 04:37:58,039 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:37:58,039 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:37:58,039 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:37:58,039 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:37:58,039 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:37:58,039 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:37:58,039 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:37:58,045 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:37:58,045 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:37:58,045 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:37:58,045 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:37:58,046 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:37:58,046 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:37:58,572 - src.llm_wrapper - INFO - Model ready in 0.53s
2025-09-05 04:37:58,572 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:37:58,572 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:37:58,572 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:37:58,572 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:37:58,573 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:37:58,573 - src.rag_pipeline - INFO - Processing validated query: 'What are neural networks?'
2025-09-05 04:37:58,581 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are neural networks?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.06it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.05it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.03it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.03it/s, batch=1/1, memory=N/A]
2025-09-05 04:37:59,168 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:37:59,171 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:37:59,561 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.979s
2025-09-05 04:37:59,571 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:37:59,572 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1353 tokens) in 0.009s
2025-09-05 04:38:15,354 - src.rag_pipeline - INFO - Generated response (238 tokens) in 15.775s
2025-09-05 04:38:15,555 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:38:15,614 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:38:15,622 - src.experiment_runner - INFO - Completed run 444/510: 17.32s
2025-09-05 04:38:15,622 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:38:15,622 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:38:15,622 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:38:15,622 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:38:15,622 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:38:15,622 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:38:15,622 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:38:15,628 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:38:15,628 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:38:15,628 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:38:15,629 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:38:15,629 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:38:15,629 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:38:16,212 - src.llm_wrapper - INFO - Model ready in 0.58s
2025-09-05 04:38:16,212 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:38:16,213 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:38:16,213 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:38:16,213 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:38:16,213 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:38:16,213 - src.rag_pipeline - INFO - Processing validated query: 'How do large language models work?'
2025-09-05 04:38:16,223 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How do large language models work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.10it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.09it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.06it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.05it/s, batch=1/1, memory=N/A]
2025-09-05 04:38:16,834 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:38:16,836 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:38:17,264 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.040s
2025-09-05 04:38:17,273 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:38:17,273 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1337 tokens) in 0.007s
2025-09-05 04:38:23,108 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.831s
2025-09-05 04:38:23,350 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:38:23,412 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:38:23,420 - src.experiment_runner - INFO - Completed run 445/510: 7.49s
2025-09-05 04:38:23,420 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:38:23,420 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:38:23,420 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:38:23,420 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:38:23,420 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:38:23,420 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:38:23,420 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:38:23,426 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:38:23,427 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:38:23,427 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:38:23,427 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:38:23,427 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:38:23,427 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:38:24,050 - src.llm_wrapper - INFO - Model ready in 0.62s
2025-09-05 04:38:24,051 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:38:24,051 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:38:24,051 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:38:24,051 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:38:24,051 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:38:24,051 - src.rag_pipeline - INFO - Processing validated query: 'What is natural language processing?'
2025-09-05 04:38:24,059 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is natural language processing?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.08it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.06it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.05it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.05it/s, batch=1/1, memory=N/A]
2025-09-05 04:38:24,770 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:38:24,772 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:38:25,170 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.110s
2025-09-05 04:38:25,178 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:38:25,179 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1749 tokens) in 0.008s
2025-09-05 04:38:32,560 - src.rag_pipeline - INFO - Generated response (55 tokens) in 7.377s
2025-09-05 04:38:32,762 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:38:32,815 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:38:32,824 - src.experiment_runner - INFO - Completed run 446/510: 9.14s
2025-09-05 04:38:32,824 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:38:32,824 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:38:32,824 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:38:32,824 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:38:32,824 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:38:32,824 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:38:32,824 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:38:32,830 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:38:32,830 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:38:32,830 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:38:32,830 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:38:32,831 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:38:32,831 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:38:33,373 - src.llm_wrapper - INFO - Model ready in 0.54s
2025-09-05 04:38:33,373 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:38:33,373 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:38:33,373 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:38:33,373 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:38:33,373 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:38:33,373 - src.rag_pipeline - INFO - Processing validated query: 'Explain computer vision techniques.'
2025-09-05 04:38:33,381 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain computer vision techniques....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.29it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.28it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.23it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.23it/s, batch=1/1, memory=N/A]
2025-09-05 04:38:33,979 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:38:33,980 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:38:34,389 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.007s
2025-09-05 04:38:34,396 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:38:34,396 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1297 tokens) in 0.007s
2025-09-05 04:38:54,346 - src.rag_pipeline - INFO - Generated response (318 tokens) in 19.936s
2025-09-05 04:38:54,556 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:38:54,615 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:38:54,623 - src.experiment_runner - INFO - Completed run 447/510: 21.52s
2025-09-05 04:38:54,623 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:38:54,623 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:38:54,623 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:38:54,623 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:38:54,623 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:38:54,623 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:38:54,623 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:38:54,628 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:38:54,629 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:38:54,629 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:38:54,629 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:38:54,629 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:38:54,629 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:38:55,189 - src.llm_wrapper - INFO - Model ready in 0.56s
2025-09-05 04:38:55,189 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:38:55,189 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:38:55,189 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:38:55,189 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:38:55,189 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:38:55,189 - src.rag_pipeline - INFO - Processing validated query: 'What is reinforcement learning?'
2025-09-05 04:38:55,198 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is reinforcement learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.13it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.12it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.08it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.08it/s, batch=1/1, memory=N/A]
2025-09-05 04:38:55,829 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:38:55,831 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:38:56,273 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.074s
2025-09-05 04:38:56,285 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:38:56,286 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1236 tokens) in 0.012s
2025-09-05 04:39:08,796 - src.rag_pipeline - INFO - Generated response (180 tokens) in 12.499s
2025-09-05 04:39:09,034 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:39:09,091 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:39:09,097 - src.experiment_runner - INFO - Completed run 448/510: 14.17s
2025-09-05 04:39:09,097 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:39:09,097 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:39:09,097 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:39:09,098 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:39:09,098 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:39:09,098 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:39:09,098 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:39:09,103 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:39:09,103 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:39:09,103 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:39:09,103 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:39:09,104 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:39:09,104 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:39:09,693 - src.llm_wrapper - INFO - Model ready in 0.59s
2025-09-05 04:39:09,693 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:39:09,693 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:39:09,693 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:39:09,693 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:39:09,693 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:39:09,693 - src.rag_pipeline - INFO - Processing validated query: 'How does data preprocessing work?'
2025-09-05 04:39:09,701 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does data preprocessing work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.35it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.32it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.27it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.27it/s, batch=1/1, memory=N/A]
2025-09-05 04:39:10,780 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:39:10,782 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:39:11,227 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.524s
2025-09-05 04:39:11,239 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:39:11,239 - src.rag_pipeline - INFO - Built prompt with 5 contexts (909 tokens) in 0.009s
2025-09-05 04:39:20,690 - src.rag_pipeline - INFO - Generated response (139 tokens) in 9.445s
2025-09-05 04:39:20,888 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:39:20,944 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:39:20,951 - src.experiment_runner - INFO - Completed run 449/510: 11.59s
2025-09-05 04:39:20,952 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:39:20,952 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:39:20,952 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:39:20,952 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:39:20,952 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:39:20,952 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:39:20,952 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:39:20,957 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:39:20,957 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:39:20,957 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:39:20,957 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:39:20,958 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:39:20,958 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:39:21,514 - src.llm_wrapper - INFO - Model ready in 0.56s
2025-09-05 04:39:21,514 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:39:21,514 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:39:21,514 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:39:21,514 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:39:21,514 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:39:21,514 - src.rag_pipeline - INFO - Processing validated query: 'What are the challenges in AI development?'
2025-09-05 04:39:21,529 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are the challenges in AI development?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.04it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.03it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.00it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.00it/s, batch=1/1, memory=N/A]
2025-09-05 04:39:22,179 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:39:22,183 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:39:22,597 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.067s
2025-09-05 04:39:22,603 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:39:22,604 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1445 tokens) in 0.006s
2025-09-05 04:39:38,602 - src.rag_pipeline - INFO - Generated response (237 tokens) in 15.992s
2025-09-05 04:39:38,799 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:39:38,858 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:39:38,866 - src.experiment_runner - INFO - Completed run 450/510: 17.65s
2025-09-05 04:39:38,866 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:39:38,866 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:39:38,866 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:39:38,866 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:39:38,866 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:39:38,866 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:39:38,866 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:39:38,872 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:39:38,872 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:39:38,872 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:39:38,872 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:39:38,872 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:39:38,873 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:39:39,410 - src.llm_wrapper - INFO - Model ready in 0.54s
2025-09-05 04:39:39,410 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:39:39,410 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:39:39,410 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:39:39,410 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:39:39,410 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:39:39,410 - src.rag_pipeline - INFO - Processing validated query: 'What is machine learning?'
2025-09-05 04:39:39,421 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is machine learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.16it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.15it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.11it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.11it/s, batch=1/1, memory=N/A]
2025-09-05 04:39:40,017 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:39:40,020 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:39:40,429 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.007s
2025-09-05 04:39:40,444 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:39:40,446 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1162 tokens) in 0.013s
2025-09-05 04:39:46,651 - src.rag_pipeline - INFO - Generated response (59 tokens) in 6.201s
2025-09-05 04:39:46,911 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:39:46,975 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:39:46,984 - src.experiment_runner - INFO - Completed run 451/510: 7.79s
2025-09-05 04:39:46,984 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:39:46,984 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:39:46,984 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:39:46,984 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:39:46,984 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:39:46,984 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:39:46,984 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:39:46,990 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:39:46,991 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:39:46,991 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:39:46,991 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:39:46,991 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:39:46,991 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:39:47,600 - src.llm_wrapper - INFO - Model ready in 0.61s
2025-09-05 04:39:47,600 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:39:47,600 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:39:47,600 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:39:47,600 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:39:47,600 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:39:47,600 - src.rag_pipeline - INFO - Processing validated query: 'How does artificial intelligence work?'
2025-09-05 04:39:47,611 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does artificial intelligence work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.42it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.39it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.35it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.35it/s, batch=1/1, memory=N/A]
2025-09-05 04:39:48,383 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:39:48,387 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:39:48,842 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.231s
2025-09-05 04:39:48,852 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:39:48,853 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1211 tokens) in 0.009s
2025-09-05 04:40:07,096 - src.rag_pipeline - INFO - Generated response (287 tokens) in 18.236s
2025-09-05 04:40:07,325 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:40:07,379 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:40:07,387 - src.experiment_runner - INFO - Completed run 452/510: 20.11s
2025-09-05 04:40:07,387 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:40:07,387 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:40:07,388 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:40:07,388 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:40:07,388 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:40:07,388 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:40:07,388 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:40:07,393 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:40:07,393 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:40:07,393 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:40:07,393 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:40:07,394 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:40:07,394 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:40:07,997 - src.llm_wrapper - INFO - Model ready in 0.60s
2025-09-05 04:40:07,997 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:40:07,997 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:40:07,997 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:40:07,997 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:40:07,998 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:40:07,998 - src.rag_pipeline - INFO - Processing validated query: 'Explain deep learning algorithms.'
2025-09-05 04:40:08,007 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain deep learning algorithms....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.98it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.97it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.96it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.96it/s, batch=1/1, memory=N/A]
2025-09-05 04:40:08,699 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:40:08,701 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:40:09,160 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.152s
2025-09-05 04:40:09,169 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:40:09,170 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1198 tokens) in 0.008s
2025-09-05 04:40:14,716 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.542s
2025-09-05 04:40:14,943 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:40:15,005 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:40:15,013 - src.experiment_runner - INFO - Completed run 453/510: 7.33s
2025-09-05 04:40:15,013 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:40:15,013 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:40:15,013 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:40:15,013 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:40:15,013 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:40:15,013 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:40:15,013 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:40:15,021 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:40:15,022 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:40:15,022 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:40:15,022 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:40:15,022 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:40:15,022 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:40:15,559 - src.llm_wrapper - INFO - Model ready in 0.54s
2025-09-05 04:40:15,560 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:40:15,560 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:40:15,560 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:40:15,560 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:40:15,560 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:40:15,560 - src.rag_pipeline - INFO - Processing validated query: 'What are neural networks?'
2025-09-05 04:40:15,567 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are neural networks?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.90it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.88it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.84it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.84it/s, batch=1/1, memory=N/A]
2025-09-05 04:40:16,349 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:40:16,352 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:40:16,791 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.223s
2025-09-05 04:40:16,800 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:40:16,801 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1353 tokens) in 0.009s
2025-09-05 04:40:32,514 - src.rag_pipeline - INFO - Generated response (238 tokens) in 15.706s
2025-09-05 04:40:32,738 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:40:32,797 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:40:32,806 - src.experiment_runner - INFO - Completed run 454/510: 17.50s
2025-09-05 04:40:32,807 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:40:32,807 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:40:32,807 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:40:32,807 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:40:32,807 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:40:32,807 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:40:32,807 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:40:32,812 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:40:32,813 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:40:32,813 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:40:32,813 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:40:32,813 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:40:32,813 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:40:33,367 - src.llm_wrapper - INFO - Model ready in 0.55s
2025-09-05 04:40:33,367 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:40:33,367 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:40:33,367 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:40:33,367 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:40:33,367 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:40:33,367 - src.rag_pipeline - INFO - Processing validated query: 'How do large language models work?'
2025-09-05 04:40:33,375 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How do large language models work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.39it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.37it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.32it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.31it/s, batch=1/1, memory=N/A]
2025-09-05 04:40:34,038 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:40:34,041 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:40:34,499 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.124s
2025-09-05 04:40:34,507 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:40:34,508 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1337 tokens) in 0.008s
2025-09-05 04:40:40,314 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.802s
2025-09-05 04:40:40,498 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:40:40,561 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:40:40,569 - src.experiment_runner - INFO - Completed run 455/510: 7.51s
2025-09-05 04:40:40,569 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:40:40,569 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:40:40,569 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:40:40,569 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:40:40,569 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:40:40,569 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:40:40,569 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:40:40,576 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:40:40,577 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:40:40,577 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:40:40,577 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:40:40,577 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:40:40,577 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:40:41,164 - src.llm_wrapper - INFO - Model ready in 0.59s
2025-09-05 04:40:41,164 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:40:41,164 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:40:41,164 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:40:41,164 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:40:41,164 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:40:41,164 - src.rag_pipeline - INFO - Processing validated query: 'What is natural language processing?'
2025-09-05 04:40:41,173 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is natural language processing?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.02it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.01it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.97it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.97it/s, batch=1/1, memory=N/A]
2025-09-05 04:40:41,793 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:40:41,795 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:40:42,216 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.042s
2025-09-05 04:40:42,225 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:40:42,226 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1749 tokens) in 0.008s
2025-09-05 04:40:49,644 - src.rag_pipeline - INFO - Generated response (55 tokens) in 7.414s
2025-09-05 04:40:49,819 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:40:49,878 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:40:49,885 - src.experiment_runner - INFO - Completed run 456/510: 9.08s
2025-09-05 04:40:49,885 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:40:49,885 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:40:49,885 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:40:49,885 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:40:49,885 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:40:49,885 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:40:49,885 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:40:49,890 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:40:49,891 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:40:49,891 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:40:49,891 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:40:49,891 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:40:49,891 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:40:50,413 - src.llm_wrapper - INFO - Model ready in 0.52s
2025-09-05 04:40:50,413 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:40:50,413 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:40:50,413 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:40:50,413 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:40:50,413 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:40:50,413 - src.rag_pipeline - INFO - Processing validated query: 'Explain computer vision techniques.'
2025-09-05 04:40:50,420 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain computer vision techniques....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.06it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.04it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.01it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.01it/s, batch=1/1, memory=N/A]
2025-09-05 04:40:51,011 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:40:51,013 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:40:51,458 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.037s
2025-09-05 04:40:51,471 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:40:51,472 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1297 tokens) in 0.011s
2025-09-05 04:41:11,858 - src.rag_pipeline - INFO - Generated response (318 tokens) in 20.378s
2025-09-05 04:41:12,095 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:41:12,150 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:41:12,159 - src.experiment_runner - INFO - Completed run 457/510: 21.97s
2025-09-05 04:41:12,159 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:41:12,159 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:41:12,159 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:41:12,159 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:41:12,159 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:41:12,159 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:41:12,159 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:41:12,164 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:41:12,164 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:41:12,164 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:41:12,164 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:41:12,165 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:41:12,165 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:41:12,764 - src.llm_wrapper - INFO - Model ready in 0.60s
2025-09-05 04:41:12,764 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:41:12,764 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:41:12,764 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:41:12,764 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:41:12,764 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:41:12,764 - src.rag_pipeline - INFO - Processing validated query: 'What is reinforcement learning?'
2025-09-05 04:41:12,773 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is reinforcement learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.82it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.81it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.77it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.77it/s, batch=1/1, memory=N/A]
2025-09-05 04:41:13,643 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:41:13,647 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:41:14,080 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.307s
2025-09-05 04:41:14,089 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:41:14,090 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1236 tokens) in 0.007s
2025-09-05 04:41:26,680 - src.rag_pipeline - INFO - Generated response (180 tokens) in 12.584s
2025-09-05 04:41:26,907 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:41:26,963 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:41:26,972 - src.experiment_runner - INFO - Completed run 458/510: 14.52s
2025-09-05 04:41:26,972 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:41:26,972 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:41:26,972 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:41:26,972 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:41:26,972 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:41:26,972 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:41:26,972 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:41:26,978 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:41:26,978 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:41:26,978 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:41:26,978 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:41:26,978 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:41:26,978 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:41:27,564 - src.llm_wrapper - INFO - Model ready in 0.59s
2025-09-05 04:41:27,564 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:41:27,564 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:41:27,564 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:41:27,564 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:41:27,564 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:41:27,564 - src.rag_pipeline - INFO - Processing validated query: 'How does data preprocessing work?'
2025-09-05 04:41:27,575 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does data preprocessing work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.97it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.97it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.95it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.95it/s, batch=1/1, memory=N/A]
2025-09-05 04:41:28,178 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:41:28,182 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:41:28,613 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.037s
2025-09-05 04:41:28,622 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:41:28,623 - src.rag_pipeline - INFO - Built prompt with 5 contexts (909 tokens) in 0.008s
2025-09-05 04:41:38,055 - src.rag_pipeline - INFO - Generated response (139 tokens) in 9.426s
2025-09-05 04:41:38,277 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:41:38,338 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:41:38,346 - src.experiment_runner - INFO - Completed run 459/510: 11.08s
2025-09-05 04:41:38,347 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:41:38,347 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:41:38,347 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:41:38,347 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:41:38,347 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:41:38,347 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:41:38,347 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:41:38,355 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:41:38,356 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:41:38,356 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:41:38,356 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:41:38,356 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:41:38,356 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:41:38,911 - src.llm_wrapper - INFO - Model ready in 0.56s
2025-09-05 04:41:38,911 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:41:38,911 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:41:38,911 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:41:38,912 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:41:38,912 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:41:38,912 - src.rag_pipeline - INFO - Processing validated query: 'What are the challenges in AI development?'
2025-09-05 04:41:38,919 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are the challenges in AI development?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.83it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.83it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.81it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.81it/s, batch=1/1, memory=N/A]
2025-09-05 04:41:39,568 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:41:39,572 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:41:39,972 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.053s
2025-09-05 04:41:39,983 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:41:39,984 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1445 tokens) in 0.010s
2025-09-05 04:41:56,372 - src.rag_pipeline - INFO - Generated response (237 tokens) in 16.382s
2025-09-05 04:41:56,579 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:41:56,635 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:41:56,643 - src.experiment_runner - INFO - Completed run 460/510: 18.03s
2025-09-05 04:41:56,644 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:41:56,644 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:41:56,644 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:41:56,644 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:41:56,644 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:41:56,644 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:41:56,644 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:41:56,650 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:41:56,651 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:41:56,651 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:41:56,651 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:41:56,651 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:41:56,651 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:41:57,205 - src.llm_wrapper - INFO - Model ready in 0.55s
2025-09-05 04:41:57,205 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:41:57,205 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:41:57,205 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:41:57,205 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:41:57,205 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:41:57,205 - src.rag_pipeline - INFO - Processing validated query: 'What is machine learning?'
2025-09-05 04:41:57,212 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is machine learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.19it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.18it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.15it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.15it/s, batch=1/1, memory=N/A]
2025-09-05 04:41:57,885 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:41:57,887 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:41:58,327 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.114s
2025-09-05 04:41:58,337 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:41:58,338 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1162 tokens) in 0.009s
2025-09-05 04:42:04,432 - src.rag_pipeline - INFO - Generated response (59 tokens) in 6.089s
2025-09-05 04:42:04,623 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:42:04,681 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:42:04,689 - src.experiment_runner - INFO - Completed run 461/510: 7.79s
2025-09-05 04:42:04,690 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:42:04,690 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:42:04,690 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:42:04,690 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:42:04,690 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:42:04,690 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:42:04,690 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:42:04,695 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:42:04,695 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:42:04,695 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:42:04,695 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:42:04,695 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:42:04,695 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:42:05,237 - src.llm_wrapper - INFO - Model ready in 0.54s
2025-09-05 04:42:05,238 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:42:05,238 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:42:05,238 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:42:05,238 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:42:05,238 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:42:05,238 - src.rag_pipeline - INFO - Processing validated query: 'How does artificial intelligence work?'
2025-09-05 04:42:05,245 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does artificial intelligence work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.01it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.00it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.98it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.98it/s, batch=1/1, memory=N/A]
2025-09-05 04:42:05,853 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:42:05,855 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:42:06,282 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.037s
2025-09-05 04:42:06,293 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:42:06,293 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1211 tokens) in 0.009s
2025-09-05 04:42:24,300 - src.rag_pipeline - INFO - Generated response (287 tokens) in 18.002s
2025-09-05 04:42:24,517 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:42:24,570 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:42:24,583 - src.experiment_runner - INFO - Completed run 462/510: 19.61s
2025-09-05 04:42:24,583 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:42:24,583 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:42:24,583 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:42:24,583 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:42:24,583 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:42:24,583 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:42:24,583 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:42:24,590 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:42:24,590 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:42:24,590 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:42:24,590 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:42:24,591 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:42:24,591 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:42:25,136 - src.llm_wrapper - INFO - Model ready in 0.55s
2025-09-05 04:42:25,136 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:42:25,136 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:42:25,136 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:42:25,136 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:42:25,136 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:42:25,136 - src.rag_pipeline - INFO - Processing validated query: 'Explain deep learning algorithms.'
2025-09-05 04:42:25,143 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain deep learning algorithms....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.05it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.04it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.02it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.02it/s, batch=1/1, memory=N/A]
2025-09-05 04:42:25,791 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:42:25,793 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:42:26,216 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.072s
2025-09-05 04:42:26,228 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:42:26,229 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1198 tokens) in 0.010s
2025-09-05 04:42:31,759 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.527s
2025-09-05 04:42:31,942 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:42:31,998 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:42:32,008 - src.experiment_runner - INFO - Completed run 463/510: 7.18s
2025-09-05 04:42:32,008 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:42:32,008 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:42:32,008 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:42:32,008 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:42:32,008 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:42:32,008 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:42:32,008 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:42:32,013 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:42:32,013 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:42:32,013 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:42:32,013 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:42:32,014 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:42:32,014 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:42:32,583 - src.llm_wrapper - INFO - Model ready in 0.57s
2025-09-05 04:42:32,583 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:42:32,583 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:42:32,583 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:42:32,583 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:42:32,583 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:42:32,583 - src.rag_pipeline - INFO - Processing validated query: 'What are neural networks?'
2025-09-05 04:42:32,590 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are neural networks?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.25it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.24it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.21it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.21it/s, batch=1/1, memory=N/A]
2025-09-05 04:42:33,062 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:42:33,064 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:42:33,468 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.878s
2025-09-05 04:42:33,475 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:42:33,477 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1353 tokens) in 0.006s
2025-09-05 04:42:49,439 - src.rag_pipeline - INFO - Generated response (238 tokens) in 15.948s
2025-09-05 04:42:49,666 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:42:49,734 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:42:49,742 - src.experiment_runner - INFO - Completed run 464/510: 17.43s
2025-09-05 04:42:49,742 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:42:49,742 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:42:49,742 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:42:49,743 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:42:49,743 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:42:49,743 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:42:49,743 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:42:49,748 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:42:49,749 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:42:49,749 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:42:49,749 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:42:49,749 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:42:49,749 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:42:50,318 - src.llm_wrapper - INFO - Model ready in 0.57s
2025-09-05 04:42:50,318 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:42:50,318 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:42:50,318 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:42:50,318 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:42:50,318 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:42:50,318 - src.rag_pipeline - INFO - Processing validated query: 'How do large language models work?'
2025-09-05 04:42:50,326 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How do large language models work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.03it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.02it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.99it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.98it/s, batch=1/1, memory=N/A]
2025-09-05 04:42:51,039 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:42:51,042 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:42:51,482 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.156s
2025-09-05 04:42:51,493 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:42:51,493 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1337 tokens) in 0.008s
2025-09-05 04:42:57,351 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.853s
2025-09-05 04:42:57,537 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:42:57,588 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:42:57,596 - src.experiment_runner - INFO - Completed run 465/510: 7.61s
2025-09-05 04:42:57,596 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:42:57,596 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:42:57,596 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:42:57,596 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:42:57,596 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:42:57,596 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:42:57,596 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:42:57,601 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:42:57,601 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:42:57,602 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:42:57,602 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:42:57,602 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:42:57,602 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:42:58,168 - src.llm_wrapper - INFO - Model ready in 0.57s
2025-09-05 04:42:58,168 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:42:58,168 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:42:58,168 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:42:58,169 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:42:58,169 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:42:58,169 - src.rag_pipeline - INFO - Processing validated query: 'What is natural language processing?'
2025-09-05 04:42:58,176 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is natural language processing?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.14it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.13it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.10it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.10it/s, batch=1/1, memory=N/A]
2025-09-05 04:42:58,775 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:42:58,776 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:42:59,189 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.013s
2025-09-05 04:42:59,193 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:42:59,193 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1749 tokens) in 0.003s
2025-09-05 04:43:06,571 - src.rag_pipeline - INFO - Generated response (55 tokens) in 7.373s
2025-09-05 04:43:06,763 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:43:06,815 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:43:06,825 - src.experiment_runner - INFO - Completed run 466/510: 8.98s
2025-09-05 04:43:06,826 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:43:06,826 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:43:06,826 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:43:06,826 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:43:06,826 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:43:06,826 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:43:06,826 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:43:06,832 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:43:06,833 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:43:06,833 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:43:06,833 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:43:06,833 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:43:06,833 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:43:07,377 - src.llm_wrapper - INFO - Model ready in 0.54s
2025-09-05 04:43:07,377 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:43:07,377 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:43:07,377 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:43:07,377 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:43:07,377 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:43:07,377 - src.rag_pipeline - INFO - Processing validated query: 'Explain computer vision techniques.'
2025-09-05 04:43:07,397 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain computer vision techniques....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.99it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.97it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.92it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.92it/s, batch=1/1, memory=N/A]
2025-09-05 04:43:08,010 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:43:08,013 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:43:08,465 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.068s
2025-09-05 04:43:08,471 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:43:08,472 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1297 tokens) in 0.005s
2025-09-05 04:43:28,383 - src.rag_pipeline - INFO - Generated response (318 tokens) in 19.904s
2025-09-05 04:43:28,575 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:43:28,639 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:43:28,647 - src.experiment_runner - INFO - Completed run 467/510: 21.56s
2025-09-05 04:43:28,647 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:43:28,647 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:43:28,647 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:43:28,647 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:43:28,647 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:43:28,647 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:43:28,647 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:43:28,653 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:43:28,653 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:43:28,653 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:43:28,653 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:43:28,654 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:43:28,654 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:43:29,287 - src.llm_wrapper - INFO - Model ready in 0.63s
2025-09-05 04:43:29,287 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:43:29,287 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:43:29,287 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:43:29,287 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:43:29,287 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:43:29,287 - src.rag_pipeline - INFO - Processing validated query: 'What is reinforcement learning?'
2025-09-05 04:43:29,302 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is reinforcement learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.77it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.76it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.72it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.72it/s, batch=1/1, memory=N/A]
2025-09-05 04:43:29,945 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:43:29,947 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:43:30,561 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.258s
2025-09-05 04:43:30,575 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:43:30,575 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1236 tokens) in 0.011s
2025-09-05 04:43:43,003 - src.rag_pipeline - INFO - Generated response (180 tokens) in 12.422s
2025-09-05 04:43:43,211 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:43:43,282 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:43:43,291 - src.experiment_runner - INFO - Completed run 468/510: 14.36s
2025-09-05 04:43:43,292 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:43:43,292 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:43:43,292 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:43:43,292 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:43:43,292 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:43:43,292 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:43:43,292 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:43:43,297 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:43:43,298 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:43:43,298 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:43:43,298 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:43:43,298 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:43:43,298 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:43:43,857 - src.llm_wrapper - INFO - Model ready in 0.56s
2025-09-05 04:43:43,857 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:43:43,857 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:43:43,857 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:43:43,857 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:43:43,857 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:43:43,857 - src.rag_pipeline - INFO - Processing validated query: 'How does data preprocessing work?'
2025-09-05 04:43:43,867 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does data preprocessing work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.98it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.98it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.95it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.95it/s, batch=1/1, memory=N/A]
2025-09-05 04:43:44,416 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:43:44,418 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:43:44,828 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.959s
2025-09-05 04:43:44,833 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:43:44,833 - src.rag_pipeline - INFO - Built prompt with 5 contexts (909 tokens) in 0.004s
2025-09-05 04:43:54,363 - src.rag_pipeline - INFO - Generated response (139 tokens) in 9.524s
2025-09-05 04:43:54,581 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:43:54,637 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:43:54,647 - src.experiment_runner - INFO - Completed run 469/510: 11.07s
2025-09-05 04:43:54,647 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:43:54,647 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:43:54,647 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:43:54,648 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:43:54,648 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:43:54,648 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:43:54,648 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:43:54,653 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:43:54,653 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:43:54,653 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:43:54,653 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:43:54,654 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:43:54,654 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:43:55,184 - src.llm_wrapper - INFO - Model ready in 0.53s
2025-09-05 04:43:55,185 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:43:55,185 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:43:55,185 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:43:55,185 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:43:55,185 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:43:55,185 - src.rag_pipeline - INFO - Processing validated query: 'What are the challenges in AI development?'
2025-09-05 04:43:55,195 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are the challenges in AI development?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.18it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.15it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.13it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.13it/s, batch=1/1, memory=N/A]
2025-09-05 04:43:55,755 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:43:55,759 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:43:56,143 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.947s
2025-09-05 04:43:56,150 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:43:56,151 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1445 tokens) in 0.006s
2025-09-05 04:44:12,249 - src.rag_pipeline - INFO - Generated response (237 tokens) in 16.092s
2025-09-05 04:44:12,430 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:44:12,491 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:44:12,500 - src.experiment_runner - INFO - Completed run 470/510: 17.60s
2025-09-05 04:44:12,501 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:44:12,501 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:44:12,501 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:44:12,501 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:44:12,501 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:44:12,501 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:44:12,501 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:44:12,508 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:44:12,508 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:44:12,508 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:44:12,508 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:44:12,508 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:44:12,508 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:44:13,092 - src.llm_wrapper - INFO - Model ready in 0.58s
2025-09-05 04:44:13,093 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:44:13,093 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:44:13,093 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:44:13,093 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:44:13,093 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:44:13,093 - src.rag_pipeline - INFO - Processing validated query: 'What is machine learning?'
2025-09-05 04:44:13,102 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is machine learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.88it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.87it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.83it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.83it/s, batch=1/1, memory=N/A]
2025-09-05 04:44:13,816 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:44:13,820 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:44:14,274 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.171s
2025-09-05 04:44:14,286 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:44:14,287 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1162 tokens) in 0.010s
2025-09-05 04:44:20,401 - src.rag_pipeline - INFO - Generated response (59 tokens) in 6.109s
2025-09-05 04:44:20,612 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:44:20,684 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:44:20,693 - src.experiment_runner - INFO - Completed run 471/510: 7.90s
2025-09-05 04:44:20,693 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:44:20,693 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:44:20,693 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:44:20,693 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:44:20,693 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:44:20,693 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:44:20,693 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:44:20,703 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:44:20,704 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:44:20,704 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:44:20,704 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:44:20,704 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:44:20,704 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:44:21,339 - src.llm_wrapper - INFO - Model ready in 0.64s
2025-09-05 04:44:21,340 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:44:21,340 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:44:21,340 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:44:21,340 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:44:21,340 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:44:21,340 - src.rag_pipeline - INFO - Processing validated query: 'How does artificial intelligence work?'
2025-09-05 04:44:21,349 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does artificial intelligence work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.25it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.23it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.21it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.21it/s, batch=1/1, memory=N/A]
2025-09-05 04:44:22,014 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:44:22,015 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:44:22,443 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.094s
2025-09-05 04:44:22,448 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:44:22,450 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1211 tokens) in 0.004s
2025-09-05 04:44:40,932 - src.rag_pipeline - INFO - Generated response (287 tokens) in 18.463s
2025-09-05 04:44:41,259 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:44:41,333 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:44:41,341 - src.experiment_runner - INFO - Completed run 472/510: 20.24s
2025-09-05 04:44:41,341 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:44:41,341 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:44:41,342 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:44:41,342 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:44:41,342 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:44:41,342 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:44:41,342 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:44:41,347 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:44:41,348 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:44:41,348 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:44:41,348 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:44:41,348 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:44:41,348 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:44:42,031 - src.llm_wrapper - INFO - Model ready in 0.68s
2025-09-05 04:44:42,032 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:44:42,032 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:44:42,032 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:44:42,032 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:44:42,032 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:44:42,032 - src.rag_pipeline - INFO - Processing validated query: 'Explain deep learning algorithms.'
2025-09-05 04:44:42,043 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain deep learning algorithms....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.54it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.53it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.47it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.47it/s, batch=1/1, memory=N/A]
2025-09-05 04:44:42,774 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:44:42,777 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:44:43,321 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.276s
2025-09-05 04:44:43,330 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:44:43,331 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1198 tokens) in 0.008s
2025-09-05 04:44:48,929 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.594s
2025-09-05 04:44:49,123 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:44:49,196 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:44:49,203 - src.experiment_runner - INFO - Completed run 473/510: 7.59s
2025-09-05 04:44:49,203 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:44:49,203 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:44:49,204 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:44:49,204 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:44:49,204 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:44:49,204 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:44:49,204 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:44:49,210 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:44:49,210 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:44:49,210 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:44:49,210 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:44:49,210 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:44:49,210 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:44:49,856 - src.llm_wrapper - INFO - Model ready in 0.65s
2025-09-05 04:44:49,856 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:44:49,856 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:44:49,856 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:44:49,856 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:44:49,856 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:44:49,856 - src.rag_pipeline - INFO - Processing validated query: 'What are neural networks?'
2025-09-05 04:44:49,868 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are neural networks?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.12it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.11it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.08it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.08it/s, batch=1/1, memory=N/A]
2025-09-05 04:44:50,476 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:44:50,479 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:44:50,933 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.066s
2025-09-05 04:44:50,944 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:44:50,945 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1353 tokens) in 0.010s
2025-09-05 04:45:06,777 - src.rag_pipeline - INFO - Generated response (238 tokens) in 15.827s
2025-09-05 04:45:06,996 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:45:07,055 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:45:07,063 - src.experiment_runner - INFO - Completed run 474/510: 17.57s
2025-09-05 04:45:07,063 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:45:07,063 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:45:07,063 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:45:07,063 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:45:07,064 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:45:07,064 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:45:07,064 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:45:07,069 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:45:07,070 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:45:07,070 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:45:07,070 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:45:07,070 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:45:07,070 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:45:07,609 - src.llm_wrapper - INFO - Model ready in 0.54s
2025-09-05 04:45:07,609 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:45:07,609 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:45:07,609 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:45:07,609 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:45:07,609 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:45:07,609 - src.rag_pipeline - INFO - Processing validated query: 'How do large language models work?'
2025-09-05 04:45:07,616 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How do large language models work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.95it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.94it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.91it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.91it/s, batch=1/1, memory=N/A]
2025-09-05 04:45:08,266 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:45:08,269 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:45:08,735 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.118s
2025-09-05 04:45:08,745 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:45:08,747 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1337 tokens) in 0.010s
2025-09-05 04:45:14,695 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.944s
2025-09-05 04:45:14,885 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:45:14,951 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:45:14,959 - src.experiment_runner - INFO - Completed run 475/510: 7.63s
2025-09-05 04:45:14,959 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:45:14,959 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:45:14,959 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:45:14,960 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:45:14,960 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:45:14,960 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:45:14,960 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:45:14,967 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:45:14,967 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:45:14,967 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:45:14,968 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:45:14,968 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:45:14,968 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:45:15,504 - src.llm_wrapper - INFO - Model ready in 0.54s
2025-09-05 04:45:15,504 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:45:15,504 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:45:15,504 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:45:15,504 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:45:15,504 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:45:15,504 - src.rag_pipeline - INFO - Processing validated query: 'What is natural language processing?'
2025-09-05 04:45:15,511 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is natural language processing?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.69it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.69it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.67it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.66it/s, batch=1/1, memory=N/A]
2025-09-05 04:45:15,903 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:45:15,904 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:45:16,232 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.721s
2025-09-05 04:45:16,240 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:45:16,241 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1749 tokens) in 0.008s
2025-09-05 04:45:23,592 - src.rag_pipeline - INFO - Generated response (55 tokens) in 7.347s
2025-09-05 04:45:23,791 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:45:23,851 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:45:23,859 - src.experiment_runner - INFO - Completed run 476/510: 8.63s
2025-09-05 04:45:23,859 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:45:23,859 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:45:23,859 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:45:23,859 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:45:23,859 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:45:23,859 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:45:23,859 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:45:23,864 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:45:23,865 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:45:23,865 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:45:23,865 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:45:23,865 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:45:23,865 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:45:24,386 - src.llm_wrapper - INFO - Model ready in 0.52s
2025-09-05 04:45:24,386 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:45:24,386 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:45:24,386 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:45:24,386 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:45:24,386 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:45:24,386 - src.rag_pipeline - INFO - Processing validated query: 'Explain computer vision techniques.'
2025-09-05 04:45:24,394 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain computer vision techniques....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.65it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.64it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.62it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.62it/s, batch=1/1, memory=N/A]
2025-09-05 04:45:24,811 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:45:24,812 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:45:25,150 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.756s
2025-09-05 04:45:25,152 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:45:25,152 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1297 tokens) in 0.002s
2025-09-05 04:45:44,889 - src.rag_pipeline - INFO - Generated response (318 tokens) in 19.731s
2025-09-05 04:45:45,130 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:45:45,187 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:45:45,195 - src.experiment_runner - INFO - Completed run 477/510: 21.03s
2025-09-05 04:45:45,195 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:45:45,195 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:45:45,195 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:45:45,196 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:45:45,196 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:45:45,196 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:45:45,196 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:45:45,201 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:45:45,201 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:45:45,201 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:45:45,201 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:45:45,201 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:45:45,201 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:45:45,727 - src.llm_wrapper - INFO - Model ready in 0.53s
2025-09-05 04:45:45,727 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:45:45,727 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:45:45,727 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:45:45,727 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:45:45,727 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:45:45,727 - src.rag_pipeline - INFO - Processing validated query: 'What is reinforcement learning?'
2025-09-05 04:45:45,734 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is reinforcement learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.22it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.21it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.19it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.19it/s, batch=1/1, memory=N/A]
2025-09-05 04:45:46,282 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:45:46,283 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:45:46,643 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.908s
2025-09-05 04:45:46,650 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:45:46,652 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1236 tokens) in 0.008s
2025-09-05 04:45:59,165 - src.rag_pipeline - INFO - Generated response (180 tokens) in 12.507s
2025-09-05 04:45:59,351 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:45:59,406 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:45:59,414 - src.experiment_runner - INFO - Completed run 478/510: 13.97s
2025-09-05 04:45:59,414 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:45:59,414 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:45:59,415 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:45:59,415 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:45:59,415 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:45:59,415 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:45:59,415 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:45:59,420 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:45:59,421 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:45:59,421 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:45:59,421 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:45:59,421 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:45:59,421 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:45:59,982 - src.llm_wrapper - INFO - Model ready in 0.56s
2025-09-05 04:45:59,982 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:45:59,982 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:45:59,982 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:45:59,982 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:45:59,982 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:45:59,982 - src.rag_pipeline - INFO - Processing validated query: 'How does data preprocessing work?'
2025-09-05 04:45:59,989 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does data preprocessing work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.25it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.23it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.22it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.22it/s, batch=1/1, memory=N/A]
2025-09-05 04:46:00,421 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:46:00,423 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:46:00,818 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.828s
2025-09-05 04:46:00,831 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:46:00,833 - src.rag_pipeline - INFO - Built prompt with 5 contexts (909 tokens) in 0.012s
2025-09-05 04:46:10,169 - src.rag_pipeline - INFO - Generated response (139 tokens) in 9.330s
2025-09-05 04:46:10,409 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:46:10,478 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:46:10,485 - src.experiment_runner - INFO - Completed run 479/510: 10.76s
2025-09-05 04:46:10,485 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:46:10,485 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:46:10,485 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:46:10,485 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:46:10,485 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:46:10,485 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:46:10,485 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:46:10,491 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:46:10,491 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:46:10,491 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:46:10,491 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:46:10,491 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:46:10,491 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:46:11,060 - src.llm_wrapper - INFO - Model ready in 0.57s
2025-09-05 04:46:11,060 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:46:11,061 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:46:11,061 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:46:11,061 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:46:11,061 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:46:11,061 - src.rag_pipeline - INFO - Processing validated query: 'What are the challenges in AI development?'
2025-09-05 04:46:11,069 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are the challenges in AI development?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.02it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.01it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.97it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.97it/s, batch=1/1, memory=N/A]
2025-09-05 04:46:11,715 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:46:11,718 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:46:12,147 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.077s
2025-09-05 04:46:12,156 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:46:12,157 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1445 tokens) in 0.007s
2025-09-05 04:46:28,203 - src.rag_pipeline - INFO - Generated response (237 tokens) in 16.040s
2025-09-05 04:46:28,417 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:46:28,475 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:46:28,481 - src.experiment_runner - INFO - Completed run 480/510: 17.72s
2025-09-05 04:46:28,482 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:46:28,482 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:46:28,482 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:46:28,482 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:46:28,482 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:46:28,482 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:46:28,482 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:46:28,488 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:46:28,488 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:46:28,488 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:46:28,488 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:46:28,488 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:46:28,488 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:46:29,023 - src.llm_wrapper - INFO - Model ready in 0.54s
2025-09-05 04:46:29,023 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:46:29,023 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:46:29,023 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:46:29,023 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:46:29,023 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:46:29,024 - src.rag_pipeline - INFO - Processing validated query: 'What is machine learning?'
2025-09-05 04:46:29,031 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is machine learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.39it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.39it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.38it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.38it/s, batch=1/1, memory=N/A]
2025-09-05 04:46:29,451 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:46:29,452 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:46:29,865 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.834s
2025-09-05 04:46:29,872 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:46:29,872 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1162 tokens) in 0.006s
2025-09-05 04:46:35,872 - src.rag_pipeline - INFO - Generated response (59 tokens) in 5.996s
2025-09-05 04:46:36,056 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:46:36,116 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:46:36,124 - src.experiment_runner - INFO - Completed run 481/510: 7.39s
2025-09-05 04:46:36,124 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:46:36,124 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:46:36,124 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:46:36,124 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:46:36,124 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:46:36,124 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:46:36,124 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:46:36,129 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:46:36,130 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:46:36,130 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:46:36,130 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:46:36,130 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:46:36,130 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:46:36,661 - src.llm_wrapper - INFO - Model ready in 0.53s
2025-09-05 04:46:36,661 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:46:36,661 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:46:36,661 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:46:36,661 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:46:36,661 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:46:36,661 - src.rag_pipeline - INFO - Processing validated query: 'How does artificial intelligence work?'
2025-09-05 04:46:36,668 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does artificial intelligence work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.14it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.13it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.08it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.08it/s, batch=1/1, memory=N/A]
2025-09-05 04:46:37,249 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:46:37,250 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:46:37,646 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.978s
2025-09-05 04:46:37,655 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:46:37,655 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1211 tokens) in 0.008s
2025-09-05 04:46:55,581 - src.rag_pipeline - INFO - Generated response (287 tokens) in 17.908s
2025-09-05 04:46:55,766 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:46:55,818 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:46:55,832 - src.experiment_runner - INFO - Completed run 482/510: 19.46s
2025-09-05 04:46:55,832 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:46:55,832 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:46:55,832 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:46:55,832 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:46:55,832 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:46:55,832 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:46:55,832 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:46:55,838 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:46:55,838 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:46:55,838 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:46:55,838 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:46:55,839 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:46:55,839 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:46:56,396 - src.llm_wrapper - INFO - Model ready in 0.56s
2025-09-05 04:46:56,396 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:46:56,397 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:46:56,397 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:46:56,397 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:46:56,397 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:46:56,397 - src.rag_pipeline - INFO - Processing validated query: 'Explain deep learning algorithms.'
2025-09-05 04:46:56,405 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain deep learning algorithms....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.31it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.31it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.29it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.29it/s, batch=1/1, memory=N/A]
2025-09-05 04:46:56,821 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:46:56,822 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:46:57,233 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.828s
2025-09-05 04:46:57,243 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:46:57,244 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1198 tokens) in 0.010s
2025-09-05 04:47:02,742 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.494s
2025-09-05 04:47:02,914 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:47:02,970 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:47:02,979 - src.experiment_runner - INFO - Completed run 483/510: 6.91s
2025-09-05 04:47:02,979 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:47:02,979 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:47:02,979 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:47:02,979 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:47:02,979 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:47:02,979 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:47:02,979 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:47:02,985 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:47:02,985 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:47:02,985 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:47:02,986 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:47:02,986 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:47:02,986 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:47:03,515 - src.llm_wrapper - INFO - Model ready in 0.53s
2025-09-05 04:47:03,515 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:47:03,515 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:47:03,515 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:47:03,515 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:47:03,515 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:47:03,515 - src.rag_pipeline - INFO - Processing validated query: 'What are neural networks?'
2025-09-05 04:47:03,523 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are neural networks?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.58it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.56it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.54it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.54it/s, batch=1/1, memory=N/A]
2025-09-05 04:47:03,935 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:47:03,937 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:47:04,318 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.795s
2025-09-05 04:47:04,322 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:47:04,324 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1353 tokens) in 0.004s
2025-09-05 04:47:19,968 - src.rag_pipeline - INFO - Generated response (238 tokens) in 15.637s
2025-09-05 04:47:20,170 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:47:20,233 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:47:20,241 - src.experiment_runner - INFO - Completed run 484/510: 16.99s
2025-09-05 04:47:20,241 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:47:20,241 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:47:20,241 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:47:20,241 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:47:20,241 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:47:20,241 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:47:20,241 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:47:20,247 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:47:20,247 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:47:20,247 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:47:20,247 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:47:20,247 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:47:20,247 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:47:20,838 - src.llm_wrapper - INFO - Model ready in 0.59s
2025-09-05 04:47:20,838 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:47:20,838 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:47:20,838 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:47:20,838 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:47:20,838 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:47:20,838 - src.rag_pipeline - INFO - Processing validated query: 'How do large language models work?'
2025-09-05 04:47:20,845 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How do large language models work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.93it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.93it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.91it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.91it/s, batch=1/1, memory=N/A]
2025-09-05 04:47:21,469 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:47:21,471 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:47:21,963 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.118s
2025-09-05 04:47:21,972 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:47:21,972 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1337 tokens) in 0.008s
2025-09-05 04:47:27,795 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.819s
2025-09-05 04:47:27,971 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:47:28,025 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:47:28,033 - src.experiment_runner - INFO - Completed run 485/510: 7.55s
2025-09-05 04:47:28,033 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:47:28,033 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:47:28,033 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:47:28,033 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:47:28,033 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:47:28,033 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:47:28,033 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:47:28,040 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:47:28,040 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:47:28,040 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:47:28,040 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:47:28,040 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:47:28,040 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:47:28,588 - src.llm_wrapper - INFO - Model ready in 0.55s
2025-09-05 04:47:28,588 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:47:28,588 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:47:28,588 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:47:28,588 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:47:28,588 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:47:28,588 - src.rag_pipeline - INFO - Processing validated query: 'What is natural language processing?'
2025-09-05 04:47:28,596 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is natural language processing?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.30it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.28it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.26it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.26it/s, batch=1/1, memory=N/A]
2025-09-05 04:47:29,140 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:47:29,141 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:47:29,568 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.971s
2025-09-05 04:47:29,577 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:47:29,579 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1749 tokens) in 0.006s
2025-09-05 04:47:36,984 - src.rag_pipeline - INFO - Generated response (55 tokens) in 7.400s
2025-09-05 04:47:37,163 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:47:37,217 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:47:37,224 - src.experiment_runner - INFO - Completed run 486/510: 8.95s
2025-09-05 04:47:37,224 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:47:37,224 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:47:37,224 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:47:37,224 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:47:37,224 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:47:37,224 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:47:37,224 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:47:37,230 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:47:37,230 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:47:37,230 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:47:37,230 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:47:37,231 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:47:37,231 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:47:37,749 - src.llm_wrapper - INFO - Model ready in 0.52s
2025-09-05 04:47:37,749 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:47:37,749 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:47:37,749 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:47:37,749 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:47:37,749 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:47:37,749 - src.rag_pipeline - INFO - Processing validated query: 'Explain computer vision techniques.'
2025-09-05 04:47:37,756 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain computer vision techniques....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.15it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.14it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.13it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.13it/s, batch=1/1, memory=N/A]
2025-09-05 04:47:38,279 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:47:38,282 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:47:38,703 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.946s
2025-09-05 04:47:38,707 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:47:38,708 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1297 tokens) in 0.004s
2025-09-05 04:47:58,393 - src.rag_pipeline - INFO - Generated response (318 tokens) in 19.678s
2025-09-05 04:47:58,607 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:47:58,663 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:47:58,670 - src.experiment_runner - INFO - Completed run 487/510: 21.17s
2025-09-05 04:47:58,670 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:47:58,670 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:47:58,670 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:47:58,670 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:47:58,670 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:47:58,670 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:47:58,670 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:47:58,675 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:47:58,676 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:47:58,676 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:47:58,676 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:47:58,676 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:47:58,676 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:47:59,252 - src.llm_wrapper - INFO - Model ready in 0.58s
2025-09-05 04:47:59,252 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:47:59,252 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:47:59,252 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:47:59,252 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:47:59,252 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:47:59,252 - src.rag_pipeline - INFO - Processing validated query: 'What is reinforcement learning?'
2025-09-05 04:47:59,259 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is reinforcement learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.19it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.17it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.13it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.13it/s, batch=1/1, memory=N/A]
2025-09-05 04:47:59,801 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:47:59,802 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:48:00,207 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.947s
2025-09-05 04:48:00,216 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:48:00,217 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1236 tokens) in 0.009s
2025-09-05 04:48:12,603 - src.rag_pipeline - INFO - Generated response (180 tokens) in 12.379s
2025-09-05 04:48:12,800 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:48:12,856 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:48:12,863 - src.experiment_runner - INFO - Completed run 488/510: 13.93s
2025-09-05 04:48:12,863 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:48:12,863 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:48:12,863 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:48:12,863 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:48:12,863 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:48:12,863 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:48:12,863 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:48:12,869 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:48:12,869 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:48:12,869 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:48:12,869 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:48:12,869 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:48:12,869 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:48:13,432 - src.llm_wrapper - INFO - Model ready in 0.56s
2025-09-05 04:48:13,432 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:48:13,432 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:48:13,432 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:48:13,432 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:48:13,432 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:48:13,432 - src.rag_pipeline - INFO - Processing validated query: 'How does data preprocessing work?'
2025-09-05 04:48:13,441 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does data preprocessing work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.91it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.90it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.87it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.87it/s, batch=1/1, memory=N/A]
2025-09-05 04:48:14,064 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:48:14,065 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:48:14,492 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.050s
2025-09-05 04:48:14,496 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:48:14,496 - src.rag_pipeline - INFO - Built prompt with 5 contexts (909 tokens) in 0.003s
2025-09-05 04:48:24,099 - src.rag_pipeline - INFO - Generated response (139 tokens) in 9.596s
2025-09-05 04:48:24,304 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:48:24,372 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:48:24,379 - src.experiment_runner - INFO - Completed run 489/510: 11.24s
2025-09-05 04:48:24,379 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:48:24,379 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:48:24,379 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:48:24,379 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:48:24,379 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:48:24,379 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:48:24,379 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:48:24,385 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:48:24,385 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:48:24,385 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:48:24,385 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:48:24,386 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:48:24,386 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:48:24,940 - src.llm_wrapper - INFO - Model ready in 0.55s
2025-09-05 04:48:24,940 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:48:24,940 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:48:24,940 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:48:24,941 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:48:24,941 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:48:24,941 - src.rag_pipeline - INFO - Processing validated query: 'What are the challenges in AI development?'
2025-09-05 04:48:24,948 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are the challenges in AI development?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.96it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.94it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.92it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.92it/s, batch=1/1, memory=N/A]
2025-09-05 04:48:25,635 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:48:25,637 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:48:26,080 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.132s
2025-09-05 04:48:26,087 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:48:26,088 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1445 tokens) in 0.006s
2025-09-05 04:48:42,199 - src.rag_pipeline - INFO - Generated response (237 tokens) in 16.105s
2025-09-05 04:48:42,395 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:48:42,453 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:48:42,461 - src.experiment_runner - INFO - Completed run 490/510: 17.82s
2025-09-05 04:48:42,461 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:48:42,461 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:48:42,462 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:48:42,462 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:48:42,462 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:48:42,462 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:48:42,462 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:48:42,467 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:48:42,467 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:48:42,467 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:48:42,467 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:48:42,468 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:48:42,468 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:48:43,038 - src.llm_wrapper - INFO - Model ready in 0.57s
2025-09-05 04:48:43,038 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:48:43,039 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:48:43,039 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:48:43,039 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:48:43,039 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:48:43,039 - src.rag_pipeline - INFO - Processing validated query: 'What is machine learning?'
2025-09-05 04:48:43,046 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is machine learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.13it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.12it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.10it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.10it/s, batch=1/1, memory=N/A]
2025-09-05 04:48:43,657 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:48:43,660 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:48:44,095 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.048s
2025-09-05 04:48:44,105 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:48:44,106 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1162 tokens) in 0.009s
2025-09-05 04:48:50,164 - src.rag_pipeline - INFO - Generated response (59 tokens) in 6.053s
2025-09-05 04:48:50,363 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:48:50,420 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:48:50,428 - src.experiment_runner - INFO - Completed run 491/510: 7.70s
2025-09-05 04:48:50,428 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:48:50,428 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:48:50,429 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:48:50,429 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:48:50,429 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:48:50,429 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:48:50,429 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:48:50,435 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:48:50,436 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:48:50,436 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:48:50,436 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:48:50,436 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:48:50,436 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:48:50,965 - src.llm_wrapper - INFO - Model ready in 0.53s
2025-09-05 04:48:50,965 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:48:50,965 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:48:50,965 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:48:50,965 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:48:50,965 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:48:50,965 - src.rag_pipeline - INFO - Processing validated query: 'How does artificial intelligence work?'
2025-09-05 04:48:50,972 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does artificial intelligence work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.27it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.26it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.20it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.20it/s, batch=1/1, memory=N/A]
2025-09-05 04:48:51,545 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:48:51,548 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:48:51,933 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.960s
2025-09-05 04:48:51,940 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:48:51,942 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1211 tokens) in 0.006s
2025-09-05 04:49:09,829 - src.rag_pipeline - INFO - Generated response (287 tokens) in 17.880s
2025-09-05 04:49:10,045 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:49:10,100 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:49:10,108 - src.experiment_runner - INFO - Completed run 492/510: 19.40s
2025-09-05 04:49:10,108 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:49:10,108 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:49:10,108 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:49:10,108 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:49:10,108 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:49:10,108 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:49:10,108 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:49:10,114 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:49:10,115 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:49:10,115 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:49:10,115 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:49:10,115 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:49:10,115 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:49:10,714 - src.llm_wrapper - INFO - Model ready in 0.60s
2025-09-05 04:49:10,714 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:49:10,714 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:49:10,714 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:49:10,714 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:49:10,714 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:49:10,714 - src.rag_pipeline - INFO - Processing validated query: 'Explain deep learning algorithms.'
2025-09-05 04:49:10,722 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain deep learning algorithms....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.29it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.28it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.23it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.23it/s, batch=1/1, memory=N/A]
2025-09-05 04:49:11,308 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:49:11,309 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:49:11,711 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.988s
2025-09-05 04:49:11,720 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:49:11,721 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1198 tokens) in 0.008s
2025-09-05 04:49:17,222 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.492s
2025-09-05 04:49:17,409 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:49:17,480 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:49:17,487 - src.experiment_runner - INFO - Completed run 493/510: 7.11s
2025-09-05 04:49:17,487 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:49:17,487 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:49:17,487 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:49:17,487 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:49:17,487 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:49:17,487 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:49:17,487 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:49:17,493 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:49:17,493 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:49:17,493 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:49:17,493 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:49:17,494 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:49:17,494 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:49:18,028 - src.llm_wrapper - INFO - Model ready in 0.53s
2025-09-05 04:49:18,028 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:49:18,028 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:49:18,028 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:49:18,028 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:49:18,028 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:49:18,028 - src.rag_pipeline - INFO - Processing validated query: 'What are neural networks?'
2025-09-05 04:49:18,035 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are neural networks?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.19it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.19it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.16it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.16it/s, batch=1/1, memory=N/A]
2025-09-05 04:49:18,742 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:49:18,745 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:49:19,179 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.144s
2025-09-05 04:49:19,191 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:49:19,192 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1353 tokens) in 0.010s
2025-09-05 04:49:34,924 - src.rag_pipeline - INFO - Generated response (238 tokens) in 15.726s
2025-09-05 04:49:35,149 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:49:35,223 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:49:35,230 - src.experiment_runner - INFO - Completed run 494/510: 17.44s
2025-09-05 04:49:35,230 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:49:35,230 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:49:35,230 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:49:35,230 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:49:35,230 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:49:35,230 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:49:35,230 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:49:35,235 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:49:35,236 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:49:35,236 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:49:35,236 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:49:35,236 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:49:35,236 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:49:35,797 - src.llm_wrapper - INFO - Model ready in 0.56s
2025-09-05 04:49:35,798 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:49:35,798 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:49:35,798 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:49:35,798 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:49:35,798 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:49:35,798 - src.rag_pipeline - INFO - Processing validated query: 'How do large language models work?'
2025-09-05 04:49:35,805 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How do large language models work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.13it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.12it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.07it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.07it/s, batch=1/1, memory=N/A]
2025-09-05 04:49:36,454 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:49:36,456 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:49:36,879 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.073s
2025-09-05 04:49:36,891 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:49:36,892 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1337 tokens) in 0.011s
2025-09-05 04:49:42,746 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.838s
2025-09-05 04:49:42,981 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:49:43,041 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:49:43,050 - src.experiment_runner - INFO - Completed run 495/510: 7.52s
2025-09-05 04:49:43,050 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:49:43,050 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:49:43,050 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:49:43,050 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:49:43,050 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:49:43,050 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:49:43,050 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:49:43,056 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:49:43,056 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:49:43,056 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:49:43,056 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:49:43,056 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:49:43,056 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:49:43,606 - src.llm_wrapper - INFO - Model ready in 0.55s
2025-09-05 04:49:43,606 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:49:43,606 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:49:43,607 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:49:43,607 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:49:43,607 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:49:43,607 - src.rag_pipeline - INFO - Processing validated query: 'What is natural language processing?'
2025-09-05 04:49:43,617 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is natural language processing?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.93it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.91it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.81it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.81it/s, batch=1/1, memory=N/A]
2025-09-05 04:49:44,217 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:49:44,219 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:49:44,630 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.013s
2025-09-05 04:49:44,639 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:49:44,640 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1749 tokens) in 0.009s
2025-09-05 04:49:52,047 - src.rag_pipeline - INFO - Generated response (55 tokens) in 7.402s
2025-09-05 04:49:52,260 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:49:52,320 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:49:52,330 - src.experiment_runner - INFO - Completed run 496/510: 9.00s
2025-09-05 04:49:52,330 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:49:52,330 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:49:52,330 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:49:52,330 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:49:52,330 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:49:52,330 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:49:52,330 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:49:52,336 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:49:52,336 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:49:52,336 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:49:52,336 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:49:52,336 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:49:52,336 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:49:52,903 - src.llm_wrapper - INFO - Model ready in 0.57s
2025-09-05 04:49:52,903 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:49:52,903 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:49:52,903 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:49:52,903 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:49:52,903 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:49:52,903 - src.rag_pipeline - INFO - Processing validated query: 'Explain computer vision techniques.'
2025-09-05 04:49:52,911 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain computer vision techniques....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.81it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.79it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.75it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.75it/s, batch=1/1, memory=N/A]
2025-09-05 04:49:53,579 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:49:53,581 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:49:54,042 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.131s
2025-09-05 04:49:54,049 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:49:54,050 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1297 tokens) in 0.006s
2025-09-05 04:50:13,881 - src.rag_pipeline - INFO - Generated response (318 tokens) in 19.826s
2025-09-05 04:50:14,091 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:50:14,147 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:50:14,153 - src.experiment_runner - INFO - Completed run 497/510: 21.55s
2025-09-05 04:50:14,153 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:50:14,153 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:50:14,153 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:50:14,153 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:50:14,153 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:50:14,153 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:50:14,153 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:50:14,159 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:50:14,159 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:50:14,159 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:50:14,159 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:50:14,159 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:50:14,159 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:50:14,741 - src.llm_wrapper - INFO - Model ready in 0.58s
2025-09-05 04:50:14,741 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:50:14,741 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:50:14,741 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:50:14,741 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:50:14,741 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:50:14,741 - src.rag_pipeline - INFO - Processing validated query: 'What is reinforcement learning?'
2025-09-05 04:50:14,748 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is reinforcement learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.23it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.21it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.18it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.18it/s, batch=1/1, memory=N/A]
2025-09-05 04:50:15,486 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:50:15,490 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:50:15,939 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.190s
2025-09-05 04:50:15,949 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:50:15,951 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1236 tokens) in 0.009s
2025-09-05 04:50:28,426 - src.rag_pipeline - INFO - Generated response (180 tokens) in 12.467s
2025-09-05 04:50:28,634 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:50:28,695 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:50:28,703 - src.experiment_runner - INFO - Completed run 498/510: 14.27s
2025-09-05 04:50:28,703 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:50:28,703 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:50:28,704 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:50:28,704 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:50:28,704 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:50:28,704 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:50:28,704 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:50:28,709 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:50:28,710 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:50:28,710 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:50:28,710 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:50:28,710 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:50:28,710 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:50:29,253 - src.llm_wrapper - INFO - Model ready in 0.54s
2025-09-05 04:50:29,253 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:50:29,253 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:50:29,253 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:50:29,253 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:50:29,253 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:50:29,253 - src.rag_pipeline - INFO - Processing validated query: 'How does data preprocessing work?'
2025-09-05 04:50:29,261 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does data preprocessing work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.20it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.19it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.14it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.14it/s, batch=1/1, memory=N/A]
2025-09-05 04:50:29,860 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:50:29,865 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:50:30,288 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.026s
2025-09-05 04:50:30,301 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:50:30,303 - src.rag_pipeline - INFO - Built prompt with 5 contexts (909 tokens) in 0.011s
2025-09-05 04:50:39,892 - src.rag_pipeline - INFO - Generated response (139 tokens) in 9.583s
2025-09-05 04:50:40,091 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:50:40,155 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:50:40,163 - src.experiment_runner - INFO - Completed run 499/510: 11.19s
2025-09-05 04:50:40,163 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:50:40,163 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:50:40,163 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:50:40,164 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:50:40,164 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:50:40,164 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:50:40,164 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:50:40,169 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:50:40,169 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:50:40,169 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:50:40,169 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:50:40,169 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:50:40,170 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:50:40,718 - src.llm_wrapper - INFO - Model ready in 0.55s
2025-09-05 04:50:40,718 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:50:40,718 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:50:40,718 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:50:40,718 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:50:40,718 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:50:40,718 - src.rag_pipeline - INFO - Processing validated query: 'What are the challenges in AI development?'
2025-09-05 04:50:40,728 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are the challenges in AI development?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.93it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.91it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.88it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.88it/s, batch=1/1, memory=N/A]
2025-09-05 04:50:41,427 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:50:41,433 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:50:41,874 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.145s
2025-09-05 04:50:41,885 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:50:41,886 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1445 tokens) in 0.007s
2025-09-05 04:50:58,009 - src.rag_pipeline - INFO - Generated response (237 tokens) in 16.117s
2025-09-05 04:50:58,209 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:50:58,270 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:50:58,279 - src.experiment_runner - INFO - Completed run 500/510: 17.85s
2025-09-05 04:50:58,279 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:50:58,279 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:50:58,279 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:50:58,279 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:50:58,279 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:50:58,279 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:50:58,279 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:50:58,284 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:50:58,285 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:50:58,285 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:50:58,285 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:50:58,285 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:50:58,285 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:50:58,838 - src.llm_wrapper - INFO - Model ready in 0.55s
2025-09-05 04:50:58,839 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:50:58,839 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:50:58,839 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:50:58,839 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:50:58,839 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:50:58,839 - src.rag_pipeline - INFO - Processing validated query: 'What is machine learning?'
2025-09-05 04:50:58,846 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is machine learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.02it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.99it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.97it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.97it/s, batch=1/1, memory=N/A]
2025-09-05 04:50:59,409 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:50:59,411 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:50:59,843 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.996s
2025-09-05 04:50:59,851 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:50:59,852 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1162 tokens) in 0.007s
2025-09-05 04:51:05,841 - src.rag_pipeline - INFO - Generated response (59 tokens) in 5.985s
2025-09-05 04:51:06,017 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:51:06,079 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:51:06,085 - src.experiment_runner - INFO - Completed run 501/510: 7.56s
2025-09-05 04:51:06,085 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:51:06,085 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:51:06,086 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:51:06,086 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:51:06,086 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:51:06,086 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:51:06,086 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:51:06,091 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:51:06,091 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:51:06,092 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:51:06,092 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:51:06,092 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:51:06,092 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:51:06,612 - src.llm_wrapper - INFO - Model ready in 0.52s
2025-09-05 04:51:06,612 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:51:06,612 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:51:06,612 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:51:06,612 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:51:06,612 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:51:06,612 - src.rag_pipeline - INFO - Processing validated query: 'How does artificial intelligence work?'
2025-09-05 04:51:06,619 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does artificial intelligence work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.31it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.30it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.25it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.25it/s, batch=1/1, memory=N/A]
2025-09-05 04:51:07,254 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:51:07,259 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:51:07,687 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.067s
2025-09-05 04:51:07,699 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:51:07,701 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1211 tokens) in 0.010s
2025-09-05 04:51:26,049 - src.rag_pipeline - INFO - Generated response (287 tokens) in 18.342s
2025-09-05 04:51:26,259 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:51:26,315 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:51:26,324 - src.experiment_runner - INFO - Completed run 502/510: 19.96s
2025-09-05 04:51:26,324 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:51:26,324 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:51:26,324 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:51:26,324 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:51:26,324 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:51:26,325 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:51:26,325 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:51:26,330 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:51:26,330 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:51:26,330 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:51:26,330 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:51:26,330 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:51:26,330 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:51:26,890 - src.llm_wrapper - INFO - Model ready in 0.56s
2025-09-05 04:51:26,890 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:51:26,890 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:51:26,890 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:51:26,890 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:51:26,890 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:51:26,890 - src.rag_pipeline - INFO - Processing validated query: 'Explain deep learning algorithms.'
2025-09-05 04:51:26,899 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain deep learning algorithms....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.17it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.15it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.11it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.11it/s, batch=1/1, memory=N/A]
2025-09-05 04:51:27,673 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:51:27,679 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:51:28,166 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.266s
2025-09-05 04:51:28,178 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:51:28,180 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1198 tokens) in 0.012s
2025-09-05 04:51:33,707 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.525s
2025-09-05 04:51:33,878 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:51:33,934 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:51:33,943 - src.experiment_runner - INFO - Completed run 503/510: 7.38s
2025-09-05 04:51:33,943 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:51:33,943 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:51:33,943 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:51:33,943 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:51:33,943 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:51:33,943 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:51:33,943 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:51:33,949 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:51:33,949 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:51:33,949 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:51:33,949 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:51:33,950 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:51:33,950 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:51:34,510 - src.llm_wrapper - INFO - Model ready in 0.56s
2025-09-05 04:51:34,510 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:51:34,510 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:51:34,510 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:51:34,510 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:51:34,510 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:51:34,510 - src.rag_pipeline - INFO - Processing validated query: 'What are neural networks?'
2025-09-05 04:51:34,517 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are neural networks?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.10it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.10it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.08it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.08it/s, batch=1/1, memory=N/A]
2025-09-05 04:51:35,040 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:51:35,042 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:51:35,482 - src.rag_pipeline - INFO - Retrieved 5 contexts in 0.964s
2025-09-05 04:51:35,495 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:51:35,495 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1353 tokens) in 0.012s
2025-09-05 04:51:51,341 - src.rag_pipeline - INFO - Generated response (238 tokens) in 15.840s
2025-09-05 04:51:51,578 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:51:51,636 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:51:51,645 - src.experiment_runner - INFO - Completed run 504/510: 17.40s
2025-09-05 04:51:51,645 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:51:51,645 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:51:51,645 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:51:51,645 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:51:51,645 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:51:51,645 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:51:51,645 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:51:51,652 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:51:51,652 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:51:51,653 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:51:51,653 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:51:51,653 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:51:51,653 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:51:52,216 - src.llm_wrapper - INFO - Model ready in 0.56s
2025-09-05 04:51:52,216 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:51:52,216 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:51:52,216 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:51:52,216 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:51:52,216 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:51:52,216 - src.rag_pipeline - INFO - Processing validated query: 'How do large language models work?'
2025-09-05 04:51:52,225 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How do large language models work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.03it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.03it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.00it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.00it/s, batch=1/1, memory=N/A]
2025-09-05 04:51:52,932 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:51:52,934 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:51:53,373 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.145s
2025-09-05 04:51:53,385 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:51:53,385 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1337 tokens) in 0.009s
2025-09-05 04:51:59,302 - src.rag_pipeline - INFO - Generated response (47 tokens) in 5.912s
2025-09-05 04:51:59,498 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:51:59,553 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:51:59,561 - src.experiment_runner - INFO - Completed run 505/510: 7.66s
2025-09-05 04:51:59,561 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:51:59,561 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:51:59,561 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:51:59,562 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:51:59,562 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:51:59,562 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:51:59,562 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:51:59,568 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:51:59,568 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:51:59,568 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:51:59,568 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:51:59,568 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:51:59,568 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:52:00,138 - src.llm_wrapper - INFO - Model ready in 0.57s
2025-09-05 04:52:00,138 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:52:00,138 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:52:00,138 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:52:00,138 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:52:00,138 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:52:00,139 - src.rag_pipeline - INFO - Processing validated query: 'What is natural language processing?'
2025-09-05 04:52:00,148 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is natural language processing?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.16it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.15it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.11it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.11it/s, batch=1/1, memory=N/A]
2025-09-05 04:52:00,792 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:52:00,795 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:52:01,266 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.117s
2025-09-05 04:52:01,279 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:52:01,280 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1749 tokens) in 0.012s
2025-09-05 04:52:08,653 - src.rag_pipeline - INFO - Generated response (55 tokens) in 7.369s
2025-09-05 04:52:08,870 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:52:08,927 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:52:08,934 - src.experiment_runner - INFO - Completed run 506/510: 9.09s
2025-09-05 04:52:08,935 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:52:08,935 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:52:08,935 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:52:08,935 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:52:08,935 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:52:08,935 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:52:08,935 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:52:08,940 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:52:08,941 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:52:08,941 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:52:08,941 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:52:08,941 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:52:08,941 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:52:09,486 - src.llm_wrapper - INFO - Model ready in 0.55s
2025-09-05 04:52:09,486 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:52:09,486 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:52:09,487 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:52:09,487 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:52:09,487 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:52:09,487 - src.rag_pipeline - INFO - Processing validated query: 'Explain computer vision techniques.'
2025-09-05 04:52:09,500 - src.rag_pipeline - INFO - Retrieving contexts for query: 'Explain computer vision techniques....'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.19it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.17it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.14it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.14it/s, batch=1/1, memory=N/A]
2025-09-05 04:52:10,101 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:52:10,103 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:52:10,598 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.097s
2025-09-05 04:52:10,608 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:52:10,608 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1297 tokens) in 0.008s
2025-09-05 04:52:30,460 - src.rag_pipeline - INFO - Generated response (318 tokens) in 19.844s
2025-09-05 04:52:30,675 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:52:30,733 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:52:30,741 - src.experiment_runner - INFO - Completed run 507/510: 21.53s
2025-09-05 04:52:30,741 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:52:30,741 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:52:30,741 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:52:30,741 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:52:30,741 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:52:30,741 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:52:30,741 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:52:30,746 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:52:30,746 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:52:30,746 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:52:30,746 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:52:30,747 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:52:30,747 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:52:31,285 - src.llm_wrapper - INFO - Model ready in 0.54s
2025-09-05 04:52:31,285 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:52:31,286 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:52:31,286 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:52:31,286 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:52:31,286 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:52:31,286 - src.rag_pipeline - INFO - Processing validated query: 'What is reinforcement learning?'
2025-09-05 04:52:31,298 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What is reinforcement learning?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.04it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.04it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.02it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.01it/s, batch=1/1, memory=N/A]
2025-09-05 04:52:31,942 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:52:31,945 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:52:32,365 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.066s
2025-09-05 04:52:32,376 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:52:32,376 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1236 tokens) in 0.010s
2025-09-05 04:52:44,853 - src.rag_pipeline - INFO - Generated response (180 tokens) in 12.471s
2025-09-05 04:52:45,059 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:52:45,112 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:52:45,121 - src.experiment_runner - INFO - Completed run 508/510: 14.11s
2025-09-05 04:52:45,121 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:52:45,121 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:52:45,121 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:52:45,121 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:52:45,121 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:52:45,121 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:52:45,121 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:52:45,127 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:52:45,127 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:52:45,127 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:52:45,127 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:52:45,127 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:52:45,128 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:52:45,741 - src.llm_wrapper - INFO - Model ready in 0.61s
2025-09-05 04:52:45,741 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:52:45,741 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:52:45,742 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:52:45,742 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:52:45,742 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:52:45,742 - src.rag_pipeline - INFO - Processing validated query: 'How does data preprocessing work?'
2025-09-05 04:52:45,749 - src.rag_pipeline - INFO - Retrieving contexts for query: 'How does data preprocessing work?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.05it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.04it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.01it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.01it/s, batch=1/1, memory=N/A]
2025-09-05 04:52:46,360 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:52:46,363 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:52:46,754 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.004s
2025-09-05 04:52:46,766 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:52:46,766 - src.rag_pipeline - INFO - Built prompt with 5 contexts (909 tokens) in 0.010s
2025-09-05 04:52:56,362 - src.rag_pipeline - INFO - Generated response (139 tokens) in 9.585s
2025-09-05 04:52:56,692 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:52:56,768 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:52:56,776 - src.experiment_runner - INFO - Completed run 509/510: 11.24s
2025-09-05 04:52:56,776 - src.rag_pipeline - INFO - Initializing RAG pipeline components
2025-09-05 04:52:56,776 - src.rag_pipeline - INFO - Creating retriever...
2025-09-05 04:52:56,777 - src.embedding_service - INFO - Acquiring embedding model from cache: sentence-transformers/all-MiniLM-L6-v2
2025-09-05 04:52:56,777 - src.embedding_service - INFO - Embedding model ready:
2025-09-05 04:52:56,777 - src.embedding_service - INFO -   Device: mps
2025-09-05 04:52:56,777 - src.embedding_service - INFO -   Max sequence length: 256
2025-09-05 04:52:56,777 - src.embedding_service - INFO -   Embedding dimension: 384
2025-09-05 04:52:56,785 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:52:56,785 - src.vector_database - INFO - sqlite-vec vector search table created successfully
2025-09-05 04:52:56,785 - src.rag_pipeline - INFO - Creating prompt builder...
2025-09-05 04:52:56,785 - src.rag_pipeline - INFO - Creating LLM wrapper...
2025-09-05 04:52:56,786 - src.llm_wrapper - INFO - Acquiring LLM model from cache: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
2025-09-05 04:52:56,786 - src.llm_wrapper - INFO - Loading LLM model from: /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/models/gemma-3-4b-it-q4_0.gguf
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
2025-09-05 04:52:57,501 - src.llm_wrapper - INFO - Model ready in 0.72s
2025-09-05 04:52:57,501 - src.llm_wrapper - INFO -   Context window: 8192
2025-09-05 04:52:57,501 - src.llm_wrapper - INFO -   GPU layers: -1
2025-09-05 04:52:57,501 - src.llm_wrapper - INFO -   Metal acceleration: enabled
2025-09-05 04:52:57,502 - src.rag_pipeline - INFO - RAG pipeline initialized successfully
2025-09-05 04:52:57,502 - src.rag_pipeline - INFO - Set corpus context to: scifact_scientific
2025-09-05 04:52:57,502 - src.rag_pipeline - INFO - Processing validated query: 'What are the challenges in AI development?'
2025-09-05 04:52:57,511 - src.rag_pipeline - INFO - Retrieving contexts for query: 'What are the challenges in AI development?...'
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]
Batches:   0%|          | 0/1 [00:00<?, ?it/s][A
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.38it/s][ABatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.34it/s]
Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.32it/s, batch=1/1, memory=N/A]Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.31it/s, batch=1/1, memory=N/A]
2025-09-05 04:52:58,700 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:52:58,707 - src.vector_database - INFO - Successfully loaded sqlite-vec extension using Python package
2025-09-05 04:52:59,166 - src.rag_pipeline - INFO - Retrieved 5 contexts in 1.654s
2025-09-05 04:52:59,177 - src.prompt_builder - INFO - Included 5/5 contexts
2025-09-05 04:52:59,178 - src.rag_pipeline - INFO - Built prompt with 5 contexts (1445 tokens) in 0.010s
2025-09-05 04:53:15,314 - src.rag_pipeline - INFO - Generated response (237 tokens) in 16.129s
2025-09-05 04:53:15,540 - src.llm_wrapper - INFO - Unloading model and evicting from cache to free memory
2025-09-05 04:53:15,604 - src.llm_wrapper - INFO - Model evicted from cache successfully
2025-09-05 04:53:15,611 - src.experiment_runner - INFO - Completed run 510/510: 18.54s
2025-09-05 04:53:15,613 - src.experiment_runner - INFO - Parameter sweep completed in 14888.4s

‚úÖ Template experiment 'Chunk Size and Overlap Optimization' completed!
Experiment ID: template_Chunk Size and Overlap Optimization_1757025907
Total runtime: 14888.4s
Configurations tested: 1
‚úÖ Saved 510 experiment results to /Users/nickwiebe/Documents/claude-workspace/RAGagentProject/agent/Opus/Opus-2/Opus-Experiments/experiments/chunking/results/scifact_sequential_optimization.json
   Enhanced format includes full config provenance
üíæ Results saved to 
/Users/nickwiebe/Documents/claude-workspace/RAGagentProject/agent/Opus/Opus-2/Op
us-Experiments/experiments/chunking/results/scifact_sequential_optimization.json
üéâ SciFact experiment queued and started successfully!
